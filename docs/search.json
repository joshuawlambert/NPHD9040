[
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html",
    "href": "Lectures/introduction-to-simple-linear-regression.html",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Understand the fundamentals of Simple Linear Regression\nLearn the definition and purpose of Simple Linear Regression\nIdentify key concepts and assumptions of Simple Linear Regression\nVisualize the relationship between variables\nCalculate and interpret correlation coefficients\nFormulate the regression equation\nEstimate the regression coefficients\nTest the significance of the regression coefficients\nCalculate confidence intervals for the coefficients\nUnderstand the coefficient of determination (R-squared)\nPerform residual analysis and diagnostic plots\nInterpret regression coefficients\nCheck assumptions\nExplore examples of Simple Linear Regression in nursing research\n\n\n\n\nSimple Linear Regression is a statistical method used to model the relationship between a single independent variable (predictor) and a continuous dependent variable (outcome). It aims to predict the dependent variable based on the values of the independent variable.\n\n\n\nLinearity: The relationship between the predictor and outcome is linear.\nIndependence: Observations are independent.\nHomoscedasticity: The variance of residuals is constant.\nNormality: Residuals are normally distributed.\n\n\n\n\n\nUse scatter plots to visualize the relationship between the predictor and outcome variables.\n\n\n\n\nR Code Example:\n\n\n\n# Scatter plot in R\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 3 * x + rnorm(100)\n\nplot(x, y, main = \"Scatter Plot of Predictor vs. Outcome\", xlab = \"Predictor (x)\", ylab = \"Outcome (y)\", col = \"blue\")\n      \n\n\n\n\n\n\n\n\n\nMeasures the strength and direction of the linear relationship between two continuous variables.\n\n\n\n\\[\nr = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\n\\]\n\n\n\n\n( r = +1 ): Perfect positive correlation\n( r = -1 ): Perfect negative correlation\n( r = 0 ): No correlation\n\n\n\n\n\nThe regression equation represents the linear relationship between the predictor (( x )) and outcome (( y )).\n\n\n\\[\ny = \\beta_0 + \\beta_1 x\n\\] where: - ( _0 ): Intercept (constant) - ( _1 ): Slope (coefficient)\n\n\n\n\nThe coefficients are estimated using the method of least squares, which minimizes the sum of squared residuals.\n\n\n\nSlope (Coefficient): \\[\n\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n\\]\nIntercept (Constant): \\[\n\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n\\]\n\n\n\n\n\n\n\n\nNull Hypothesis (( H_0 )): The coefficient is not significantly different from zero (( _1 = 0 )).\nAlternative Hypothesis (( H_1 )): The coefficient is significantly different from zero (( _1 )).\n\n\n\n\nThe significance of the coefficients is tested using a t-test.\n\n\n\n\\[\nt = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\n\\] where: - ( ): Estimated coefficient - ( SE() ): Standard error of the coefficient\n\n\n\n\nA confidence interval provides a range within which the true coefficient is likely to fall.\n\n\n\\[\n\\hat{\\beta} \\pm t^* \\cdot SE(\\hat{\\beta})\n\\] where: - ( t^* ): Critical value from the t-distribution\n\n\n\n\nMeasures the proportion of the variance in the outcome variable explained by the predictor.\n\n\n\\[\nR^2 = \\frac{\\sum (\\hat{y} - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}\n\\]\n\n\n\n\n\n\nPlot residuals against the predicted values to check for homoscedasticity and non-linearity.\n\n\n\nCheck the normality of residuals by plotting them against a normal distribution.\n\n\n\n\n\nR Code Example:\n\n\n\n# Residual plots and Q-Q plot in R\nfit &lt;- lm(y ~ x)\n\n# Residual vs. Fitted plot\nplot(fit, which = 1, main = \"Residuals vs. Fitted\")\n\n# Q-Q plot\nplot(fit, which = 2, main = \"Q-Q Plot\")\n      \n\n\n\n\n\n\n\n\n**Intercept (( _0 )):** The expected value of the outcome when the predictor is zero.\n**Slope (( _1 )):** The expected change in the outcome for a one-unit increase in the predictor.\n\n\n\n\n\nLinearity: Residual vs. Fitted plot should show a random pattern.\nIndependence: Observations should be independent.\nHomoscedasticity: Residual vs. Fitted plot should show constant variance.\nNormality: Q-Q plot should show residuals roughly following a straight line.\n\n\n\n\n\n\n\nResearch Question: How does BMI affect blood pressure in adults?\nPredictor (x): BMI\nOutcome (y): Blood Pressure\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nSelect your predictor variable and move it to the Construct Model Effects  column.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(123)\nBMI &lt;- rnorm(100, mean = 25, sd = 3)\nBP &lt;- 2 * BMI + rnorm(100, sd = 5)\n\n# Simple Linear Regression\nfit1 &lt;- lm(BP ~ BMI)\nsummary(fit1)\n\n# Plot\nplot(BMI, BP, main = \"BMI vs. Blood Pressure\", xlab = \"BMI\", ylab = \"Blood Pressure\")\nabline(fit1, col = \"red\")\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(123)\ndata = pd.DataFrame({\n  'BMI': np.random.normal(25, 3, 100),\n  'BP': 2 * np.random.normal(25, 3, 100) + np.random.normal(0, 5, 100)\n})\n\n# Simple Linear Regression\nmodel1 = ols('BP ~ BMI', data=data).fit()\nprint(model1.summary())\n\n# Plot\nimport matplotlib.pyplot as plt\nplt.scatter(data['BMI'], data['BP'])\nplt.plot(data['BMI'], model1.predict(data), color='red')\nplt.xlabel('BMI')\nplt.ylabel('Blood Pressure')\nplt.title('BMI vs. Blood Pressure')\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Regression &gt; Linear.\n\n\nSelect your continuous response variable and move it to the Dependent box.\n\n\nSelect your predictor variable and move it to the Block 1 of 1 box.\n\n\nClick Statistics and check Estimates, R squared change, and Durbin-Watson.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 123\n\n* Simulated data\nset obs 100\ngen BMI = rnormal(25, 3)\ngen BP = 2 * BMI + rnormal(0, 5)\n\n* Simple Linear Regression\nregress BP BMI"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#objectives",
    "href": "Lectures/introduction-to-simple-linear-regression.html#objectives",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Understand the fundamentals of Simple Linear Regression\nLearn the definition and purpose of Simple Linear Regression\nIdentify key concepts and assumptions of Simple Linear Regression\nVisualize the relationship between variables\nCalculate and interpret correlation coefficients\nFormulate the regression equation\nEstimate the regression coefficients\nTest the significance of the regression coefficients\nCalculate confidence intervals for the coefficients\nUnderstand the coefficient of determination (R-squared)\nPerform residual analysis and diagnostic plots\nInterpret regression coefficients\nCheck assumptions\nExplore examples of Simple Linear Regression in nursing research"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#definition-and-purpose-of-simple-linear-regression",
    "href": "Lectures/introduction-to-simple-linear-regression.html#definition-and-purpose-of-simple-linear-regression",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Simple Linear Regression is a statistical method used to model the relationship between a single independent variable (predictor) and a continuous dependent variable (outcome). It aims to predict the dependent variable based on the values of the independent variable.\n\n\n\nLinearity: The relationship between the predictor and outcome is linear.\nIndependence: Observations are independent.\nHomoscedasticity: The variance of residuals is constant.\nNormality: Residuals are normally distributed."
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#visualizing-the-relationship-between-variables",
    "href": "Lectures/introduction-to-simple-linear-regression.html#visualizing-the-relationship-between-variables",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Use scatter plots to visualize the relationship between the predictor and outcome variables.\n\n\n\n\nR Code Example:\n\n\n\n# Scatter plot in R\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 3 * x + rnorm(100)\n\nplot(x, y, main = \"Scatter Plot of Predictor vs. Outcome\", xlab = \"Predictor (x)\", ylab = \"Outcome (y)\", col = \"blue\")"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#calculating-and-interpreting-correlation-coefficients",
    "href": "Lectures/introduction-to-simple-linear-regression.html#calculating-and-interpreting-correlation-coefficients",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Measures the strength and direction of the linear relationship between two continuous variables.\n\n\n\n\\[\nr = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\n\\]\n\n\n\n\n( r = +1 ): Perfect positive correlation\n( r = -1 ): Perfect negative correlation\n( r = 0 ): No correlation"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#formulating-the-regression-equation",
    "href": "Lectures/introduction-to-simple-linear-regression.html#formulating-the-regression-equation",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "The regression equation represents the linear relationship between the predictor (( x )) and outcome (( y )).\n\n\n\\[\ny = \\beta_0 + \\beta_1 x\n\\] where: - ( _0 ): Intercept (constant) - ( _1 ): Slope (coefficient)"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#estimating-the-regression-coefficients",
    "href": "Lectures/introduction-to-simple-linear-regression.html#estimating-the-regression-coefficients",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "The coefficients are estimated using the method of least squares, which minimizes the sum of squared residuals.\n\n\n\nSlope (Coefficient): \\[\n\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n\\]\nIntercept (Constant): \\[\n\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#testing-the-significance-of-the-regression-coefficients",
    "href": "Lectures/introduction-to-simple-linear-regression.html#testing-the-significance-of-the-regression-coefficients",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Null Hypothesis (( H_0 )): The coefficient is not significantly different from zero (( _1 = 0 )).\nAlternative Hypothesis (( H_1 )): The coefficient is significantly different from zero (( _1 )).\n\n\n\n\nThe significance of the coefficients is tested using a t-test.\n\n\n\n\\[\nt = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\n\\] where: - ( ): Estimated coefficient - ( SE() ): Standard error of the coefficient"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#confidence-intervals-for-the-coefficients",
    "href": "Lectures/introduction-to-simple-linear-regression.html#confidence-intervals-for-the-coefficients",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "A confidence interval provides a range within which the true coefficient is likely to fall.\n\n\n\\[\n\\hat{\\beta} \\pm t^* \\cdot SE(\\hat{\\beta})\n\\] where: - ( t^* ): Critical value from the t-distribution"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#coefficient-of-determination-r-squared",
    "href": "Lectures/introduction-to-simple-linear-regression.html#coefficient-of-determination-r-squared",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Measures the proportion of the variance in the outcome variable explained by the predictor.\n\n\n\\[\nR^2 = \\frac{\\sum (\\hat{y} - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#residual-analysis-and-diagnostic-plots",
    "href": "Lectures/introduction-to-simple-linear-regression.html#residual-analysis-and-diagnostic-plots",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Plot residuals against the predicted values to check for homoscedasticity and non-linearity.\n\n\n\nCheck the normality of residuals by plotting them against a normal distribution.\n\n\n\n\n\nR Code Example:\n\n\n\n# Residual plots and Q-Q plot in R\nfit &lt;- lm(y ~ x)\n\n# Residual vs. Fitted plot\nplot(fit, which = 1, main = \"Residuals vs. Fitted\")\n\n# Q-Q plot\nplot(fit, which = 2, main = \"Q-Q Plot\")"
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#interpretation-of-regression-coefficients",
    "href": "Lectures/introduction-to-simple-linear-regression.html#interpretation-of-regression-coefficients",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "**Intercept (( _0 )):** The expected value of the outcome when the predictor is zero.\n**Slope (( _1 )):** The expected change in the outcome for a one-unit increase in the predictor."
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#checking-assumptions",
    "href": "Lectures/introduction-to-simple-linear-regression.html#checking-assumptions",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Linearity: Residual vs. Fitted plot should show a random pattern.\nIndependence: Observations should be independent.\nHomoscedasticity: Residual vs. Fitted plot should show constant variance.\nNormality: Q-Q plot should show residuals roughly following a straight line."
  },
  {
    "objectID": "Lectures/introduction-to-simple-linear-regression.html#examples-of-simple-linear-regression-in-nursing-research",
    "href": "Lectures/introduction-to-simple-linear-regression.html#examples-of-simple-linear-regression-in-nursing-research",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Research Question: How does BMI affect blood pressure in adults?\nPredictor (x): BMI\nOutcome (y): Blood Pressure\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nSelect your predictor variable and move it to the Construct Model Effects  column.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(123)\nBMI &lt;- rnorm(100, mean = 25, sd = 3)\nBP &lt;- 2 * BMI + rnorm(100, sd = 5)\n\n# Simple Linear Regression\nfit1 &lt;- lm(BP ~ BMI)\nsummary(fit1)\n\n# Plot\nplot(BMI, BP, main = \"BMI vs. Blood Pressure\", xlab = \"BMI\", ylab = \"Blood Pressure\")\nabline(fit1, col = \"red\")\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(123)\ndata = pd.DataFrame({\n  'BMI': np.random.normal(25, 3, 100),\n  'BP': 2 * np.random.normal(25, 3, 100) + np.random.normal(0, 5, 100)\n})\n\n# Simple Linear Regression\nmodel1 = ols('BP ~ BMI', data=data).fit()\nprint(model1.summary())\n\n# Plot\nimport matplotlib.pyplot as plt\nplt.scatter(data['BMI'], data['BP'])\nplt.plot(data['BMI'], model1.predict(data), color='red')\nplt.xlabel('BMI')\nplt.ylabel('Blood Pressure')\nplt.title('BMI vs. Blood Pressure')\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Regression &gt; Linear.\n\n\nSelect your continuous response variable and move it to the Dependent box.\n\n\nSelect your predictor variable and move it to the Block 1 of 1 box.\n\n\nClick Statistics and check Estimates, R squared change, and Durbin-Watson.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 123\n\n* Simulated data\nset obs 100\ngen BMI = rnormal(25, 3)\ngen BP = 2 * BMI + rnormal(0, 5)\n\n* Simple Linear Regression\nregress BP BMI"
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html",
    "href": "Lectures/introduction-to-two-way-anova.html",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Understand the fundamentals of Two-Way ANOVA\nLearn the definition and purpose of Two-Way ANOVA\nIdentify the key differences between One-Way ANOVA and Two-Way ANOVA\nUnderstand the structure of a Two-Way ANOVA design\nRecognize main effects and interaction\nComprehend the assumptions of Two-Way ANOVA\nCalculate Sum of Squares (SS) and Mean Squares (MS)\nConduct hypothesis testing for main effects and interaction\nMeasure effect sizes and significance\nInterpret main effects and interaction\nConduct post-hoc analysis\nIdentify outliers and influential observations, normality, and homogeneity\nExplore examples of Two-Way ANOVA in nursing research\n\n\n\n\nTwo-Way ANOVA is a statistical test used to examine the impact of two independent variables on a continuous dependent variable. It is an extension of One-Way ANOVA but allows for the simultaneous evaluation of two factors, making it suitable for analyzing more complex experimental designs.\n\n\n\nOne-Way ANOVA: Evaluates the effect of a single independent variable on a continuous outcome.\nTwo-Way ANOVA: Evaluates the effects of two independent variables (factors), including any interaction between them.\n\n\n\n\n\nIn a Two-Way ANOVA, each factor (independent variable) can have multiple levels. For example, in a clinical trial examining the effects of drug dosage (Factor 1) and diet (Factor 2), the design might include: - Factor 1 (Dosage): Low, Medium, High - Factor 2 (Diet): Control, Low-Fat, High-Fat\n\n\n\nMain Effect: The impact of one factor on the dependent variable, ignoring the levels of the other factor.\nInteraction Effect: The combined effect of both factors on the dependent variable that is different from the sum of their individual effects.\n\n\n\n\n\n\n\nResiduals (errors) should be normally distributed.\n\n\n\nVariances should be approximately equal across all levels of the factors.\n\n\n\n\n\n\n\nTotal Sum of Squares (SST): \\[\nSST = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\n\\] where ( Y_i ) is each individual observation, and ( {Y} ) is the overall mean.\nSum of Squares for Factor A (SSA): \\[\nSSA = \\sum_{j=1}^{a} n_{j} (\\bar{Y}_{j\\cdot} - \\bar{Y})^2\n\\] where ( n_{j} ) is the number of observations in each level of Factor A, ( {Y}_{j} ) is the mean for each level of Factor\n\n\n\nSum of Squares for Factor B (SSB): \\[\nSSB = \\sum_{k=1}^{b} n_{k} (\\bar{Y}_{\\cdot k} - \\bar{Y})^2\n\\] where ( n_{k} ) is the number of observations in each level of Factor B, ( {Y}_{k} ) is the mean for each level of Factor B.\nSum of Squares for Interaction (SSAB): \\[\nSSAB = \\sum_{j=1}^{a} \\sum_{k=1}^{b} n_{jk} (\\bar{Y}_{jk} - \\bar{Y}_{j\\cdot} - \\bar{Y}_{\\cdot k} + \\bar{Y})^2\n\\] where ( {Y}_{jk} ) is the mean of each combination of levels in Factors A and B.\nSum of Squares for Error (SSE): \\[\nSSE = \\sum_{j=1}^{a} \\sum_{k=1}^{b} \\sum_{i=1}^{n_{jk}} (Y_{ijk} - \\bar{Y}_{jk})^2\n\\]\n\n\n\n\nMean Squares are obtained by dividing each Sum of Squares by the corresponding degrees of freedom.\n\nMean Square for Factor A (MSA): \\[\nMSA = \\frac{SSA}{a - 1}\n\\]\nMean Square for Factor B (MSB): \\[\nMSB = \\frac{SSB}{b - 1}\n\\]\nMean Square for Interaction (MSAB): \\[\nMSAB = \\frac{SSAB}{(a - 1)(b - 1)}\n\\]\nMean Square for Error (MSE): \\[\nMSE = \\frac{SSE}{ab(n - 1)}\n\\]\n\n\n\n\n\n\n\n\nFactor A (Main Effect):\n\nNull Hypothesis (( H_0 )): All levels of Factor A have the same mean.\nAlternative Hypothesis (( H_1 )): At least one level of Factor A has a different mean.\n\nFactor B (Main Effect):\n\nNull Hypothesis (( H_0 )): All levels of Factor B have the same mean.\nAlternative Hypothesis (( H_1 )): At least one level of Factor B has a different mean.\n\nInteraction Effect:\n\nNull Hypothesis (( H_0 )): No interaction between Factors A and\n\n\n\nAlternative Hypothesis (( H_1 )): Interaction exists between Factors A and B.\n\n\n\n\n\n\nFactor A: \\[\nF_A = \\frac{MSA}{MSE}\n\\]\nFactor B: \\[\nF_B = \\frac{MSB}{MSE}\n\\]\nInteraction (A x B): \\[\nF_{AB} = \\frac{MSAB}{MSE}\n\\]\n\n\n\n\n\nCompare the F-ratios to the critical values from the F-distribution.\nIf the calculated F-ratio is greater than the critical value, reject the null hypothesis.\n\n\n\n\n\n\n\n\n\nRepresents the proportion of the total variance explained by each factor.\n\nFactor A: \\[\n\\eta^2_A = \\frac{SSA}{SST}\n\\]\nFactor B: \\[\n\\eta^2_B = \\frac{SSB}{SST}\n\\]\nInteraction (A x B): \\[\n\\eta^2_{AB} = \\frac{SSAB}{SST}\n\\]\n\n\n\n\n\nIf significant main or interaction effects are found, post-hoc tests (e.g., Tukey’s HSD) can identify which specific groups differ.\n\n\n\\[\nHSD = q \\sqrt{\\frac{MSE}{n}}\n\\] where ( q ) is the studentized range statistic, and ( n ) is the sample size per group.\n\n\n\n\n\n\n\nUse Q-Q plots to check the normality of residuals.\n\n\n\n\n\nUse Levene’s test or Bartlett’s test.\n\n\n\n\n\nUse residual plots or Cook’s distance to identify influential points.\n\n\n\n\n\n\n\n\nResearch Question: How do drug dosage and diet affect blood pressure levels in patients?\nFactors:\n\nDosage (Low, Medium, High)\nDiet (Control, Low-Fat, High-Fat)\n\nResponse Variable: Blood Pressure Levels\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Construct Model Effects, add both factors and their interaction.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(123)\ndata &lt;- data.frame(\n  Dosage = factor(rep(c(\"Low\", \"Medium\", \"High\"), each = 30)),\n  Diet = factor(rep(c(\"Control\", \"Low-Fat\", \"High-Fat\"), times = 30)),\n  BP = rnorm(90, mean = rep(c(120, 115, 130), each = 30), sd = 10)\n)\n\n# Two-Way ANOVA\nmodel &lt;- aov(BP ~ Dosage * Diet, data = data)\nsummary(model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(123)\ndata = pd.DataFrame({\n  'Dosage': pd.Categorical(['Low'] * 30 + ['Medium'] * 30 + ['High'] * 30),\n  'Diet': pd.Categorical(['Control', 'Low-Fat', 'High-Fat'] * 30),\n  'BP': np.random.normal(loc=[120, 115, 130], scale=10, size=90)\n})\n\n# Two-Way ANOVA\nmodel = ols('BP ~ Dosage * Diet', data=data).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\nprint(anova_table)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your continuous dependent variable and move it to the Dependent Variable box.\n\n\nSelect both factors and move them to the Fixed Factors box.\n\n\nClick Model and specify the interaction between the two factors.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 123\n\n* Simulated data\nset obs 90\ngen Dosage = cond(mod(_n, 3) == 1, \"Low\", cond(mod(_n, 3) == 2, \"Medium\", \"High\"))\ngen Diet = cond(_n &lt;= 30, \"Control\", cond(_n &lt;= 60, \"Low-Fat\", \"High-Fat\"))\ngen BP = rnormal(cond(mod(_n, 3) == 1, 120, cond(mod(_n, 3) == 2, 115, 130)), 10)\n\n* Two-Way ANOVA\nanova BP Dosage##Diet\n      \n\n\n\n\n\n\n\nResearch Question: How do a nursing intervention and patient demographics affect recovery time?\nFactors:\n\nIntervention (Control, Intensive)\nGender (Male, Female)\n\nResponse Variable: Recovery Time\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Construct Model Effects, add both factors and their interaction.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(456)\ndata2 &lt;- data.frame(\n  Intervention = factor(rep(c(\"Control\", \"Intensive\"), each = 50)),\n  Gender = factor(rep(c(\"Male\", \"Female\"), times = 50)),\n  RecoveryTime = rnorm(100, mean = rep(c(14, 12), each = 50), sd = 3)\n)\n\n# Two-Way ANOVA\nmodel2 &lt;- aov(RecoveryTime ~ Intervention * Gender, data = data2)\nsummary(model2)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(456)\ndata2 = pd.DataFrame({\n  'Intervention': pd.Categorical(['Control'] * 50 + ['Intensive'] * 50),\n  'Gender': pd.Categorical(['Male', 'Female'] * 50),\n  'RecoveryTime': np.random.normal(loc=[14, 12], scale=3, size=100)\n})\n\n# Two-Way ANOVA\nmodel2 = ols('RecoveryTime ~ Intervention * Gender', data=data2).fit()\nanova_table2 = sm.stats.anova_lm(model2, typ=2)\nprint(anova_table2)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your continuous dependent variable and move it to the Dependent Variable box.\n\n\nSelect both factors and move them to the Fixed Factors box.\n\n\nClick Model and specify the interaction between the two factors.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 456\n\n* Simulated data\nset obs 100\ngen Intervention = cond(_n &lt;= 50, \"Control\", \"Intensive\")\ngen Gender = cond(mod(_n, 2) == 0, \"Female\", \"Male\")\ngen RecoveryTime = rnormal(cond(_n &lt;= 50, 14, 12), 3)\n\n* Two-Way ANOVA\nanova RecoveryTime Intervention##Gender"
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#objectives",
    "href": "Lectures/introduction-to-two-way-anova.html#objectives",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Understand the fundamentals of Two-Way ANOVA\nLearn the definition and purpose of Two-Way ANOVA\nIdentify the key differences between One-Way ANOVA and Two-Way ANOVA\nUnderstand the structure of a Two-Way ANOVA design\nRecognize main effects and interaction\nComprehend the assumptions of Two-Way ANOVA\nCalculate Sum of Squares (SS) and Mean Squares (MS)\nConduct hypothesis testing for main effects and interaction\nMeasure effect sizes and significance\nInterpret main effects and interaction\nConduct post-hoc analysis\nIdentify outliers and influential observations, normality, and homogeneity\nExplore examples of Two-Way ANOVA in nursing research"
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#definition-and-purpose-of-two-way-anova",
    "href": "Lectures/introduction-to-two-way-anova.html#definition-and-purpose-of-two-way-anova",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Two-Way ANOVA is a statistical test used to examine the impact of two independent variables on a continuous dependent variable. It is an extension of One-Way ANOVA but allows for the simultaneous evaluation of two factors, making it suitable for analyzing more complex experimental designs.\n\n\n\nOne-Way ANOVA: Evaluates the effect of a single independent variable on a continuous outcome.\nTwo-Way ANOVA: Evaluates the effects of two independent variables (factors), including any interaction between them."
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#understanding-the-structure-of-a-two-way-anova-design",
    "href": "Lectures/introduction-to-two-way-anova.html#understanding-the-structure-of-a-two-way-anova-design",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "In a Two-Way ANOVA, each factor (independent variable) can have multiple levels. For example, in a clinical trial examining the effects of drug dosage (Factor 1) and diet (Factor 2), the design might include: - Factor 1 (Dosage): Low, Medium, High - Factor 2 (Diet): Control, Low-Fat, High-Fat\n\n\n\nMain Effect: The impact of one factor on the dependent variable, ignoring the levels of the other factor.\nInteraction Effect: The combined effect of both factors on the dependent variable that is different from the sum of their individual effects."
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#assumptions-of-two-way-anova",
    "href": "Lectures/introduction-to-two-way-anova.html#assumptions-of-two-way-anova",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Residuals (errors) should be normally distributed.\n\n\n\nVariances should be approximately equal across all levels of the factors."
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#calculation-of-ss-sum-of-squares-and-ms-mean-squares",
    "href": "Lectures/introduction-to-two-way-anova.html#calculation-of-ss-sum-of-squares-and-ms-mean-squares",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Total Sum of Squares (SST): \\[\nSST = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\n\\] where ( Y_i ) is each individual observation, and ( {Y} ) is the overall mean.\nSum of Squares for Factor A (SSA): \\[\nSSA = \\sum_{j=1}^{a} n_{j} (\\bar{Y}_{j\\cdot} - \\bar{Y})^2\n\\] where ( n_{j} ) is the number of observations in each level of Factor A, ( {Y}_{j} ) is the mean for each level of Factor\n\n\n\nSum of Squares for Factor B (SSB): \\[\nSSB = \\sum_{k=1}^{b} n_{k} (\\bar{Y}_{\\cdot k} - \\bar{Y})^2\n\\] where ( n_{k} ) is the number of observations in each level of Factor B, ( {Y}_{k} ) is the mean for each level of Factor B.\nSum of Squares for Interaction (SSAB): \\[\nSSAB = \\sum_{j=1}^{a} \\sum_{k=1}^{b} n_{jk} (\\bar{Y}_{jk} - \\bar{Y}_{j\\cdot} - \\bar{Y}_{\\cdot k} + \\bar{Y})^2\n\\] where ( {Y}_{jk} ) is the mean of each combination of levels in Factors A and B.\nSum of Squares for Error (SSE): \\[\nSSE = \\sum_{j=1}^{a} \\sum_{k=1}^{b} \\sum_{i=1}^{n_{jk}} (Y_{ijk} - \\bar{Y}_{jk})^2\n\\]\n\n\n\n\nMean Squares are obtained by dividing each Sum of Squares by the corresponding degrees of freedom.\n\nMean Square for Factor A (MSA): \\[\nMSA = \\frac{SSA}{a - 1}\n\\]\nMean Square for Factor B (MSB): \\[\nMSB = \\frac{SSB}{b - 1}\n\\]\nMean Square for Interaction (MSAB): \\[\nMSAB = \\frac{SSAB}{(a - 1)(b - 1)}\n\\]\nMean Square for Error (MSE): \\[\nMSE = \\frac{SSE}{ab(n - 1)}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#hypothesis-testing-for-main-effects-and-interaction",
    "href": "Lectures/introduction-to-two-way-anova.html#hypothesis-testing-for-main-effects-and-interaction",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Factor A (Main Effect):\n\nNull Hypothesis (( H_0 )): All levels of Factor A have the same mean.\nAlternative Hypothesis (( H_1 )): At least one level of Factor A has a different mean.\n\nFactor B (Main Effect):\n\nNull Hypothesis (( H_0 )): All levels of Factor B have the same mean.\nAlternative Hypothesis (( H_1 )): At least one level of Factor B has a different mean.\n\nInteraction Effect:\n\nNull Hypothesis (( H_0 )): No interaction between Factors A and\n\n\n\nAlternative Hypothesis (( H_1 )): Interaction exists between Factors A and B.\n\n\n\n\n\n\nFactor A: \\[\nF_A = \\frac{MSA}{MSE}\n\\]\nFactor B: \\[\nF_B = \\frac{MSB}{MSE}\n\\]\nInteraction (A x B): \\[\nF_{AB} = \\frac{MSAB}{MSE}\n\\]\n\n\n\n\n\nCompare the F-ratios to the critical values from the F-distribution.\nIf the calculated F-ratio is greater than the critical value, reject the null hypothesis."
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#effect-sizes-and-significance",
    "href": "Lectures/introduction-to-two-way-anova.html#effect-sizes-and-significance",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Represents the proportion of the total variance explained by each factor.\n\nFactor A: \\[\n\\eta^2_A = \\frac{SSA}{SST}\n\\]\nFactor B: \\[\n\\eta^2_B = \\frac{SSB}{SST}\n\\]\nInteraction (A x B): \\[\n\\eta^2_{AB} = \\frac{SSAB}{SST}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#post-hoc-analysis",
    "href": "Lectures/introduction-to-two-way-anova.html#post-hoc-analysis",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "If significant main or interaction effects are found, post-hoc tests (e.g., Tukey’s HSD) can identify which specific groups differ.\n\n\n\\[\nHSD = q \\sqrt{\\frac{MSE}{n}}\n\\] where ( q ) is the studentized range statistic, and ( n ) is the sample size per group."
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#identifying-outliers-and-influential-observations",
    "href": "Lectures/introduction-to-two-way-anova.html#identifying-outliers-and-influential-observations",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Use Q-Q plots to check the normality of residuals.\n\n\n\n\n\nUse Levene’s test or Bartlett’s test.\n\n\n\n\n\nUse residual plots or Cook’s distance to identify influential points."
  },
  {
    "objectID": "Lectures/introduction-to-two-way-anova.html#examples-of-two-way-anova-in-nursing-research",
    "href": "Lectures/introduction-to-two-way-anova.html#examples-of-two-way-anova-in-nursing-research",
    "title": "Introduction to Two-Way ANOVA",
    "section": "",
    "text": "Research Question: How do drug dosage and diet affect blood pressure levels in patients?\nFactors:\n\nDosage (Low, Medium, High)\nDiet (Control, Low-Fat, High-Fat)\n\nResponse Variable: Blood Pressure Levels\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Construct Model Effects, add both factors and their interaction.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(123)\ndata &lt;- data.frame(\n  Dosage = factor(rep(c(\"Low\", \"Medium\", \"High\"), each = 30)),\n  Diet = factor(rep(c(\"Control\", \"Low-Fat\", \"High-Fat\"), times = 30)),\n  BP = rnorm(90, mean = rep(c(120, 115, 130), each = 30), sd = 10)\n)\n\n# Two-Way ANOVA\nmodel &lt;- aov(BP ~ Dosage * Diet, data = data)\nsummary(model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(123)\ndata = pd.DataFrame({\n  'Dosage': pd.Categorical(['Low'] * 30 + ['Medium'] * 30 + ['High'] * 30),\n  'Diet': pd.Categorical(['Control', 'Low-Fat', 'High-Fat'] * 30),\n  'BP': np.random.normal(loc=[120, 115, 130], scale=10, size=90)\n})\n\n# Two-Way ANOVA\nmodel = ols('BP ~ Dosage * Diet', data=data).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\nprint(anova_table)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your continuous dependent variable and move it to the Dependent Variable box.\n\n\nSelect both factors and move them to the Fixed Factors box.\n\n\nClick Model and specify the interaction between the two factors.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 123\n\n* Simulated data\nset obs 90\ngen Dosage = cond(mod(_n, 3) == 1, \"Low\", cond(mod(_n, 3) == 2, \"Medium\", \"High\"))\ngen Diet = cond(_n &lt;= 30, \"Control\", cond(_n &lt;= 60, \"Low-Fat\", \"High-Fat\"))\ngen BP = rnormal(cond(mod(_n, 3) == 1, 120, cond(mod(_n, 3) == 2, 115, 130)), 10)\n\n* Two-Way ANOVA\nanova BP Dosage##Diet\n      \n\n\n\n\n\n\n\nResearch Question: How do a nursing intervention and patient demographics affect recovery time?\nFactors:\n\nIntervention (Control, Intensive)\nGender (Male, Female)\n\nResponse Variable: Recovery Time\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Construct Model Effects, add both factors and their interaction.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(456)\ndata2 &lt;- data.frame(\n  Intervention = factor(rep(c(\"Control\", \"Intensive\"), each = 50)),\n  Gender = factor(rep(c(\"Male\", \"Female\"), times = 50)),\n  RecoveryTime = rnorm(100, mean = rep(c(14, 12), each = 50), sd = 3)\n)\n\n# Two-Way ANOVA\nmodel2 &lt;- aov(RecoveryTime ~ Intervention * Gender, data = data2)\nsummary(model2)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(456)\ndata2 = pd.DataFrame({\n  'Intervention': pd.Categorical(['Control'] * 50 + ['Intensive'] * 50),\n  'Gender': pd.Categorical(['Male', 'Female'] * 50),\n  'RecoveryTime': np.random.normal(loc=[14, 12], scale=3, size=100)\n})\n\n# Two-Way ANOVA\nmodel2 = ols('RecoveryTime ~ Intervention * Gender', data=data2).fit()\nanova_table2 = sm.stats.anova_lm(model2, typ=2)\nprint(anova_table2)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your continuous dependent variable and move it to the Dependent Variable box.\n\n\nSelect both factors and move them to the Fixed Factors box.\n\n\nClick Model and specify the interaction between the two factors.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 456\n\n* Simulated data\nset obs 100\ngen Intervention = cond(_n &lt;= 50, \"Control\", \"Intensive\")\ngen Gender = cond(mod(_n, 2) == 0, \"Female\", \"Male\")\ngen RecoveryTime = rnormal(cond(_n &lt;= 50, 14, 12), 3)\n\n* Two-Way ANOVA\nanova RecoveryTime Intervention##Gender"
  },
  {
    "objectID": "Lectures/graphing-your-data.html",
    "href": "Lectures/graphing-your-data.html",
    "title": "Graphing Your Data",
    "section": "",
    "text": "Graphs are crucial in data analysis and presentation because they help:\n\nQuick Interpretation: Graphs provide a quick overview of data trends, making them easier to interpret than tables.\nPattern Identification: They help identify patterns, relationships, and outliers.\nHypothesis Generation: Aids in forming research hypotheses.\nAudience Engagement: Captures and maintains the audience’s attention.\nDecision Support: Facilitates evidence-based decision-making.\n\n\n\n\n\nCategorical Data:\n\nNominal: No inherent order or ranking (e.g., coffee, tea, water).\n\nOrdinal: Has a meaningful order or ranking (e.g., small, medium, large).\n\nPros: Easy to categorize and analyze.\nCons: Limited statistical techniques apply, and numerical differences are not meaningful.\nContinuous Data:\n\nInterval: Has equal intervals between values but no true zero (e.g., temperature, dates).\n\nRatio: Has a meaningful zero point and equal intervals (e.g., weight, strength, pressure).\n\nPros: Wide range of statistical techniques available.\nCons: Requires more complex handling due to scale.\n\n\n\n\n\nUnivariate Graphs:\n\nDefinition: Focuses on a single variable.\nExamples: Histograms, Box plots.\n\nPros:\n\nSimple and intuitive.\nProvides distributional insights.\n\nCons:\n\nLimited to understanding one variable.\n\nBivariate Graphs:\n\nDefinition: Displays the relationship between two variables.\nExamples: Scatter plots, Box plots.\n\nPros:\n\nUseful for studying relationships and trends.\n\nCons:\n\nMight not reveal hidden multivariate trends.\n\nMultivariate Graphs:\n\nDefinition: Involves multiple variables.\nExamples: Scatter plot matrices, Mosaic plots, Treemaps.\n\nPros:\n\nEffective for uncovering complex relationships.\n\nCons:\n\nInterpretation can be challenging without proper labels.\n\n\n\n\n\n\n\nKnow Your Audience: Understand how your audience processes information to tailor your visualizations effectively.\nDefine Your Message: Clearly convey your message simply and quickly.\nUse Visual Elements Appropriately: Combine words, numbers, and images meaningfully.\nPrinciples of Excellence: Based on Edward Tufte’s principles of good graphical design:\n\nEnforce Visual Comparisons: Allow comparisons between data points.\nShow Causality: Highlight causal relationships if possible.\nIntegrate All Visual Elements: Include relevant text and numbers to provide context.\nContent-Driven Design: Ensure design is driven by data quality, relevance, and integrity.\n\n\n\n\n\n\n\n\nGraphical Excellence:\n\nShow Multivariate Data: Present data across multiple dimensions.\nIntegrate All Visual Elements: Include text, numbers, and images cohesively.\nUse Quality, Relevant, and Honest Data: Ensure data integrity and honesty.\n\nCommon Mistakes:\n\nDistorting Data Meaning: Misleading visuals distort interpretation.\nIncorrect Scaling: Scaling errors can misrepresent data trends.\nPoor Data-to-Ink Ratio: Avoid excessive chart junk that doesn’t add value.\n\n\nThe 27 Worst Charts Of All Time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Graphical representation of data distribution using bins of equal length to count frequencies.\nPros:\n\nHelps visualize the distribution shape.\nEasy to compare different distributions.\nIdentifies outliers and skewness.\n\nCons:\n\nBin size selection can lead to over or under-smoothing.\nNot suitable for small data sets.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Distribution.\n\n\nSelect your continuous variable and move it to the Y column.\n\n\nClick OK.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a histogram in R\n        data &lt;- rnorm(100)\n        hist(data, main = \"Histogram of Data\", xlab = \"Values\", col = \"blue\", border = \"black\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(50, 10, 1000)\nplt.hist(data, bins=15, color='blue')\nplt.title(\"Histogram Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /HISTOGRAM=Age\n          /TITLE='Histogram Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 1000\n        generate age = rnormal(50, 10)\n        hist age, bin(15) color(blue)\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Graphically represents data distribution based on quartiles, highlighting outliers, median, and spread.\nPros:\n\nEffective for identifying outliers.\nCompares multiple groups easily.\nRobust to non-normal data.\n\nCons:\n\nLess informative for small data sets.\nNot ideal for displaying multimodal distributions.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your continuous variable to the Y axis.\n\n\nDrag your categorical variable to the X axis.\n\n\nSelect Box Plot from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a box plot in R\n        data &lt;- rnorm(100)\n        boxplot(data, main = \"Box Plot Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(50, 10, 100)\nplt.boxplot(data)\nplt.title(\"Box Plot Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /BOXPLOT=VARIABLES(Age)\n          /TITLE='Box Plot Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 100\n        generate age = rnormal(50, 10)\n        graph box age, title(\"Box Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays relative proportions in part-to-whole relationships using slices of a circle.\nPros:\n\nVisually intuitive.\nEffective for simple categorical data.\n\nCons:\n\nDifficult to compare proportions across different charts.\nNot suitable for large numbers of categories.\nCan distort differences due to angle perception issues.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your categorical variable to the X axis.\n\n\nSelect Pie Chart from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a pie chart in R\n        values &lt;- c(10, 20, 30, 40)\n        labels &lt;- c(\"A\", \"B\", \"C\", \"D\")\n        pie(values, labels = labels, main = \"Pie Chart Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\n\nvalues = [10, 20, 30, 40]\nlabels = [\"A\", \"B\", \"C\", \"D\"]\nplt.pie(values, labels=labels)\nplt.title(\"Pie Chart Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          EXAMINE VARIABLES=age\n          /COMPARE\n          /PLOT=BOXPLOT\n          /STATISTICS=NONE\n          /NOTOTAL\n          /TITLE='Pie Chart Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 4\n        generate group = _n\n        generate value = 10 * group\n        graph pie value, over(group) title(\"Pie Chart Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Shows the relationship between two continuous variables.\nPros:\n\nIdentifies correlations and relationships between variables.\nHighlights clusters and patterns.\nDetects outliers effectively.\n\nCons:\n\nCan suffer from overplotting with large data sets.\nRequires understanding of correlation interpretation.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your first continuous variable to the X axis.\n\n\nDrag your second continuous variable to the Y axis.\n\n\nSelect Scatter Plot from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a scatter plot in R\n        x &lt;- rnorm(100)\n        y &lt;- rnorm(100)\n        plot(x, y, main = \"Scatter Plot Example\", xlab = \"X-axis\", ylab = \"Y-axis\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.normal(50, 10, 100)\ny = np.random.normal(50, 10, 100)\nplt.scatter(x, y)\nplt.title(\"Scatter Plot Example\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /SCATTERPLOT(BIVAR)=income WITH age\n          /MISSING=LISTWISE.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 100\n        generate age = rnormal(50, 10)\n        generate income = rnormal(50, 10)\n        scatter age income, title(\"Scatter Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays the relationships between two or more categorical variables using a stacked rectangle visualization.\nPros:\n\nEffective for identifying associations in contingency tables.\nHighlights interactions between variables.\n\nCons:\n\nInterpretation can be challenging for complex relationships.\nLess intuitive than simpler visualizations.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Y by X.\n\n\nSelect your response variable and move it to the Y column.\n\n\nSelect your explanatory variable and move it to the X column.\n\n\nClick OK.\n\n\nClick the red triangle and select Mosaic Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a mosaic plot in R\n        library(vcd)\n        data(Titanic)\n        mosaic(Titanic, shade = TRUE, legend = TRUE)\n      \n\n\n\n\n\nPython Instructions:\n\n\n\n        import matplotlib.pyplot as plt\n        from statsmodels.graphics.mosaicplot import mosaic\n\n        data = {'Class A': 40, 'Class B': 30, 'Class C': 20, 'Class D': 10}\n        mosaic(data, title='Mosaic Plot Example')\n        plt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /MOSAICPLOT=VARIABLES(Class, Survival)\n          /TITLE='Mosaic Plot Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 4\n        generate class = _n\n        generate survival = 10 * class\n        graph mosaic class survival, title(\"Mosaic Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: A bar or column chart compares data across categories using rectangular bars, where each bar’s length represents the value or frequency of a category. The bars can be vertical (column chart) or horizontal (bar chart).\nPros:\n\nEasily Compares Data:\n\nMakes it simple to compare data across different categories.\nHighlights significant differences between groups.\n\nQuick Insight:\n\nProvides immediate visual insights into category differences.\nSuitable for presenting data to non-technical audiences.\n\nIntuitive Interpretation:\n\nClear and straightforward representation.\nSuitable for a wide range of audiences due to its intuitive nature.\n\nFlexibility:\n\nCan represent both frequency (count data) and summary statistics (e.g., means, medians).\nCan be customized with different bar colors, stacking, grouping, etc.\n\n\nCons:\n\nMisleading Scaling:\n\nCan mislead if bar lengths, axis scales, or data representations are inconsistent.\nImproper axis truncation may exaggerate or minimize differences.\n\nOverloading with Categories:\n\nToo many categories can clutter the chart, making it hard to interpret.\nLimited space can lead to overlapping labels, hindering readability.\n\nNot Ideal for Continuous Data:\n\nWorks best with categorical data but not suitable for continuous variables.\nSummarizing continuous data into categories can lead to loss of detail or oversimplification.\n\nChart Junk:\n\nExcessive use of gridlines, 3D effects, or non-data elements can lead to “chart junk,” distracting from the data itself.\n\nData-to-Ink Ratio:\n\nLow data-to-ink ratio due to large bars relative to the actual information conveyed.\n\n\n\nSummary:\nBar/column charts are highly effective for comparing categorical data and highlighting category differences. However, careful attention to scaling and data representation is crucial to ensure accurate and clear visualization.\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your categorical variable to the X axis.\n\n\nDrag your continuous variable to the Y axis.\n\n\nSelect Bar Chart from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a bar chart in R\n        categories &lt;- c(\"A\", \"B\", \"C\", \"D\")\n        values &lt;- c(10, 20, 30, 40)\n        barplot(values, names.arg = categories, main = \"Bar Chart Example\", col = \"blue\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\n\ncategories = [\"A\", \"B\", \"C\", \"D\"]\nvalues = [10, 20, 30, 40]\nplt.bar(categories, values, color='blue')\nplt.title(\"Bar Chart Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /BAR(GROUPED)=VALUE BY CATEGORY\n          /TITLE='Bar Chart Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 4\n        generate category = _n\n        generate value = 10 * category\n        graph bar value, over(category) title(\"Bar Chart Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays cumulative quantiles of a distribution versus expected quantiles (e.g., normal distribution).\nPros:\n\nIdentifies Distribution Deviations:\n\nClearly shows deviations of data distribution from a theoretical distribution.\n\nEfficient Distribution Fit Determination:\n\nQuickly determines whether the data fits a specific distribution.\n\n\nCons:\n\nRequires Statistical Knowledge:\n\nAccurate interpretation requires understanding quantile statistics.\n\nLess Intuitive:\n\nLess intuitive than histograms or box plots for non-technical audiences.\n\n\n\nSummary:\nQuantile plots are ideal for identifying distribution deviations from theoretical distributions. However, they require statistical knowledge for accurate interpretation and might be less intuitive than other visualization techniques.\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Distribution.\n\n\nSelect your continuous variable and move it to the Y column.\n\n\nClick OK.\n\n\nClick the red triangle and select Normal Quantile Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a quantile plot in R\n        data &lt;- rnorm(100)\n        qqnorm(data)\n        qqline(data, col = \"blue\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\ndata = np.random.normal(50, 10, 100)\nstats.probplot(data, dist=\"norm\", plot=plt)\nplt.title(\"Quantile Plot Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        Q-QPLOT VARIABLES=Age\n          /DISTRIBUTION=NORMAL.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 100\n        generate age = rnormal(50, 10)\n        qnorm age, title(\"Quantile Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Visualizes hierarchical data using nested rectangles.\nPros:\n\nDisplays Large Amounts of Data Efficiently:\n\nSuitable for representing large hierarchical data sets.\n\nIntuitive for Showing Hierarchical Relationships:\n\nClearly shows hierarchical relationships in a visual format.\n\nHighlights Patterns and Clusters Visually:\n\nHighlights patterns, clusters, and outliers effectively.\n\n\nCons:\n\nDistorts Proportions if Sizes are Too Small:\n\nSmaller rectangles can become unreadable and distort proportions.\n\nChallenging to Interpret for High-Depth Hierarchies:\n\nDeep hierarchies can make interpretation challenging and confusing.\n\n\n\nSummary:\nTreemaps are excellent for visualizing hierarchical data efficiently and intuitively. However, careful design is crucial to avoid distortions and ensure interpretability, especially with deep hierarchies.\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your categorical variable to the X axis.\n\n\nDrag your continuous variable to the Y axis.\n\n\nSelect Treemap from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a treemap in R\n        library(treemap)\n        \n        data &lt;- data.frame(\n          category = c(\"A\", \"B\", \"C\", \"D\"),\n          subcategory = c(\"A1\", \"B1\", \"C1\", \"D1\"),\n          value = c(10, 20, 30, 40)\n        )\n        \n        treemap(\n          data,\n          index = c(\"category\", \"subcategory\"),\n          vSize = \"value\",\n          title = \"Treemap Example\"\n        )\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport squarify\n\nlabels = [\"A\", \"B\", \"C\", \"D\"]\nsizes = [10, 20, 30, 40]\nsquarify.plot(sizes=sizes, label=labels, color=[\"blue\", \"green\", \"red\", \"purple\"], alpha=.7)\nplt.title(\"Treemap Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /TREEMAP=VARIABLES(Category, Subcategory)\n          /VALUE=Value\n          /TITLE='Treemap Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        ssc install treemap\n        clear\n        set obs 4\n        generate category = _n\n        generate subcategory = category\n        generate value = 10 * category\n        treemap value, by(category subcategory) title(\"Treemap Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays data trends over time.\nPros:\n\nIdeal for identifying trends, cycles, and seasonal patterns.\nSupports multiple time series on the same graph.\nFacilitates forecasting and future planning.\n\nCons:\n\nLess effective for non-temporal data.\nCan suffer from overplotting with too many series.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt;  Specialized Modeling &gt; Time Series.\n\n\nSelect your time variable and move it to the X, Time ID column.\n\n\nSelect your response variable and move it to the Y, Time Series  column.\n\n\nClick OK.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a time series plot in R\n        time &lt;- seq(from = as.Date(\"2022-01-01\"), by = \"month\", length.out = 12)\n        values &lt;- rnorm(12, mean = 50, sd = 10)\n        plot(time, values, type = \"l\", main = \"Time Series Plot Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ntime = pd.date_range('2022-01-01', periods=12, freq='M')\nvalues = np.random.normal(50, 10, 12)\nplt.plot(time, values)\nplt.title(\"Time Series Plot Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        TSPLOT\n          /TIME=Date\n          /SERIES=Value\n          /TITLE='Time Series Plot Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 12\n        generate time = tq(2022q1)\n        generate value = rnormal(50, 10)\n        tsset time\n        tsline value, title(\"Time Series Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Visualizes geographic data by plotting locations on maps.\nPros:\n\nProvides spatial context to data.\nEffective for highlighting geographic patterns.\nSupports data aggregation at regional levels.\n\nCons:\n\nInterpretation can be misleading without proper scaling.\nRequires geographic knowledge for accurate interpretation.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your longitude and latitude variables to the X and Y axes, respectively.\n\n\nSelect Map from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a map in R\n        library(ggplot2)\n        library(maps)\n\n        world_map &lt;- map_data(\"world\")\n        ggplot(world_map, aes(x = long, y = lat, group = group)) +\n          geom_polygon(fill = \"lightblue\", color = \"white\") +\n          ggtitle(\"World Map Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.plot()\nplt.title(\"World Map Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        MAP\n          /MAPTYPE=world\n          /TITLE='World Map Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        ssc install spmap\n        clear\n        sysuse world\n        spmap using world_coordinates, title(\"World Map Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Summarizes text data numerically to gain insights.\nPros:\n\nExtracts useful information from textual data.\nIdentifies patterns and trends in large text corpora.\nSupports topic modeling and sentiment analysis.\n\nCons:\n\nRequires preprocessing to handle noise and inconsistencies.\nInterpretation challenges due to language ambiguities.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Text Explorer.\n\n\nSelect your text variable and move it to the Text column.\n\n\nClick OK.\n\n\nClick the red triangle and select  Display Options  &gt;  Show Word Cloud.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Summarize unstructured text in R\n        library(tm)\n        library(wordcloud)\n\n        text &lt;- c(\"word1\", \"word2\", \"word3\", \"word1\", \"word2\")\n        corpus &lt;- Corpus(VectorSource(text))\n        tdm &lt;- TermDocumentMatrix(corpus)\n        word_freq &lt;- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)\n        wordcloud(names(word_freq), word_freq, scale = c(3, 0.5), colors = brewer.pal(8, \"Dark2\"))\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ntext = \"word1 word2 word3 word1 word2\"\nwordcloud = WordCloud(scale=3, max_words=50, background_color='white').generate(text)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        TEXTANALYSIS\n          /TEXT=Comments\n          /TITLE='Unstructured Text Analysis Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 5\n        generate comments = \"word1 word2 word3 word1 word2\"\n        wordcloud comments, maxwords(50)\n      \n\n\n\n\n\n\n\n\nTufte, Edward R. The Visual Display of Quantitative Information. Graphics Press, 2001.\nFew, Stephen. Now You See It: Simple Visualization Techniques for Quantitative Analysis. Analytics Press, 2009."
  },
  {
    "objectID": "Lectures/graphing-your-data.html#identifying-differences-in-types-of-data-and-how-to-graph",
    "href": "Lectures/graphing-your-data.html#identifying-differences-in-types-of-data-and-how-to-graph",
    "title": "Graphing Your Data",
    "section": "",
    "text": "Graphs are crucial in data analysis and presentation because they help:\n\nQuick Interpretation: Graphs provide a quick overview of data trends, making them easier to interpret than tables.\nPattern Identification: They help identify patterns, relationships, and outliers.\nHypothesis Generation: Aids in forming research hypotheses.\nAudience Engagement: Captures and maintains the audience’s attention.\nDecision Support: Facilitates evidence-based decision-making.\n\n\n\n\n\nCategorical Data:\n\nNominal: No inherent order or ranking (e.g., coffee, tea, water).\n\nOrdinal: Has a meaningful order or ranking (e.g., small, medium, large).\n\nPros: Easy to categorize and analyze.\nCons: Limited statistical techniques apply, and numerical differences are not meaningful.\nContinuous Data:\n\nInterval: Has equal intervals between values but no true zero (e.g., temperature, dates).\n\nRatio: Has a meaningful zero point and equal intervals (e.g., weight, strength, pressure).\n\nPros: Wide range of statistical techniques available.\nCons: Requires more complex handling due to scale.\n\n\n\n\n\nUnivariate Graphs:\n\nDefinition: Focuses on a single variable.\nExamples: Histograms, Box plots.\n\nPros:\n\nSimple and intuitive.\nProvides distributional insights.\n\nCons:\n\nLimited to understanding one variable.\n\nBivariate Graphs:\n\nDefinition: Displays the relationship between two variables.\nExamples: Scatter plots, Box plots.\n\nPros:\n\nUseful for studying relationships and trends.\n\nCons:\n\nMight not reveal hidden multivariate trends.\n\nMultivariate Graphs:\n\nDefinition: Involves multiple variables.\nExamples: Scatter plot matrices, Mosaic plots, Treemaps.\n\nPros:\n\nEffective for uncovering complex relationships.\n\nCons:\n\nInterpretation can be challenging without proper labels."
  },
  {
    "objectID": "Lectures/graphing-your-data.html#stephen-fews-graphic-data-display-key-points",
    "href": "Lectures/graphing-your-data.html#stephen-fews-graphic-data-display-key-points",
    "title": "Graphing Your Data",
    "section": "",
    "text": "Know Your Audience: Understand how your audience processes information to tailor your visualizations effectively.\nDefine Your Message: Clearly convey your message simply and quickly.\nUse Visual Elements Appropriately: Combine words, numbers, and images meaningfully.\nPrinciples of Excellence: Based on Edward Tufte’s principles of good graphical design:\n\nEnforce Visual Comparisons: Allow comparisons between data points.\nShow Causality: Highlight causal relationships if possible.\nIntegrate All Visual Elements: Include relevant text and numbers to provide context.\nContent-Driven Design: Ensure design is driven by data quality, relevance, and integrity."
  },
  {
    "objectID": "Lectures/graphing-your-data.html#principles-of-good-and-bad-graphical-design",
    "href": "Lectures/graphing-your-data.html#principles-of-good-and-bad-graphical-design",
    "title": "Graphing Your Data",
    "section": "",
    "text": "Graphical Excellence:\n\nShow Multivariate Data: Present data across multiple dimensions.\nIntegrate All Visual Elements: Include text, numbers, and images cohesively.\nUse Quality, Relevant, and Honest Data: Ensure data integrity and honesty.\n\nCommon Mistakes:\n\nDistorting Data Meaning: Misleading visuals distort interpretation.\nIncorrect Scaling: Scaling errors can misrepresent data trends.\nPoor Data-to-Ink Ratio: Avoid excessive chart junk that doesn’t add value.\n\n\nThe 27 Worst Charts Of All Time"
  },
  {
    "objectID": "Lectures/graphing-your-data.html#graph-types-and-how-to-use-them",
    "href": "Lectures/graphing-your-data.html#graph-types-and-how-to-use-them",
    "title": "Graphing Your Data",
    "section": "",
    "text": "Definition: Graphical representation of data distribution using bins of equal length to count frequencies.\nPros:\n\nHelps visualize the distribution shape.\nEasy to compare different distributions.\nIdentifies outliers and skewness.\n\nCons:\n\nBin size selection can lead to over or under-smoothing.\nNot suitable for small data sets.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Distribution.\n\n\nSelect your continuous variable and move it to the Y column.\n\n\nClick OK.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a histogram in R\n        data &lt;- rnorm(100)\n        hist(data, main = \"Histogram of Data\", xlab = \"Values\", col = \"blue\", border = \"black\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(50, 10, 1000)\nplt.hist(data, bins=15, color='blue')\nplt.title(\"Histogram Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /HISTOGRAM=Age\n          /TITLE='Histogram Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 1000\n        generate age = rnormal(50, 10)\n        hist age, bin(15) color(blue)\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Graphically represents data distribution based on quartiles, highlighting outliers, median, and spread.\nPros:\n\nEffective for identifying outliers.\nCompares multiple groups easily.\nRobust to non-normal data.\n\nCons:\n\nLess informative for small data sets.\nNot ideal for displaying multimodal distributions.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your continuous variable to the Y axis.\n\n\nDrag your categorical variable to the X axis.\n\n\nSelect Box Plot from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a box plot in R\n        data &lt;- rnorm(100)\n        boxplot(data, main = \"Box Plot Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(50, 10, 100)\nplt.boxplot(data)\nplt.title(\"Box Plot Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /BOXPLOT=VARIABLES(Age)\n          /TITLE='Box Plot Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 100\n        generate age = rnormal(50, 10)\n        graph box age, title(\"Box Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays relative proportions in part-to-whole relationships using slices of a circle.\nPros:\n\nVisually intuitive.\nEffective for simple categorical data.\n\nCons:\n\nDifficult to compare proportions across different charts.\nNot suitable for large numbers of categories.\nCan distort differences due to angle perception issues.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your categorical variable to the X axis.\n\n\nSelect Pie Chart from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a pie chart in R\n        values &lt;- c(10, 20, 30, 40)\n        labels &lt;- c(\"A\", \"B\", \"C\", \"D\")\n        pie(values, labels = labels, main = \"Pie Chart Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\n\nvalues = [10, 20, 30, 40]\nlabels = [\"A\", \"B\", \"C\", \"D\"]\nplt.pie(values, labels=labels)\nplt.title(\"Pie Chart Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          EXAMINE VARIABLES=age\n          /COMPARE\n          /PLOT=BOXPLOT\n          /STATISTICS=NONE\n          /NOTOTAL\n          /TITLE='Pie Chart Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 4\n        generate group = _n\n        generate value = 10 * group\n        graph pie value, over(group) title(\"Pie Chart Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Shows the relationship between two continuous variables.\nPros:\n\nIdentifies correlations and relationships between variables.\nHighlights clusters and patterns.\nDetects outliers effectively.\n\nCons:\n\nCan suffer from overplotting with large data sets.\nRequires understanding of correlation interpretation.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your first continuous variable to the X axis.\n\n\nDrag your second continuous variable to the Y axis.\n\n\nSelect Scatter Plot from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a scatter plot in R\n        x &lt;- rnorm(100)\n        y &lt;- rnorm(100)\n        plot(x, y, main = \"Scatter Plot Example\", xlab = \"X-axis\", ylab = \"Y-axis\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.normal(50, 10, 100)\ny = np.random.normal(50, 10, 100)\nplt.scatter(x, y)\nplt.title(\"Scatter Plot Example\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /SCATTERPLOT(BIVAR)=income WITH age\n          /MISSING=LISTWISE.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 100\n        generate age = rnormal(50, 10)\n        generate income = rnormal(50, 10)\n        scatter age income, title(\"Scatter Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays the relationships between two or more categorical variables using a stacked rectangle visualization.\nPros:\n\nEffective for identifying associations in contingency tables.\nHighlights interactions between variables.\n\nCons:\n\nInterpretation can be challenging for complex relationships.\nLess intuitive than simpler visualizations.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Y by X.\n\n\nSelect your response variable and move it to the Y column.\n\n\nSelect your explanatory variable and move it to the X column.\n\n\nClick OK.\n\n\nClick the red triangle and select Mosaic Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a mosaic plot in R\n        library(vcd)\n        data(Titanic)\n        mosaic(Titanic, shade = TRUE, legend = TRUE)\n      \n\n\n\n\n\nPython Instructions:\n\n\n\n        import matplotlib.pyplot as plt\n        from statsmodels.graphics.mosaicplot import mosaic\n\n        data = {'Class A': 40, 'Class B': 30, 'Class C': 20, 'Class D': 10}\n        mosaic(data, title='Mosaic Plot Example')\n        plt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /MOSAICPLOT=VARIABLES(Class, Survival)\n          /TITLE='Mosaic Plot Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 4\n        generate class = _n\n        generate survival = 10 * class\n        graph mosaic class survival, title(\"Mosaic Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: A bar or column chart compares data across categories using rectangular bars, where each bar’s length represents the value or frequency of a category. The bars can be vertical (column chart) or horizontal (bar chart).\nPros:\n\nEasily Compares Data:\n\nMakes it simple to compare data across different categories.\nHighlights significant differences between groups.\n\nQuick Insight:\n\nProvides immediate visual insights into category differences.\nSuitable for presenting data to non-technical audiences.\n\nIntuitive Interpretation:\n\nClear and straightforward representation.\nSuitable for a wide range of audiences due to its intuitive nature.\n\nFlexibility:\n\nCan represent both frequency (count data) and summary statistics (e.g., means, medians).\nCan be customized with different bar colors, stacking, grouping, etc.\n\n\nCons:\n\nMisleading Scaling:\n\nCan mislead if bar lengths, axis scales, or data representations are inconsistent.\nImproper axis truncation may exaggerate or minimize differences.\n\nOverloading with Categories:\n\nToo many categories can clutter the chart, making it hard to interpret.\nLimited space can lead to overlapping labels, hindering readability.\n\nNot Ideal for Continuous Data:\n\nWorks best with categorical data but not suitable for continuous variables.\nSummarizing continuous data into categories can lead to loss of detail or oversimplification.\n\nChart Junk:\n\nExcessive use of gridlines, 3D effects, or non-data elements can lead to “chart junk,” distracting from the data itself.\n\nData-to-Ink Ratio:\n\nLow data-to-ink ratio due to large bars relative to the actual information conveyed.\n\n\n\nSummary:\nBar/column charts are highly effective for comparing categorical data and highlighting category differences. However, careful attention to scaling and data representation is crucial to ensure accurate and clear visualization.\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your categorical variable to the X axis.\n\n\nDrag your continuous variable to the Y axis.\n\n\nSelect Bar Chart from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a bar chart in R\n        categories &lt;- c(\"A\", \"B\", \"C\", \"D\")\n        values &lt;- c(10, 20, 30, 40)\n        barplot(values, names.arg = categories, main = \"Bar Chart Example\", col = \"blue\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\n\ncategories = [\"A\", \"B\", \"C\", \"D\"]\nvalues = [10, 20, 30, 40]\nplt.bar(categories, values, color='blue')\nplt.title(\"Bar Chart Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /BAR(GROUPED)=VALUE BY CATEGORY\n          /TITLE='Bar Chart Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 4\n        generate category = _n\n        generate value = 10 * category\n        graph bar value, over(category) title(\"Bar Chart Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Displays cumulative quantiles of a distribution versus expected quantiles (e.g., normal distribution).\nPros:\n\nIdentifies Distribution Deviations:\n\nClearly shows deviations of data distribution from a theoretical distribution.\n\nEfficient Distribution Fit Determination:\n\nQuickly determines whether the data fits a specific distribution.\n\n\nCons:\n\nRequires Statistical Knowledge:\n\nAccurate interpretation requires understanding quantile statistics.\n\nLess Intuitive:\n\nLess intuitive than histograms or box plots for non-technical audiences.\n\n\n\nSummary:\nQuantile plots are ideal for identifying distribution deviations from theoretical distributions. However, they require statistical knowledge for accurate interpretation and might be less intuitive than other visualization techniques.\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Distribution.\n\n\nSelect your continuous variable and move it to the Y column.\n\n\nClick OK.\n\n\nClick the red triangle and select Normal Quantile Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a quantile plot in R\n        data &lt;- rnorm(100)\n        qqnorm(data)\n        qqline(data, col = \"blue\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\ndata = np.random.normal(50, 10, 100)\nstats.probplot(data, dist=\"norm\", plot=plt)\nplt.title(\"Quantile Plot Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        Q-QPLOT VARIABLES=Age\n          /DISTRIBUTION=NORMAL.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 100\n        generate age = rnormal(50, 10)\n        qnorm age, title(\"Quantile Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Visualizes hierarchical data using nested rectangles.\nPros:\n\nDisplays Large Amounts of Data Efficiently:\n\nSuitable for representing large hierarchical data sets.\n\nIntuitive for Showing Hierarchical Relationships:\n\nClearly shows hierarchical relationships in a visual format.\n\nHighlights Patterns and Clusters Visually:\n\nHighlights patterns, clusters, and outliers effectively.\n\n\nCons:\n\nDistorts Proportions if Sizes are Too Small:\n\nSmaller rectangles can become unreadable and distort proportions.\n\nChallenging to Interpret for High-Depth Hierarchies:\n\nDeep hierarchies can make interpretation challenging and confusing.\n\n\n\nSummary:\nTreemaps are excellent for visualizing hierarchical data efficiently and intuitively. However, careful design is crucial to avoid distortions and ensure interpretability, especially with deep hierarchies.\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your categorical variable to the X axis.\n\n\nDrag your continuous variable to the Y axis.\n\n\nSelect Treemap from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a treemap in R\n        library(treemap)\n        \n        data &lt;- data.frame(\n          category = c(\"A\", \"B\", \"C\", \"D\"),\n          subcategory = c(\"A1\", \"B1\", \"C1\", \"D1\"),\n          value = c(10, 20, 30, 40)\n        )\n        \n        treemap(\n          data,\n          index = c(\"category\", \"subcategory\"),\n          vSize = \"value\",\n          title = \"Treemap Example\"\n        )\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport squarify\n\nlabels = [\"A\", \"B\", \"C\", \"D\"]\nsizes = [10, 20, 30, 40]\nsquarify.plot(sizes=sizes, label=labels, color=[\"blue\", \"green\", \"red\", \"purple\"], alpha=.7)\nplt.title(\"Treemap Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        GRAPH\n          /TREEMAP=VARIABLES(Category, Subcategory)\n          /VALUE=Value\n          /TITLE='Treemap Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        ssc install treemap\n        clear\n        set obs 4\n        generate category = _n\n        generate subcategory = category\n        generate value = 10 * category\n        treemap value, by(category subcategory) title(\"Treemap Example\")"
  },
  {
    "objectID": "Lectures/graphing-your-data.html#advanced-graphs",
    "href": "Lectures/graphing-your-data.html#advanced-graphs",
    "title": "Graphing Your Data",
    "section": "",
    "text": "Definition: Displays data trends over time.\nPros:\n\nIdeal for identifying trends, cycles, and seasonal patterns.\nSupports multiple time series on the same graph.\nFacilitates forecasting and future planning.\n\nCons:\n\nLess effective for non-temporal data.\nCan suffer from overplotting with too many series.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt;  Specialized Modeling &gt; Time Series.\n\n\nSelect your time variable and move it to the X, Time ID column.\n\n\nSelect your response variable and move it to the Y, Time Series  column.\n\n\nClick OK.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a time series plot in R\n        time &lt;- seq(from = as.Date(\"2022-01-01\"), by = \"month\", length.out = 12)\n        values &lt;- rnorm(12, mean = 50, sd = 10)\n        plot(time, values, type = \"l\", main = \"Time Series Plot Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ntime = pd.date_range('2022-01-01', periods=12, freq='M')\nvalues = np.random.normal(50, 10, 12)\nplt.plot(time, values)\nplt.title(\"Time Series Plot Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        TSPLOT\n          /TIME=Date\n          /SERIES=Value\n          /TITLE='Time Series Plot Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 12\n        generate time = tq(2022q1)\n        generate value = rnormal(50, 10)\n        tsset time\n        tsline value, title(\"Time Series Plot Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Visualizes geographic data by plotting locations on maps.\nPros:\n\nProvides spatial context to data.\nEffective for highlighting geographic patterns.\nSupports data aggregation at regional levels.\n\nCons:\n\nInterpretation can be misleading without proper scaling.\nRequires geographic knowledge for accurate interpretation.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nDrag your longitude and latitude variables to the X and Y axes, respectively.\n\n\nSelect Map from the options.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Create a map in R\n        library(ggplot2)\n        library(maps)\n\n        world_map &lt;- map_data(\"world\")\n        ggplot(world_map, aes(x = long, y = lat, group = group)) +\n          geom_polygon(fill = \"lightblue\", color = \"white\") +\n          ggtitle(\"World Map Example\")\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.plot()\nplt.title(\"World Map Example\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        MAP\n          /MAPTYPE=world\n          /TITLE='World Map Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        ssc install spmap\n        clear\n        sysuse world\n        spmap using world_coordinates, title(\"World Map Example\")\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Summarizes text data numerically to gain insights.\nPros:\n\nExtracts useful information from textual data.\nIdentifies patterns and trends in large text corpora.\nSupports topic modeling and sentiment analysis.\n\nCons:\n\nRequires preprocessing to handle noise and inconsistencies.\nInterpretation challenges due to language ambiguities.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Text Explorer.\n\n\nSelect your text variable and move it to the Text column.\n\n\nClick OK.\n\n\nClick the red triangle and select  Display Options  &gt;  Show Word Cloud.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Summarize unstructured text in R\n        library(tm)\n        library(wordcloud)\n\n        text &lt;- c(\"word1\", \"word2\", \"word3\", \"word1\", \"word2\")\n        corpus &lt;- Corpus(VectorSource(text))\n        tdm &lt;- TermDocumentMatrix(corpus)\n        word_freq &lt;- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)\n        wordcloud(names(word_freq), word_freq, scale = c(3, 0.5), colors = brewer.pal(8, \"Dark2\"))\n      \n\n\n\n\n\nPython Instructions:\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ntext = \"word1 word2 word3 word1 word2\"\nwordcloud = WordCloud(scale=3, max_words=50, background_color='white').generate(text)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\n        TEXTANALYSIS\n          /TEXT=Comments\n          /TITLE='Unstructured Text Analysis Example'.\n      \n\n\n\n\n\nSTATA Instructions:\n\n\n\n        clear\n        set obs 5\n        generate comments = \"word1 word2 word3 word1 word2\"\n        wordcloud comments, maxwords(50)\n      \n\n\n\n\n\n\n\n\nTufte, Edward R. The Visual Display of Quantitative Information. Graphics Press, 2001.\nFew, Stephen. Now You See It: Simple Visualization Techniques for Quantitative Analysis. Analytics Press, 2009."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html",
    "href": "Lectures/introduction-to-logistic-regression.html",
    "title": "Introduction to Logistic Regression",
    "section": "",
    "text": "Logistic regression is a fundamental statistical method used to model the relationship between a dependent variable (often binary) and one or more independent variables. It is particularly useful in healthcare and nursing research to understand the influence of various factors on patient outcomes."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#introduction",
    "href": "Lectures/introduction-to-logistic-regression.html#introduction",
    "title": "Introduction to Logistic Regression",
    "section": "",
    "text": "Logistic regression is a fundamental statistical method used to model the relationship between a dependent variable (often binary) and one or more independent variables. It is particularly useful in healthcare and nursing research to understand the influence of various factors on patient outcomes."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#objectives",
    "href": "Lectures/introduction-to-logistic-regression.html#objectives",
    "title": "Introduction to Logistic Regression",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the fundamentals of logistic regression.\nLearn the key differences between logistic and linear regression.\nExplore the logistic function and its interpretation.\nCalculate and interpret odds ratios.\nFormulate the logistic regression equation using maximum likelihood estimation (MLE).\nEstimate and test the significance of the regression coefficients.\nExplore goodness-of-fit tests and the Hosmer-Lemeshow test.\nLearn about stepwise logistic regression.\nCheck assumptions and assess multicollinearity.\nProvide examples in JMP, R, Python, SPSS, and Stata."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#definition-and-purpose-of-logistic-regression",
    "href": "Lectures/introduction-to-logistic-regression.html#definition-and-purpose-of-logistic-regression",
    "title": "Introduction to Logistic Regression",
    "section": "Definition and Purpose of Logistic Regression",
    "text": "Definition and Purpose of Logistic Regression\nLogistic regression is used to predict the probability of a binary outcome (e.g., success/failure, disease/no disease) based on one or more predictor variables. It helps in answering questions like: - What are the chances that a patient has diabetes given their age, BMI, and family history? - How likely is a patient to respond to a specific treatment based on their health metrics?"
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#key-differences-between-logistic-regression-and-linear-regression",
    "href": "Lectures/introduction-to-logistic-regression.html#key-differences-between-logistic-regression-and-linear-regression",
    "title": "Introduction to Logistic Regression",
    "section": "Key Differences Between Logistic Regression and Linear Regression",
    "text": "Key Differences Between Logistic Regression and Linear Regression\n\nOutcome Variable: Logistic regression deals with binary outcomes, while linear regression handles continuous outcomes.\nPrediction Interpretation: Logistic regression outputs probabilities (between 0 and 1) instead of continuous values.\nModel Fit Measure: Logistic regression uses likelihood-based measures like deviance, while linear regression relies on metrics like R-squared."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#understanding-the-logistic-function-and-its-interpretation",
    "href": "Lectures/introduction-to-logistic-regression.html#understanding-the-logistic-function-and-its-interpretation",
    "title": "Introduction to Logistic Regression",
    "section": "Understanding the Logistic Function and Its Interpretation",
    "text": "Understanding the Logistic Function and Its Interpretation\nThe logistic function transforms a linear combination of predictors into probabilities:\n\\[ p = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k}}{1 + e^{\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k}} \\]\nwhere ( p ) is the predicted probability, and ( _i ) are the regression coefficients.\nThe log-odds (logit) transformation:\n\\[ \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k \\]"
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#calculation-and-interpretation-of-odds-ratios",
    "href": "Lectures/introduction-to-logistic-regression.html#calculation-and-interpretation-of-odds-ratios",
    "title": "Introduction to Logistic Regression",
    "section": "Calculation and Interpretation of Odds Ratios",
    "text": "Calculation and Interpretation of Odds Ratios\nAn odds ratio (OR) is a measure of association between an exposure and an outcome. It is interpreted as the odds of the outcome occurring given the presence of a particular predictor.\n\\[ \\text{OR} = \\exp(\\beta_i) \\]\n\nOR &gt; 1: Positive association between predictor and outcome.\nOR &lt; 1: Negative association between predictor and outcome.\nOR = 1: No association."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#formulating-the-logistic-regression-equation-maximum-likelihood-estimation-mle",
    "href": "Lectures/introduction-to-logistic-regression.html#formulating-the-logistic-regression-equation-maximum-likelihood-estimation-mle",
    "title": "Introduction to Logistic Regression",
    "section": "Formulating the Logistic Regression Equation; Maximum Likelihood Estimation (MLE)",
    "text": "Formulating the Logistic Regression Equation; Maximum Likelihood Estimation (MLE)\n\nModel Specification: \\[ \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k \\]\nMaximum Likelihood Estimation (MLE): The logistic regression coefficients are estimated by maximizing the likelihood function: \\[ L(\\beta) = \\prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i} \\] where ( p_i ) is the predicted probability for the ( i )-th observation."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#estimating-the-regression-coefficients",
    "href": "Lectures/introduction-to-logistic-regression.html#estimating-the-regression-coefficients",
    "title": "Introduction to Logistic Regression",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nThe coefficients are estimated using MLE. Most statistical software provides direct estimation."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#testing-the-significance-of-the-regression-coefficients",
    "href": "Lectures/introduction-to-logistic-regression.html#testing-the-significance-of-the-regression-coefficients",
    "title": "Introduction to Logistic Regression",
    "section": "Testing the Significance of the Regression Coefficients",
    "text": "Testing the Significance of the Regression Coefficients\n\nWald Test: Tests individual coefficients.\nLikelihood Ratio Test: Compares nested models.\nScore Test: Similar to the likelihood ratio test."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#confidence-intervals-for-the-coefficients",
    "href": "Lectures/introduction-to-logistic-regression.html#confidence-intervals-for-the-coefficients",
    "title": "Introduction to Logistic Regression",
    "section": "Confidence Intervals for the Coefficients",
    "text": "Confidence Intervals for the Coefficients\nConfidence intervals provide a range of plausible values for the coefficients.\n\\[ CI = \\hat{\\beta_i} \\pm z^* \\cdot SE(\\hat{\\beta_i}) \\]"
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#deviance-and-goodness-of-fit-tests",
    "href": "Lectures/introduction-to-logistic-regression.html#deviance-and-goodness-of-fit-tests",
    "title": "Introduction to Logistic Regression",
    "section": "Deviance and Goodness-of-Fit Tests",
    "text": "Deviance and Goodness-of-Fit Tests\n\nDeviance: Measure of model fit, similar to residual sum of squares.\nLikelihood Ratio Test (Chi-Square): Compares the deviance of nested models.\nHosmer-Lemeshow Test: Assesses model calibration."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#interpretation-of-regression-coefficients-understanding-the-impact-of-predictors-on-odds",
    "href": "Lectures/introduction-to-logistic-regression.html#interpretation-of-regression-coefficients-understanding-the-impact-of-predictors-on-odds",
    "title": "Introduction to Logistic Regression",
    "section": "Interpretation of Regression Coefficients; Understanding the Impact of Predictors on Odds",
    "text": "Interpretation of Regression Coefficients; Understanding the Impact of Predictors on Odds\n\nExample\nAssume a logistic regression model predicting diabetes (yes/no) based on age and BMI:\n\\[ \\log\\left(\\frac{p}{1 - p}\\right) = -5 + 0.05 \\cdot \\text{Age} + 0.2 \\cdot \\text{BMI} \\]\nInterpretation: - Intercept (-5): Baseline log-odds of diabetes for a patient with Age = 0 and BMI = 0. - Age Coefficient (0.05): For each one-year increase in age, the odds of diabetes increase by ( e^{0.05} ) times. - BMI Coefficient (0.2): For each unit increase in BMI, the odds of diabetes increase by ( e^{0.2} ) times."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#stepwise-logistic-regression",
    "href": "Lectures/introduction-to-logistic-regression.html#stepwise-logistic-regression",
    "title": "Introduction to Logistic Regression",
    "section": "Stepwise Logistic Regression",
    "text": "Stepwise Logistic Regression\nStepwise logistic regression involves automatic selection of predictors based on criteria like AIC, BIC, or p-values.\n\nForward Selection: Starts with no variables and adds one at a time.\nBackward Elimination: Starts with all variables and removes one at a time.\nBidirectional Elimination: Combines forward and backward methods."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#interpreting-odds-ratios-for-categorical-and-continuous-predictors",
    "href": "Lectures/introduction-to-logistic-regression.html#interpreting-odds-ratios-for-categorical-and-continuous-predictors",
    "title": "Introduction to Logistic Regression",
    "section": "Interpreting Odds Ratios for Categorical and Continuous Predictors",
    "text": "Interpreting Odds Ratios for Categorical and Continuous Predictors\n\nCategorical Predictors: Interpret the OR relative to the reference category.\nContinuous Predictors: Interpret the OR per unit increase in the predictor."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#checking-assumptions-of-logistic-regression",
    "href": "Lectures/introduction-to-logistic-regression.html#checking-assumptions-of-logistic-regression",
    "title": "Introduction to Logistic Regression",
    "section": "Checking Assumptions of Logistic Regression",
    "text": "Checking Assumptions of Logistic Regression\n\nLinearity in Logit: Ensure continuous predictors are linearly related to the logit.\nAbsence of Multicollinearity: Check variance inflation factors (VIF).\nNo Outliers/Influential Observations: Use Cook’s distance or leverage plots."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#examples-of-logistic-regression-in-nursing-research",
    "href": "Lectures/introduction-to-logistic-regression.html#examples-of-logistic-regression-in-nursing-research",
    "title": "Introduction to Logistic Regression",
    "section": "Examples of Logistic Regression in Nursing Research",
    "text": "Examples of Logistic Regression in Nursing Research\n\nExample 1: Predicting Diabetes\n\n\nR Code Example:\n\n\n\n        # Logistic Regression in R\n        data &lt;- read.csv(\"health_data.csv\")\n        model &lt;- glm(Diabetes ~ Age + BMI, data = data, family = \"binomial\")\n        summary(model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Logistic Regression in Python\n        import pandas as pd\n        import statsmodels.api as sm\n\n        data = pd.read_csv(\"health_data.csv\")\n        X = data[['Age', 'BMI']]\n        X = sm.add_constant(X)\n        y = data['Diabetes']\n\n        model = sm.Logit(y, X)\n        result = model.fit()\n        print(result.summary())\n      \n\n\n\n\n\nStata Code Example:\n\n\n\n        // Logistic Regression in Stata\n        import delimited health_data.csv\n        logit Diabetes Age BMI\n      \n\n\n\n\n\nSPSS Code Example:\n\n\n\n        * Logistic Regression in SPSS.\n        GET DATA /TYPE = TXT /FILE = 'health_data.csv' /DELCASE = LINE /DELIMITERS = \",\".\n        LOGISTIC REGRESSION VARIABLES Diabetes\n        /METHOD = ENTER Age BMI.\n      \n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your binary outcome variable (e.g., Diabetes) and move it to the Y column.\n\n\nSelect your predictor variables (e.g., Age, BMI) and move them to the Construct Model Effects column.\n\n\nChoose Generalized Linear Model and set the Distribution to Binomial with a Logit link function.\n\n\nClick Run.\n\n\n\n\n\nExample 2: Predicting Heart Disease\n\n\nR Code Example:\n\n\n\n        # Logistic Regression in R\n        data &lt;- read.csv(\"heart_data.csv\")\n        model &lt;- glm(HeartDisease ~ Age + Cholesterol + Smoking, data = data, family = \"binomial\")\n        summary(model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Logistic Regression in Python\n        import pandas as pd\n        import statsmodels.api as sm\n\n        data = pd.read_csv(\"heart_data.csv\")\n        X = data[['Age', 'Cholesterol', 'Smoking']]\n        X = sm.add_constant(X)\n        y = data['HeartDisease']\n\n        model = sm.Logit(y, X)\n        result = model.fit()\n        print(result.summary())\n      \n\n\n\n\n\nStata Code Example:\n\n\n\n        // Logistic Regression in Stata\n        import delimited heart_data.csv\n        logit HeartDisease Age Cholesterol Smoking\n      \n\n\n\n\n\nSPSS Code Example:\n\n\n\n        * Logistic Regression in SPSS.\n        GET DATA /TYPE = TXT /FILE = 'heart_data.csv' /DELCASE = LINE /DELIMITERS = \",\".\n        LOGISTIC REGRESSION VARIABLES HeartDisease\n        /METHOD = ENTER Age Cholesterol Smoking.\n      \n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your binary outcome variable (e.g., HeartDisease) and move it to the Y column.\n\n\nSelect your predictor variables (e.g., Age, Cholesterol, Smoking) and move them to the Construct Model Effects  column.\n\n\nChoose Generalized Linear Model and set the Distribution to Binomial with a Logit link function.\n\n\nClick Run."
  },
  {
    "objectID": "Lectures/introduction-to-logistic-regression.html#conclusion",
    "href": "Lectures/introduction-to-logistic-regression.html#conclusion",
    "title": "Introduction to Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nLogistic regression is a crucial tool in healthcare research, providing insights into the relationship between various factors and patient outcomes. Understanding how to interpret coefficients and use different software to fit logistic regression models enables effective decision-making and research."
  },
  {
    "objectID": "Lectures/preliminaries.html",
    "href": "Lectures/preliminaries.html",
    "title": "Preliminaries",
    "section": "",
    "text": "Before conducting statistical analyses or using software, it’s crucial to follow these steps:\n\n\n\nResearch Question: Start with a clear, concise research question relevant to your primary interest.\nHypothesis: Formulate a testable statement that answers your research question.\n\nNull Hypothesis (H0): Represents no effect or difference.\nAlternative Hypothesis (HA): Represents an effect or difference.\n\n\n\n\n\n\nIdentify key variables relevant to your research question.\nChoose between continuous or categorical variables based on the type of data available.\n\nContinuous Variables: Unlimited set of values (e.g., weight, age).\nCategorical Variables: Limited set of values (e.g., treatment status).\n\nOrdinal: Ordered categories (e.g., pain rating - low, medium, high).\nNominal: Unordered categories (e.g., sex - male, female).\nDichotomous: Only two levels.\n\n\n\n\n\n\n\nThe p-value is a statistical measure that helps assess evidence against the null hypothesis.\nPros:\n\nEasy to interpret.\nHelps determine statistical significance.\n\nCons:\n\nOften misinterpreted (e.g., “significant” does not always imply clinical relevance).\nCan be misleading in the presence of small sample sizes.\n\n\n\n\n\n\nSample Size: Ensure that the proposed sample is large enough to answer your research question confidently.\nPower Analysis: Conduct a power analysis to determine the minimum sample size required.\n\nPower: Probability of rejecting a false null hypothesis.\nType I Error (α): Probability of a false positive (usually 0.05).\nType II Error (β): Probability of a false negative.\n\nFactors Influencing Power:\n\nEffect size\nSignificance level\nSample size\n\n\n\n\n\n\nCollect Consistent and High-Quality Data:\n\nUse secure electronic records whenever possible (e.g., REDCap).\nConsult the Institutional Review Board (IRB) before beginning data collection.\nTake note of possible biases or skewed observations that may affect results.\n\nTransform and Organize Data:\n\nRaw Data: Direct data collected from your research.\nTidy Data: Organized and transformed version of raw data.\nApply necessary transformations (e.g., standardization) to clean your data.\n\nConduct Exploratory Data Analysis:\n\nSummarize your data using basic descriptive statistics.\nVisualize your data through various graphs (e.g., histograms, box plots).\nIdentify potential differences and insights within or between groups.\n\n\n\n\n\nFollowing these preliminary steps will ensure a solid foundation for statistical analysis. Carefully consider each step, from hypothesis formulation to data collection and exploration. Being thorough in the preliminaries will enhance the validity and reliability of your findings."
  },
  {
    "objectID": "Lectures/preliminaries.html#preliminary-steps-for-data-analysis",
    "href": "Lectures/preliminaries.html#preliminary-steps-for-data-analysis",
    "title": "Preliminaries",
    "section": "",
    "text": "Before conducting statistical analyses or using software, it’s crucial to follow these steps:\n\n\n\nResearch Question: Start with a clear, concise research question relevant to your primary interest.\nHypothesis: Formulate a testable statement that answers your research question.\n\nNull Hypothesis (H0): Represents no effect or difference.\nAlternative Hypothesis (HA): Represents an effect or difference.\n\n\n\n\n\n\nIdentify key variables relevant to your research question.\nChoose between continuous or categorical variables based on the type of data available.\n\nContinuous Variables: Unlimited set of values (e.g., weight, age).\nCategorical Variables: Limited set of values (e.g., treatment status).\n\nOrdinal: Ordered categories (e.g., pain rating - low, medium, high).\nNominal: Unordered categories (e.g., sex - male, female).\nDichotomous: Only two levels.\n\n\n\n\n\n\n\nThe p-value is a statistical measure that helps assess evidence against the null hypothesis.\nPros:\n\nEasy to interpret.\nHelps determine statistical significance.\n\nCons:\n\nOften misinterpreted (e.g., “significant” does not always imply clinical relevance).\nCan be misleading in the presence of small sample sizes.\n\n\n\n\n\n\nSample Size: Ensure that the proposed sample is large enough to answer your research question confidently.\nPower Analysis: Conduct a power analysis to determine the minimum sample size required.\n\nPower: Probability of rejecting a false null hypothesis.\nType I Error (α): Probability of a false positive (usually 0.05).\nType II Error (β): Probability of a false negative.\n\nFactors Influencing Power:\n\nEffect size\nSignificance level\nSample size\n\n\n\n\n\n\nCollect Consistent and High-Quality Data:\n\nUse secure electronic records whenever possible (e.g., REDCap).\nConsult the Institutional Review Board (IRB) before beginning data collection.\nTake note of possible biases or skewed observations that may affect results.\n\nTransform and Organize Data:\n\nRaw Data: Direct data collected from your research.\nTidy Data: Organized and transformed version of raw data.\nApply necessary transformations (e.g., standardization) to clean your data.\n\nConduct Exploratory Data Analysis:\n\nSummarize your data using basic descriptive statistics.\nVisualize your data through various graphs (e.g., histograms, box plots).\nIdentify potential differences and insights within or between groups.\n\n\n\n\n\nFollowing these preliminary steps will ensure a solid foundation for statistical analysis. Carefully consider each step, from hypothesis formulation to data collection and exploration. Being thorough in the preliminaries will enhance the validity and reliability of your findings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to NPHD9040",
    "section": "",
    "text": "Hello and welcome to NPHD9040 - Applied Multivariable Analysis!\nI’m Dr. Joshua Lambert, and I’ll be your instructor for this course. I’m excited to guide you through this journey as we explore the fascinating world of advanced statistical methods and their application in nursing and healthcare research.\n\n\n\nEmail: joshua.lambert@uc.edu\n\nFeel free to reach out to me with any questions or concerns. I’m here to help you succeed in this course and deepen your understanding of the topics we will cover.\n\n\n\nNPHD9040 is designed to provide an in-depth exploration of advanced statistical methods relevant to healthcare research. The course emphasizes practical applications using statistical software such as JMP and R. Topics include:\n\n\n\nResearch design and hypothesis formulation\nTypes of statistical inference\nUnderstanding variables, sampling, and power calculations\n\n\n\n\n\nPrinciples of graphical excellence\nUnivariate and bivariate graph types\nBest practices for data visualization\n\n\n\n\n\nOne-way ANOVA\nAssumptions, hypothesis testing, and interpretation\n\n\n\n\n\nAnalysis of Covariance\nControlling for covariates and interpreting results\n\n\n\n\n\nTwo-way ANOVA concepts\nInteractions and interpretation\n\n\n\n\n\nAnalyzing repeated measures data\nAssumptions and appropriate tests\n\n\n\n\n\nBasics of linear regression\nAssumptions and model evaluation\n\n\n\n\n\nExpanding linear models to multiple predictors\nInterpretation and model assumptions\n\n\n\n\n\nUnderstanding logistic regression models\nInterpreting odds ratios and diagnostic statistics"
  },
  {
    "objectID": "index.html#how-to-reach-me",
    "href": "index.html#how-to-reach-me",
    "title": "Welcome to NPHD9040",
    "section": "",
    "text": "Email: joshua.lambert@uc.edu\n\nFeel free to reach out to me with any questions or concerns. I’m here to help you succeed in this course and deepen your understanding of the topics we will cover."
  },
  {
    "objectID": "index.html#overview-of-nphd9040",
    "href": "index.html#overview-of-nphd9040",
    "title": "Welcome to NPHD9040",
    "section": "",
    "text": "NPHD9040 is designed to provide an in-depth exploration of advanced statistical methods relevant to healthcare research. The course emphasizes practical applications using statistical software such as JMP and R. Topics include:\n\n\n\nResearch design and hypothesis formulation\nTypes of statistical inference\nUnderstanding variables, sampling, and power calculations\n\n\n\n\n\nPrinciples of graphical excellence\nUnivariate and bivariate graph types\nBest practices for data visualization\n\n\n\n\n\nOne-way ANOVA\nAssumptions, hypothesis testing, and interpretation\n\n\n\n\n\nAnalysis of Covariance\nControlling for covariates and interpreting results\n\n\n\n\n\nTwo-way ANOVA concepts\nInteractions and interpretation\n\n\n\n\n\nAnalyzing repeated measures data\nAssumptions and appropriate tests\n\n\n\n\n\nBasics of linear regression\nAssumptions and model evaluation\n\n\n\n\n\nExpanding linear models to multiple predictors\nInterpretation and model assumptions\n\n\n\n\n\nUnderstanding logistic regression models\nInterpreting odds ratios and diagnostic statistics"
  },
  {
    "objectID": "Other/syl.html",
    "href": "Other/syl.html",
    "title": "NPHD9040 - Syllabus",
    "section": "",
    "text": "NPHD9040 - Applied Multivariable Analysis\n\nClass Information\n\nClass Meeting Time and Location:\n\nTuesdays 9:00 AM - 11:50 AM\nProcter Hall 287 or Online via Microsoft Teams\n\nCredit Hours/Contact Hours:\n\n3 graduate credit hours / 45 contact hours\n\n\n\n\nFaculty Information\n\nFaculty Name:\n\nJoshua W Lambert, PhD, MS, MS\n\nEmail:\n\nJoshua.lambert@uc.edu\n\nStudent First Office Hours:\n\nBy appointment (please email) in person or online via Teams\n\n\n\n\nCourse Pre/Co-Requisites\nTo take this course, you must: - Have taken the following course: NPHD9000 (minimum grade C/Pass)\n\n\nCourse Description\nThis course is the first of two that introduce advanced statistical methods used in doctoral-level nursing research. Method selection, application, results interpretation, and presentation are stressed in a flipped-classroom/workshop format. Methods introduced include partial correlation, multi-way ANOVA, ANCOVA, multiple regression, logistic regression, and multilevel models. An introduction to power and sample size calculation by method is also covered.\n\n\nStudent Learning Outcomes\nUpon completion of the course, students will be able to: 1. Select the appropriate multivariate analysis method based on research objectives and types of variables to be analyzed. 2. Generate required sample size and power for prototypical analyses by multivariate method, concurrently addressing inclusion of under-represented groups in research. 3. Display an introductory level ability to manage data and conduct preliminary analyses to assess and address data quality and potential violations of statistical assumptions by multivariate method. 4. Demonstrate an introductory level ability to generate results, using selected statistical analysis software (e.g., JMP, SPSS, SAS or R), and interpret relevant results generated for each method.\n\n\nTeaching Strategies\n\nWeekly Lectures\nAcademic Methodology Journal Articles\nStatistical Software Workshops\nQuizzes\nGroup Project Proposals\nGroup Presentations\nGroup Papers\nClass Peer Review\n\nActive learning strategies will be used throughout the semester. Examples include faculty and student-led discussions, short paper reviews, creating professional academic documents, and presentations.\nThe textbook readings and additional resources posted in the modules provide background information so you can explore, learn about, discuss, and debate the use of statistical analysis in research.\nGrammar, spelling, and correct statistical formatting are critical components of scholarly written communication and all professional communication. Please include references when referring to the work of others.\n\n\nCourse Activities\nCourse activities will be conducted within the Canvas course management system. All communication should take place through Canvas, email (joshua.lambert@uc.edu), or the Teams page.\n\n\nCourse Format\n\nFlipped-classroom/workshop format\nCombination of lectures, discussions, and practical exercises\n\n\n\nElectronic Communication Policy\n\nAll students are required to register for Canvas for this course. Please check that your correct email address is listed.\nCheck the course site regularly (at least 3 times per week).\nAccess to Microsoft Office files (Word, Excel, Teams) is required.\nYou must check your UC email account regularly, as faculty will use this for communication.\n\n\n\nCourse Materials\n\nResources\nThe main portion of the course content will be through Dr. Lambert’s weekly lecture notes on Canvas. The books listed below are recommended if your preferred learning modality is reading. Lectures will be recorded and posted on Canvas for review.\n\n\nBooks (Optional)\n\nDiscovering Statistics Using IBM SPSS Statistics\n\n4th Edition (Paper) approximately $35 on Amazon\n5th Edition (Paper) approximately $65 on Amazon\n\nNursing Research: Generating and Assessing Evidence for Nursing Practice\n\nTenth, North American Edition (Paper) approximately $60 on Amazon\n\n\n\n\nAdditional Resources\nContact Dr. Lambert if additional resources are needed.\n\n\nWeb-Based Resources\n\nWebsites and videos will be provided as the course progresses\n\n\n\nRequired Technology\n\nMicrosoft Office (Word, Excel, Teams)\nJMP (Free to UC College of Nursing students)\n\nSee Canvas page for download and installation instructions\n\nInternet access\n\n\n\n\nClassroom Procedures/Policies\n\nAcademic Integrity\nThe University Rules, including the Student Code of Conduct, and other documented policies of the department, college, and university related to academic integrity will be enforced. Any violation, including plagiarism or cheating, will be dealt with individually based on severity.\n\nAcademic Integrity\nCode of Conduct\n\n\n\nSpecial Needs and Accommodations Policy\nIf you have special needs related to your participation in the course, meet with the Disability Services Office to arrange reasonable accommodations. Contact them at 513-556-6823 or 210 University Pavilion (main campus).\n\nDisability Services\n\n\n\nAttendance Policy\nActive participation in classroom and online discussions and activities is vital. Plan to spend at least 8-10 hours per week preparing for course activities and completing assignments.\n\nReligious Observances Statement\n\nIf you will be absent, email Dr. Lambert ahead of time.\n\n\nLate Work Policy\nAssignments are due by midnight on the date indicated unless otherwise stated. If you anticipate a conflict with due dates, email Dr. Lambert. Late assignments without prior approval may receive a score as low as 0%.\n\n\nGrade Dispute\nFor grade disputes, first appeal in writing to Dr. Lambert. If not resolved, escalate to the program director, following the university’s grade dispute policy.\n\n\nNetiquette\nMaintain respectful and efficient communication using web-based tools.\n\n\nChanges in Course Syllabus\nThis syllabus is an expected projection of course progress. It may be revised due to circumstances, and changes will be communicated promptly.\n\n\n\nGrading Policy\nThe following scale will be used for grading all course graded activities and deriving the final course grade: - A+: 97-100% - A: 93-96.99% - A-: 90-92.99% - B+: 87-89.99% - B: 83-86.99% - B-: 80-82.99% - C+: 77-79.99% - C: 73-76.99% - C-: 70-72.99% - D+: 67-69.99% - D: 63-66.99% - D-: 60-62.99% - F: Below 60%\nAdditional Notes: - Dr. Lambert may provide additional grading notes during the course."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html",
    "href": "Flashcards/flashcards_anova.html",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is a null hypothesis?\n\n\nClick to reveal answer\n\n\nThe null hypothesis (H₀) in hypothesis testing states that there is no significant difference or relationship between variables.\n\n\n\n\n\nWhat is the purpose of formulating both written and mathematical hypotheses?\n\n\nClick to reveal answer\n\n\nFormulating both written and mathematical hypotheses ensures clarity and precision in stating the research question and allows for statistical testing.\n\n\n\n\n\nWhat is Type 1 Error?\n\n\nClick to reveal answer\n\n\nType 1 Error occurs when the null hypothesis is incorrectly rejected, indicating a significant result when there is no true effect or relationship (false positive).\n\n\n\n\n\nWhat is Type 2 Error?\n\n\nClick to reveal answer\n\n\nType 2 Error occurs when the null hypothesis is incorrectly retained, failing to detect a true effect or relationship (false negative).\n\n\n\n\n\nWhat are the assumptions of ANOVA?\n\n\nClick to reveal answer\n\n\nAssumptions of ANOVA include: normal distribution of the dependent variable, homogeneity of variance, independence of observations, and categorical independent variables.\n\n\n\n\n\nHow do boxplots help in visualizing data distribution?\n\n\nClick to reveal answer\n\n\nBoxplots provide a graphical representation of the distribution of data, showing the median, quartiles, and outliers across different groups or categories.\n\n\n\n\n\nHow are degrees of freedom calculated in ANOVA?\n\n\nClick to reveal answer\n\n\nDegrees of freedom in ANOVA are calculated as the number of independent observations minus the number of parameters estimated from the sample.\n\n\n\n\n\nWhat does the F statistic represent in ANOVA?\n\n\nClick to reveal answer\n\n\nThe F statistic in ANOVA measures the ratio of variance between groups to variance within groups, indicating whether there are significant differences among group means.\n\n\n\n\n\nWhat is the purpose of post-hoc analysis in ANOVA?\n\n\nClick to reveal answer\n\n\nPost-hoc analysis in ANOVA is performed after finding a significant F-statistic to identify which specific groups differ from each other.\n\n\n\n\n\nWhy is homogeneity of variance important in ANOVA?\n\n\nClick to reveal answer\n\n\nHomogeneity of variance ensures that the variability within each group is approximately equal across all groups, which is an assumption for valid ANOVA results.\n\n\n\n\n\nHow is the p-value calculated from the F distribution?\n\n\nClick to reveal answer\n\n\nThe p-value in ANOVA is calculated based on the F statistic and the degrees of freedom, representing the probability of obtaining the observed results under the null hypothesis.\n\n\n\n\n\nWhat are Sum of Squares Between (SSB) and Sum of Squares Within (SSW) in ANOVA?\n\n\nClick to reveal answer\n\n\nSSB represents the variation between groups’ means, while SSW represents the variation within each group.\n\n\n\n\n\nWhat are the null and alternative hypotheses in ANOVA?\n\n\nClick to reveal answer\n\n\nThe null hypothesis (H₀) states that all group means are equal, while the alternative hypothesis (H₁) states that at least one group mean is different.\n\n\n\n\n\nWhat assumptions should be checked before performing ANOVA?\n\n\nClick to reveal answer\n\n\nBefore performing ANOVA, it is essential to check assumptions such as normality of residuals, homogeneity of variance, independence of observations, and linearity of relationships.\n\n\n\n\n\nWhat is the difference between ANOVA and t-tests?\n\n\nClick to reveal answer\n\n\nWhile t-tests are used to compare means between two groups, ANOVA is used to compare means across multiple groups simultaneously.\n\n\n\n\n\nWhy are degrees of freedom important in statistical analysis?\n\n\nClick to reveal answer\n\n\nDegrees of freedom represent the number of independent pieces of information available for estimating statistical parameters, influencing the precision and reliability of statistical tests.\n\n\n\n\n\nHow can ANOVA results be visually represented?\n\n\nClick to reveal answer\n\n\nANOVA results can be visually represented using bar charts, boxplots, or interaction plots to illustrate group differences and statistical significance.\n\n\n\n\n\nWhen should post-hoc tests be conducted in ANOVA?\n\n\nClick to reveal answer\n\n\nPost-hoc tests should be conducted after obtaining a significant ANOVA result to determine which specific group differences contribute to the overall significance.\n\n\n\n\n\nWhat is statistical power in ANOVA?\n\n\nClick to reveal answer\n\n\nStatistical power in ANOVA represents the probability of detecting a true effect or relationship when it exists, minimizing the risk of Type 2 Error."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#understanding-hypothesis-testing",
    "href": "Flashcards/flashcards_anova.html#understanding-hypothesis-testing",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is a null hypothesis?\n\n\nClick to reveal answer\n\n\nThe null hypothesis (H₀) in hypothesis testing states that there is no significant difference or relationship between variables."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#constructing-hypotheses",
    "href": "Flashcards/flashcards_anova.html#constructing-hypotheses",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is the purpose of formulating both written and mathematical hypotheses?\n\n\nClick to reveal answer\n\n\nFormulating both written and mathematical hypotheses ensures clarity and precision in stating the research question and allows for statistical testing."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#type-1-error",
    "href": "Flashcards/flashcards_anova.html#type-1-error",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is Type 1 Error?\n\n\nClick to reveal answer\n\n\nType 1 Error occurs when the null hypothesis is incorrectly rejected, indicating a significant result when there is no true effect or relationship (false positive)."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#type-2-error",
    "href": "Flashcards/flashcards_anova.html#type-2-error",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is Type 2 Error?\n\n\nClick to reveal answer\n\n\nType 2 Error occurs when the null hypothesis is incorrectly retained, failing to detect a true effect or relationship (false negative)."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#assumptions-of-anova",
    "href": "Flashcards/flashcards_anova.html#assumptions-of-anova",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What are the assumptions of ANOVA?\n\n\nClick to reveal answer\n\n\nAssumptions of ANOVA include: normal distribution of the dependent variable, homogeneity of variance, independence of observations, and categorical independent variables."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#visualizing-data-distribution",
    "href": "Flashcards/flashcards_anova.html#visualizing-data-distribution",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "How do boxplots help in visualizing data distribution?\n\n\nClick to reveal answer\n\n\nBoxplots provide a graphical representation of the distribution of data, showing the median, quartiles, and outliers across different groups or categories."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#calculating-degrees-of-freedom",
    "href": "Flashcards/flashcards_anova.html#calculating-degrees-of-freedom",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "How are degrees of freedom calculated in ANOVA?\n\n\nClick to reveal answer\n\n\nDegrees of freedom in ANOVA are calculated as the number of independent observations minus the number of parameters estimated from the sample."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#understanding-the-f-statistic",
    "href": "Flashcards/flashcards_anova.html#understanding-the-f-statistic",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What does the F statistic represent in ANOVA?\n\n\nClick to reveal answer\n\n\nThe F statistic in ANOVA measures the ratio of variance between groups to variance within groups, indicating whether there are significant differences among group means."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#post-hoc-analysis",
    "href": "Flashcards/flashcards_anova.html#post-hoc-analysis",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is the purpose of post-hoc analysis in ANOVA?\n\n\nClick to reveal answer\n\n\nPost-hoc analysis in ANOVA is performed after finding a significant F-statistic to identify which specific groups differ from each other."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#homogeneity-of-variance",
    "href": "Flashcards/flashcards_anova.html#homogeneity-of-variance",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "Why is homogeneity of variance important in ANOVA?\n\n\nClick to reveal answer\n\n\nHomogeneity of variance ensures that the variability within each group is approximately equal across all groups, which is an assumption for valid ANOVA results."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#the-f-distribution",
    "href": "Flashcards/flashcards_anova.html#the-f-distribution",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "How is the p-value calculated from the F distribution?\n\n\nClick to reveal answer\n\n\nThe p-value in ANOVA is calculated based on the F statistic and the degrees of freedom, representing the probability of obtaining the observed results under the null hypothesis."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#sum-of-squares",
    "href": "Flashcards/flashcards_anova.html#sum-of-squares",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What are Sum of Squares Between (SSB) and Sum of Squares Within (SSW) in ANOVA?\n\n\nClick to reveal answer\n\n\nSSB represents the variation between groups’ means, while SSW represents the variation within each group."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#anova-hypotheses",
    "href": "Flashcards/flashcards_anova.html#anova-hypotheses",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What are the null and alternative hypotheses in ANOVA?\n\n\nClick to reveal answer\n\n\nThe null hypothesis (H₀) states that all group means are equal, while the alternative hypothesis (H₁) states that at least one group mean is different."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#anova-assumptions",
    "href": "Flashcards/flashcards_anova.html#anova-assumptions",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What assumptions should be checked before performing ANOVA?\n\n\nClick to reveal answer\n\n\nBefore performing ANOVA, it is essential to check assumptions such as normality of residuals, homogeneity of variance, independence of observations, and linearity of relationships."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#comparing-anova-and-t-tests",
    "href": "Flashcards/flashcards_anova.html#comparing-anova-and-t-tests",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is the difference between ANOVA and t-tests?\n\n\nClick to reveal answer\n\n\nWhile t-tests are used to compare means between two groups, ANOVA is used to compare means across multiple groups simultaneously."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#understanding-degrees-of-freedom",
    "href": "Flashcards/flashcards_anova.html#understanding-degrees-of-freedom",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "Why are degrees of freedom important in statistical analysis?\n\n\nClick to reveal answer\n\n\nDegrees of freedom represent the number of independent pieces of information available for estimating statistical parameters, influencing the precision and reliability of statistical tests."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#visualizing-anova-results",
    "href": "Flashcards/flashcards_anova.html#visualizing-anova-results",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "How can ANOVA results be visually represented?\n\n\nClick to reveal answer\n\n\nANOVA results can be visually represented using bar charts, boxplots, or interaction plots to illustrate group differences and statistical significance."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#understanding-post-hoc-tests",
    "href": "Flashcards/flashcards_anova.html#understanding-post-hoc-tests",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "When should post-hoc tests be conducted in ANOVA?\n\n\nClick to reveal answer\n\n\nPost-hoc tests should be conducted after obtaining a significant ANOVA result to determine which specific group differences contribute to the overall significance."
  },
  {
    "objectID": "Flashcards/flashcards_anova.html#statistical-power-in-anova",
    "href": "Flashcards/flashcards_anova.html#statistical-power-in-anova",
    "title": "Flashcards: ANOVA",
    "section": "",
    "text": "What is statistical power in ANOVA?\n\n\nClick to reveal answer\n\n\nStatistical power in ANOVA represents the probability of detecting a true effect or relationship when it exists, minimizing the risk of Type 2 Error."
  },
  {
    "objectID": "Assignments/hw1-copy.html",
    "href": "Assignments/hw1-copy.html",
    "title": "Homework 1",
    "section": "",
    "text": "Situations and ANOVA Appropriateness\nFor each of the following situations, indicate whether ANOVA is appropriate or not appropriate. Please justify your answer with one to two sentences.\n\nThe independent variable (IV) is age group–people in their 60s, 70s, and 80s; the dependent variable (DV) is health-related hardiness, as measured on a 20-item scale.\nThe IVs are ethnicity (White, African American, Hispanic, and Asian) and birthweight status (&lt; 2500 grams vs. 2500 grams); the DV is serum bilirubin levels.\nThe IV is maternal breastfeeding status (breastfeeds vs. does not breastfeed); the DV is maternal bonding with infant, as measured on a 10-item self-report scale.\nSuppose we wanted to compare the somatic complaints (as measured on a scale known as the Physical Symptom Survey or PSS) of three groups of people: nonsmokers, smokers, and people who recently quit smoking. Using the following data for PSS scores, do a one-way ANOVA to test the hypothesis that the population means are equal:\n\n\n\n\n\n\n\n\n\nNon-Smokers\nSmokers\nQuitters\n\n\n\n\n19\n26\n37\n\n\n23\n29\n32\n\n\n17\n22\n27\n\n\n20\n30\n41\n\n\n26\n23\n38\n\n\n\n- Input these data into JMP. What are the means for the three groups? Conduct an ANOVA and compute the sums of squares, degrees of freedom, and mean squares for these data. What is the value of F? Using an alpha of .05, is the F statistically significant? Summarize this information in both words and in an ANOVA table.\n\nUsing the data from Question 4, compute three t-tests to compare all possible pairs of means with the Bonferroni method with a family-wise α = .05. Which pairs are significantly different from one another, using this multiple comparison procedure? Also complete a contrast for testing if Nonsmokers are different from ever Smoking. What is the Null and Alternative Hypothesis for this test? What is the P-value for this contrast? Compare this result to α = .05. What is the result and interpretation?"
  },
  {
    "objectID": "Assignments/hw1.html",
    "href": "Assignments/hw1.html",
    "title": "Welcome to NPHD9040",
    "section": "",
    "text": "Situations and ANOVA Appropriateness\nFor each of the following situations, indicate whether ANOVA is appropriate or not appropriate. Please justify your answer with one to two sentences.\n\nThe independent variable (IV) is age group–people in their 60s, 70s, and 80s; the dependent variable (DV) is health-related hardiness, as measured on a 20-item scale.\nThe IVs are ethnicity (White, African American, Hispanic, and Asian) and birthweight status (&lt; 2500 grams vs. 2500 grams); the DV is serum bilirubin levels.\nThe IV is maternal breastfeeding status (breastfeeds vs. does not breastfeed); the DV is maternal bonding with infant, as measured on a 10-item self-report scale.\nSuppose we wanted to compare the somatic complaints (as measured on a scale known as the Physical Symptom Survey or PSS) of three groups of people: nonsmokers, smokers, and people who recently quit smoking. Using the following data for PSS scores, do a one-way ANOVA to test the hypothesis that the population means are equal:\n\nInput these data into JMP. What are the means for the three groups? Conduct an ANOVA and compute the sums of squares, degrees of freedom, and mean squares for these data. What is the value of F? Using an alpha of .05, is the F statistically significant? Summarize this information in both words and in an ANOVA table.\n\nUsing the data from Question 4, compute three t-tests to compare all possible pairs of means with the Bonferroni method with a family-wise α = .05. Which pairs are significantly different from one another, using this multiple comparison procedure? Also complete a contrast for testing if Nonsmokers are different from ever Smoking. What is the Null and Alternative Hypothesis for this test? What is the P-value for this contrast? Compare this result to α = .05. What is the result and interpretation?"
  },
  {
    "objectID": "Assignments/HW6.html",
    "href": "Assignments/HW6.html",
    "title": "NPHD9040:Homework 6",
    "section": "",
    "text": "Name:_____________________________\nOver 7,000 participants were surveyed during the 2015-2016 National Health and Nutrition Examination Survey (NHANES) survey collection.  NHANES is a “program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations.” \nA researcher, who just completed Dr. Lambert’s NPHD9040 HW 5 was interested in what factors contribute to health status in general. The researcher knew, that the Dataset used for HW1 had four health status variables. (ph, mh, cc, flu). They wondered what factors would be associated if they combined them into one variable, labeled AnyHealthIssue and used that as the outcome variable. This variable would be a 1 if the subject had any of the four health issues in the last 30 days, and 0 if they hadn’t had any of them. With this new categorical dichotomous outcome variable in hand the knew they would need to do logistic regression. Using this dataset, investigate the relationships between the possible independent variables and the outcome AnyHealthIssue using logistic regression and multiple logistic regression.  Complete (atleast) the following items for your investigation:\n\nDraw a conceptual framework figure on how you think the available variables connect with each other and the outcome.\nMake graphs to investigate the associations.\nThen use logistic regression with each variable separately. Report estimates in table 1 below.\nThen, make a multiple logistic regression mode with the significant variables.  Report estimates in table 2 below.\nHow well did these models fit?\nReport your findings in words as well as other interesting tidbits you found along the way.\nWhat is your statistical conclusion from the multiple logistic regression model about the effect of Diabetes on AnyHealthIssue?\nWhat remained significant from the Table 1 to Table 2?\n\nVariables: ID (unique subject identifier), AnyHealthIssue (Did the subject report one or more of: Physical Health Days in last 30 days, Mental Health Days in last 30 days, Common Cold in the last 30 days, or Flu in last 30 days; 1 for yes to any of them, or 0 for no for all of them), Age (Age of subject, continuous), BMI (Body Mass Index of subject, continuous), IsDiab (Did subject identify as diabetic; 1 for yes, 0 for no), IsMale (Did subject identify as Male; 1 for yes, 0 for no), IsBlack (Did subject identify as black; 1 for yes, 0 for no), ISMAOH (Did subject identify as Mexican American or Other Race; 1 for yes, 0 for no), IsLTHS (Did subject say that they had less than a high school education; 1 for yes, 0 for no), ISBelow1SES (Did subject report a family household income below the poverty line; 1 for yes, 0 for no)."
  },
  {
    "objectID": "Assignments/HW 2 Data Table.html",
    "href": "Assignments/HW 2 Data Table.html",
    "title": "HW 2 Data Table",
    "section": "",
    "text": "Experimental Group\nControl Group\n\n\n\n\nBaseline Data\n53.88\n52.99\n\n\nPost-Intervention Score\n65.23\n57.47"
  },
  {
    "objectID": "Assignments/HW5.html",
    "href": "Assignments/HW5.html",
    "title": "NPHD9040:Homework 5",
    "section": "",
    "text": "Name:_____________________________\nOver 7,000 participants were surveyed during the 2015-2016 National Health and Nutrition Examination Survey (NHANES) survey collection.  NHANES is a “program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations.”  A researcher is interested in understanding how demographic information (such as Sex, Age, Race), and Socio-Economic Status impact blood pressure (systolic, and diastolic) as well as pulse. After exclusion criteria were considered, and missing data removed, a final sample of 6,436 subjects remained in the dataset. You were assigned to help this research explore these relationships. Complete the following steps as well as other plots, tables, or analysis you believe to be necessary. \nVariables: SEQN (unique subject identifier), Sex (Male, or Female), Age (range from 8 to 80), SES (LT1= Below poverty line, GT1= Above poverty line), pulse (range from 36 to 142), systolic blood pressure (ranges from 72 to 236), and diastolic pressure (ranges 0 to 120).\n\nWhat the dependent variables? What types of variables are they?\nWhat are the independent variables? What types of variables are they?\nUsing a statistical software and your knowledge of regression, estimate the effect of age on systolic blood pressure. Interpret the parameter estimate from the regression. What does the residual plot look like? Does the residual plot indicate a good model fit?\nUsing a statistical software and your knowledge of regression, estimate the effect of age on systolic blood pressure while controlling for sex, race, SES, and Pulse. Interpret the parameter estimate for age from the regression. What does the residual plot look like? Does the residual plot indicate a good model fit?\nHow did the parameter estimate for age change from question 3 to 4? Which estimate do you think is more accurate and why? What other considerations did you think of why completing this assignment that might impact the results?"
  },
  {
    "objectID": "Assignments/HW2.html",
    "href": "Assignments/HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "``` [Your answer here] ```\n\n\n\n``` [Your answer here] ```"
  },
  {
    "objectID": "Assignments/HW2.html#what-statistical-procedure-is-most-appropriate-for-the-following-scenario",
    "href": "Assignments/HW2.html#what-statistical-procedure-is-most-appropriate-for-the-following-scenario",
    "title": "Homework 2",
    "section": "",
    "text": "``` [Your answer here] ```\n\n\n\n``` [Your answer here] ```"
  },
  {
    "objectID": "Assignments/HW2.html#suppose-you-were-interested-in-studying-the-effect-of-a-persons-early-retirement-at-or-below-age-62-versus-at-age-65-or-later-on-indicators-of-physical-and-emotional-health.-what-variables-would-you-suggest-using-as-covariates-to-enhance-the-comparability-of-the-groups",
    "href": "Assignments/HW2.html#suppose-you-were-interested-in-studying-the-effect-of-a-persons-early-retirement-at-or-below-age-62-versus-at-age-65-or-later-on-indicators-of-physical-and-emotional-health.-what-variables-would-you-suggest-using-as-covariates-to-enhance-the-comparability-of-the-groups",
    "title": "Homework 2",
    "section": "2. Suppose you were interested in studying the effect of a person’s early retirement (at or below age 62 versus at age 65 or later) on indicators of physical and emotional health. What variables would you suggest using as covariates to enhance the comparability of the groups?",
    "text": "2. Suppose you were interested in studying the effect of a person’s early retirement (at or below age 62 versus at age 65 or later) on indicators of physical and emotional health. What variables would you suggest using as covariates to enhance the comparability of the groups?\n``` [Your answer here] ```"
  },
  {
    "objectID": "Assignments/HW2.html#below-are-some-means-from-a-randomized-controlled-trial.-indicate-at-least-two-ways-to-analyze-the-data-to-test-for-treatment-effects.",
    "href": "Assignments/HW2.html#below-are-some-means-from-a-randomized-controlled-trial.-indicate-at-least-two-ways-to-analyze-the-data-to-test-for-treatment-effects.",
    "title": "Homework 2",
    "section": "3. Below are some means from a randomized controlled trial. Indicate at least two ways to analyze the data to test for treatment effects.",
    "text": "3. Below are some means from a randomized controlled trial. Indicate at least two ways to analyze the data to test for treatment effects.\n\n\n\n\n\n\n\n\n\nExperimental Group\nControl Group\n\n\n\n\nBaseline Data\n53.88\n52.99\n\n\nPost-Intervention Score\n65.23\n57.47\n\n\n\n``` [Your answer here] ```"
  },
  {
    "objectID": "Assignments/HW2.html#a-covariate-in-ancova-is",
    "href": "Assignments/HW2.html#a-covariate-in-ancova-is",
    "title": "Homework 2",
    "section": "4. A covariate in ANCOVA is:",
    "text": "4. A covariate in ANCOVA is:\n\na. Always a pretest measure of the outcome variable\nb. A second dependent variable\nc. An interaction term\nd. A variable the researcher wants to statistically control\n\n``` [Your answer here] ```"
  },
  {
    "objectID": "Assignments/HW2.html#suppose-we-tested-this-belief-by-taking-three-groups-of-participants-and-administering-one-group-with-a-placebo-such-as-a-sugar-pill-one-group-with-a-low-dose-of-viagra-and-one-with-a-high-dose.-the-dependent-variable-was-an-objective-measure-of-libido.-also-included-in-the-file-is-the-partners-libido.-use-the-partners-libido-as-a-covariate-and-conduct-an-ancova-on-whether-there-is-a-group-effect-on-subjects-measured-libido.-the-data-can-be-found-in-the-file-viagracovariate.sav.-please-begin-by-checking-the-assumptions-with-graphstables.-report-those-here.-then-complete-the-analysis-and-follow-up-with-post-hoc-test-when-appropriate.-interpret-any-significant-finds-in-words-and-with-figures-box-plots-and-tables.",
    "href": "Assignments/HW2.html#suppose-we-tested-this-belief-by-taking-three-groups-of-participants-and-administering-one-group-with-a-placebo-such-as-a-sugar-pill-one-group-with-a-low-dose-of-viagra-and-one-with-a-high-dose.-the-dependent-variable-was-an-objective-measure-of-libido.-also-included-in-the-file-is-the-partners-libido.-use-the-partners-libido-as-a-covariate-and-conduct-an-ancova-on-whether-there-is-a-group-effect-on-subjects-measured-libido.-the-data-can-be-found-in-the-file-viagracovariate.sav.-please-begin-by-checking-the-assumptions-with-graphstables.-report-those-here.-then-complete-the-analysis-and-follow-up-with-post-hoc-test-when-appropriate.-interpret-any-significant-finds-in-words-and-with-figures-box-plots-and-tables.",
    "title": "Homework 2",
    "section": "5. Suppose we tested this belief by taking three groups of participants and administering one group with a placebo (such as a sugar pill), one group with a low dose of Viagra and one with a high dose. The dependent variable was an objective measure of libido. Also included in the file is the partners libido. Use the partners libido as a covariate and conduct an ANCOVA on whether there is a group effect on subjects measured libido. The data can be found in the file ViagraCovariate.sav. Please begin by checking the assumptions with graphs/tables. Report those here. Then complete the analysis, and follow up with post-hoc test when appropriate. Interpret any significant finds in words and with figures (box-plots) and tables.",
    "text": "5. Suppose we tested this belief by taking three groups of participants and administering one group with a placebo (such as a sugar pill), one group with a low dose of Viagra and one with a high dose. The dependent variable was an objective measure of libido. Also included in the file is the partners libido. Use the partners libido as a covariate and conduct an ANCOVA on whether there is a group effect on subjects measured libido. The data can be found in the file ViagraCovariate.sav. Please begin by checking the assumptions with graphs/tables. Report those here. Then complete the analysis, and follow up with post-hoc test when appropriate. Interpret any significant finds in words and with figures (box-plots) and tables.\n``` [Your analysis here] ```"
  },
  {
    "objectID": "Assignments/HW3.html",
    "href": "Assignments/HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Grind: Coarseness of the ground beans\nTemperature: Temperature of the water used to brew the co8ee (in degrees Fahrenheit)\nTime: Brewing time in minutes\nCharge: Grams of co8ee beans per ounce of water\nStation: Table at which the co8ee was brewed\nStrength: Number of dissolved solids in the co8ee as measured by a refractometer\n\n\n\n``` [Your plot here] ```\n\n\n\n``` [Your ANOVA results and conclusion here] ```\n\n\n\n``` [Your post-hoc test results here] ```\n\n\n\n``` [Your conclusion here] ```"
  },
  {
    "objectID": "Assignments/HW3.html#use-the-co8ee.jmp-file-to-complete-a-2-way-anova.-the-dependent-variable-will-be-the-strength-of-the-co8ee-independent-variable-1-is-the-grind-of-the-co8ee-and-independent-variable-2-is-the-time-the-co8ee-is-brewed.-here-are-the-descriptions-of-the-variables",
    "href": "Assignments/HW3.html#use-the-co8ee.jmp-file-to-complete-a-2-way-anova.-the-dependent-variable-will-be-the-strength-of-the-co8ee-independent-variable-1-is-the-grind-of-the-co8ee-and-independent-variable-2-is-the-time-the-co8ee-is-brewed.-here-are-the-descriptions-of-the-variables",
    "title": "Homework 3",
    "section": "",
    "text": "Grind: Coarseness of the ground beans\nTemperature: Temperature of the water used to brew the co8ee (in degrees Fahrenheit)\nTime: Brewing time in minutes\nCharge: Grams of co8ee beans per ounce of water\nStation: Table at which the co8ee was brewed\nStrength: Number of dissolved solids in the co8ee as measured by a refractometer\n\n\n\n``` [Your plot here] ```\n\n\n\n``` [Your ANOVA results and conclusion here] ```\n\n\n\n``` [Your post-hoc test results here] ```\n\n\n\n``` [Your conclusion here] ```"
  },
  {
    "objectID": "Assignments/HW3.html#multiple-choice",
    "href": "Assignments/HW3.html#multiple-choice",
    "title": "Homework 3",
    "section": "Multiple Choice",
    "text": "Multiple Choice\n\n2. What is a 2-way ANOVA primarily test for?\n\nA. The effect of one independent variable on a dependent variable.\nB. The effect of two independent variables on a dependent variable and their interaction.\nC. The relationship between two dependent variables.\nD. The effect of a dependent variable on two independent variables.\n\n``` [Your answer here] ```\n\n\n3. Which assumption is unique to repeated measures ANOVA compared to a standard ANOVA?\n\nA. Homogeneity of variances.\nB. Normality.\nC. Sphericity.\nD. Independence of observations.\n\n``` [Your answer here] ```\n\n\n4. In a 2-way ANOVA, what does an interaction effect indicate?\n\nA. The effect of each independent variable is additive.\nB. The effect of one independent variable depends on the level of the other variable.\nC. Both independent variables have no effect on the dependent variable.\nD. The independent variables are correlated with each other.\n\n``` [Your answer here] ```\n\n\n5. What is a potential disadvantage of using repeated measures in an ANOVA?\n\nA. Increased risk of Type II errors.\nB. Decreased statistical power.\nC. Carryover effects.\nD. Increased risk of violating the assumption of independence.\n\n``` [Your answer here] ```\n\n\n6. In the context of 2-way ANOVA, what does the term ‘main effect’ refer to?\n\nA. The combined effect of the two independent variables.\nB. The effect of each independent variable considered separately.\nC. The effect that is common to both independent variables.\nD. The effect that is not accounted for by the independent variables.\n\n``` [Your answer here] ```\n\n\n7. Which of the following is a correct interpretation of a significant interaction in a 2-way ANOVA?\n\nA. The independent variables independently influence the dependent variable.\nB. The effect of one independent variable on the dependent variable changes across levels of the other variable.\nC. The dependent variable is not affected by either of the independent variables.\nD. The interaction is an artifact of measurement error.\n\n``` [Your answer here] ```"
  },
  {
    "objectID": "Assignments/HW4.html",
    "href": "Assignments/HW4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Name:_____________________________\n\nR Squared is known as the:\n\n\n\nCoefficient of determination.\nMultiple correlation coefficient.\nPartial correlation coefficient.\nSemi-partial correlation coefficient.\n\n\n\nWhat is b0 in SLR?\n\n\n\nThe value of the outcome when the independent variable is 0.\nThe relationship between an independent variable and the outcome variable.\nThe value of the independent variable when the outcome is zero.\nThe gradient of the regression line.\n\n\n\n_____ method is used to find the line of best fit.\n\n\n\nFisher’s\nMost minimum R Squared\nOrdinary Least Squares\nModerates\n\n\n\nInput the following data into Excel and import into your favorite statistical software. Conduct a simple linear regression where the response is SBP and the Independent variable is Age. What are the regression coefficients? What is the R Squared for the model?  What is the Null and Alternative Hypothesis for effect of Age? Is age significant? Interpret the effect of age regardless if it is significant.  \n\n\n\n\n\n\n\n\n\nSubject ID\nAge\nSystolic.Blood.Pressure\n\n\n\n\n1\n20\n100\n\n\n2\n21\n110\n\n\n3\n24\n140\n\n\n4\n25\n140\n\n\n5\n26\n125\n\n\n6\n30\n135\n\n\n7\n24\n115"
  },
  {
    "objectID": "Flashcards/flashcards_2wanova.html",
    "href": "Flashcards/flashcards_2wanova.html",
    "title": "Flashcards: Two-Way ANOVA",
    "section": "",
    "text": "What is the purpose of Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nTwo-Way ANOVA evaluates the effects of two independent variables on a continuous dependent variable, including their interaction.\n\n\n\n\nWhat is a main effect in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nA main effect is the impact of one independent variable on the dependent variable, ignoring the levels of the other independent variable.\n\n\n\n\nWhat is an interaction effect in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nAn interaction effect occurs when the combined effect of two independent variables on the dependent variable is different from the sum of their individual effects.\n\n\n\n\nHow many main effects and interaction effects can be analyzed in a Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nTwo-Way ANOVA can analyze two main effects (one for each factor) and one interaction effect between them.\n\n\n\n\nWhat is the null hypothesis for the main effect of Factor A in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nThe null hypothesis ((H_0)) for the main effect of Factor A states that all levels of Factor A have the same mean.\n\n\n\n\nWhat is the alternative hypothesis for the main effect of Factor B in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nThe alternative hypothesis ((H_1)) for the main effect of Factor B states that at least one level of Factor B has a different mean.\n\n\n\n\nWhat is the null hypothesis for the interaction effect in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nThe null hypothesis ((H_0)) for the interaction effect states that there is no interaction between Factors A and B.\n\n\n\n\nWhat are the assumptions of Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\n&lt;ul&gt;\n  &lt;li&gt;Normality of residuals&lt;/li&gt;\n  &lt;li&gt;Homogeneity of variances&lt;/li&gt;\n  &lt;li&gt;Continuous dependent variable&lt;/li&gt;\n  &lt;li&gt;Categorical independent variables&lt;/li&gt;\n  &lt;li&gt;Random sampling&lt;/li&gt;\n&lt;/ul&gt;\n\n\n\n\nWhat is the Sum of Squares for Factor A (SSA)?\n\n\nClick to reveal answer\n\n\n\\[\nSSA = \\sum_{j=1}^{a} n_{j} (\\bar{Y}_{j\\cdot} - \\bar{Y})^2\n\\]\n\n\n\n\nHow do you calculate the Mean Square for Factor B (MSB)?\n\n\nClick to reveal answer\n\n\n\\[\nMSB = \\frac{SSB}{b - 1}\n\\]\n\n\n\n\nWhat is the formula for the F-Ratio of the main effect for Factor A?\n\n\nClick to reveal answer\n\n\n\\[\nF_A = \\frac{MSA}{MSE}\n\\]\n\n\n\n\nWhat does a significant interaction effect imply in a Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nA significant interaction effect indicates that the impact of one factor on the dependent variable differs depending on the levels of the other factor.\n\n\n\n\nWhat is eta squared (( ^2 )) in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nEta squared (( ^2 )) is a measure of the proportion of the total variance explained by a factor or interaction effect.\n\n\n\n\nWhat post-hoc test is commonly used in Two-Way ANOVA to compare specific group means?\n\n\nClick to reveal answer\n\n\nTukey’s Honestly Significant Difference (HSD) test.\n\n\n\n\nHow do you interpret a significant main effect in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nA significant main effect indicates that the means of different levels of one factor are not equal.\n\n\n\n\nWhat graphical method can help identify the presence of interaction effects in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nInteraction plots can help identify the presence of interaction effects by plotting the means of different groups.\n\n\n\n\nWhat does the F-statistic represent in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nThe F-statistic represents the ratio of the variance between groups to the variance within groups.\n\n\n\n\nWhat are the degrees of freedom for the interaction effect in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\n\\[\ndf_{AB} = (a - 1)(b - 1)\n\\]\n\n\n\n\nWhat is the purpose of a residual plot in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nA residual plot helps check for homoscedasticity and outliers by plotting the residuals against fitted values.\n\n\n\n\nWhy is it important to check for influential observations in Two-Way ANOVA?\n\n\nClick to reveal answer\n\n\nInfluential observations can unduly affect the results of Two-Way ANOVA, leading to biased or incorrect conclusions."
  },
  {
    "objectID": "HW3.html",
    "href": "HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Grind: Coarseness of the ground beans\nTemperature: Temperature of the water used to brew the coffee (in degrees Fahrenheit)\nTime: Brewing time in minutes\nCharge: Grams of coffee beans per ounce of water\nStation: Table at which the coffee was brewed\nStrength: Number of dissolved solids in the coffee as measured by a refractometer\n\n\n\n``` [Your plot here] ```\n\n\n\n``` [Your ANOVA results and conclusion here] ```\n\n\n\n``` [Your post-hoc test results here] ```\n\n\n\n``` [Your conclusion here] ```"
  },
  {
    "objectID": "HW3.html#use-the-coffee.jmp-file-to-complete-a-2-way-anova.-the-dependent-variable-will-be-the-strength-of-the-coffee-independent-variable-1-is-the-grind-of-the-coffee-and-independent-variable-2-is-the-time-the-coffee-is-brewed.-here-are-the-descriptions-of-the-variables",
    "href": "HW3.html#use-the-coffee.jmp-file-to-complete-a-2-way-anova.-the-dependent-variable-will-be-the-strength-of-the-coffee-independent-variable-1-is-the-grind-of-the-coffee-and-independent-variable-2-is-the-time-the-coffee-is-brewed.-here-are-the-descriptions-of-the-variables",
    "title": "Homework 3",
    "section": "",
    "text": "Grind: Coarseness of the ground beans\nTemperature: Temperature of the water used to brew the coffee (in degrees Fahrenheit)\nTime: Brewing time in minutes\nCharge: Grams of coffee beans per ounce of water\nStation: Table at which the coffee was brewed\nStrength: Number of dissolved solids in the coffee as measured by a refractometer\n\n\n\n``` [Your plot here] ```\n\n\n\n``` [Your ANOVA results and conclusion here] ```\n\n\n\n``` [Your post-hoc test results here] ```\n\n\n\n``` [Your conclusion here] ```"
  },
  {
    "objectID": "HW3.html#multiple-choice",
    "href": "HW3.html#multiple-choice",
    "title": "Homework 3",
    "section": "Multiple Choice",
    "text": "Multiple Choice\n\n2. What is a 2-way ANOVA primarily test for?\n\nA. The effect of one independent variable on a dependent variable.\nB. The effect of two independent variables on a dependent variable and their interaction.\nC. The relationship between two dependent variables.\nD. The effect of a dependent variable on two independent variables.\n\n``` [Your answer here] ```\n\n\n3. Which assumption is unique to repeated measures ANOVA compared to a standard ANOVA?\n\nA. Homogeneity of variances.\nB. Normality.\nC. Sphericity.\nD. Independence of observations.\n\n``` [Your answer here] ```\n\n\n4. In a 2-way ANOVA, what does an interaction effect indicate?\n\nA. The effect of each independent variable is additive.\nB. The effect of one independent variable depends on the level of the other variable.\nC. Both independent variables have no effect on the dependent variable.\nD. The independent variables are correlated with each other.\n\n``` [Your answer here] ```\n\n\n5. What is a potential disadvantage of using repeated measures in an ANOVA?\n\nA. Increased risk of Type II errors.\nB. Decreased statistical power.\nC. Carryover effects.\nD. Increased risk of violating the assumption of independence.\n\n``` [Your answer here] ```\n\n\n6. In the context of 2-way ANOVA, what does the term ‘main effect’ refer to?\n\nA. The combined effect of the two independent variables.\nB. The effect of each independent variable considered separately.\nC. The effect that is common to both independent variables.\nD. The effect that is not accounted for by the independent variables.\n\n``` [Your answer here] ```\n\n\n7. Which of the following is a correct interpretation of a significant interaction in a 2-way ANOVA?\n\nA. The independent variables independently influence the dependent variable.\nB. The effect of one independent variable on the dependent variable changes across levels of the other variable.\nC. The dependent variable is not affected by either of the independent variables.\nD. The interaction is an artifact of measurement error.\n\n``` [Your answer here] ```"
  },
  {
    "objectID": "Other/aboutDrL.html",
    "href": "Other/aboutDrL.html",
    "title": "About Dr. Joshua Lambert",
    "section": "",
    "text": "Dr. Joshua Lambert holds a Ph.D. in Biostatistics and Epidemiology from the University of Kentucky (2017).\nHe has also earned master’s degrees in Statistics and Mathematics from University of Kentucky and Murray State University.\nCurrently, Dr. Lambert serves as an Associate Professor at the College of Nursing, University of Cincinnati."
  },
  {
    "objectID": "Other/aboutDrL.html#education-and-academic-positions",
    "href": "Other/aboutDrL.html#education-and-academic-positions",
    "title": "About Dr. Joshua Lambert",
    "section": "",
    "text": "Dr. Joshua Lambert holds a Ph.D. in Biostatistics and Epidemiology from the University of Kentucky (2017).\nHe has also earned master’s degrees in Statistics and Mathematics from University of Kentucky and Murray State University.\nCurrently, Dr. Lambert serves as an Associate Professor at the College of Nursing, University of Cincinnati."
  },
  {
    "objectID": "Other/aboutDrL.html#research-support-and-grants",
    "href": "Other/aboutDrL.html#research-support-and-grants",
    "title": "About Dr. Joshua Lambert",
    "section": "Research Support and Grants",
    "text": "Research Support and Grants\n\nDr. Lambert’s research endeavors have been supported by various grants, including prestigious federal and private funding agencies.\nNotably, he serves as Principal Investigator (PI) on projects funded by the National Library of Medicine and Co-Investigator (Co-I) on grants from the National Institute of Diabetes and Digestive and Kidney Diseases, among others.\nHis research spans diverse topics, from identifying clinically protective effects of existing drugs against COVID-19 to exploring outcomes in patients with acute kidney injury.\nMethodologically he works on subset selection for regression models, interaction identification when exhuastive search is not possible, and Pareto Frontier based solutions in Machine Learning and Statistics."
  },
  {
    "objectID": "Other/aboutDrL.html#publications-and-presentations",
    "href": "Other/aboutDrL.html#publications-and-presentations",
    "title": "About Dr. Joshua Lambert",
    "section": "Publications and Presentations",
    "text": "Publications and Presentations\n\nDr. Lambert has authored numerous peer-reviewed publications covering a wide range of topics in healthcare statistics including predictive modeling, opioid exposure, and acute kidney injury.\nHis scholarly work extends beyond publications to include invited presentations at esteemed conferences and institutions and showcasing his expertise and leadership in the field.\nAdditionally, he has contributed technical reports addressing critical public health issues such as drug overdose deaths and suicide attempts in Kentucky.\nResearchGate\nGoogle Scholar\nORCiD"
  },
  {
    "objectID": "Other/aboutDrL.html#teaching-and-academic-engagement",
    "href": "Other/aboutDrL.html#teaching-and-academic-engagement",
    "title": "About Dr. Joshua Lambert",
    "section": "Teaching and Academic Engagement",
    "text": "Teaching and Academic Engagement\n\nIn addition to his research pursuits, Dr. Lambert is actively involved in teaching and mentoring future healthcare professionals.\nHe has held positions as a lecturer in Mathematics and Statistics at various universities, fostering a passion for statistical literacy among students.\nDr. Lambert’s commitment to education is further evidenced by his presentations on statistical methodologies at academic conferences and workshops.\nDr. Lambert teaches NPHD9040 (Multivariate Analysis) and NPHD9042 (Multivariate Analysis) for the PhD program in the College of Nursing at the University of Cincinnati."
  },
  {
    "objectID": "Other/aboutDrL.html#contact-information",
    "href": "Other/aboutDrL.html#contact-information",
    "title": "About Dr. Joshua Lambert",
    "section": "Contact Information",
    "text": "Contact Information\n\nDr. Joshua Lambert can be reached at his academic office in Procter Hall.\nEmail: joshua.lambert@uc.edu"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html",
    "href": "Lectures/introduction-to-multiple-linear-regression.html",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Understand the fundamentals of Multiple Linear Regression\nLearn the definition and purpose of Multiple Linear Regression\nIdentify key concepts and assumptions\nFormulate the regression equation\nEstimate the regression coefficients\nTest the significance of the regression coefficients\nCalculate confidence intervals for the coefficients\nUnderstand the coefficient of determination (R-squared)\nAdjust R-squared and interpret the F-statistic\nInterpret the regression results\nIdentify and handle multicollinearity\nExplore variable selection techniques\nCheck assumptions and perform diagnostics\nExplore examples of Multiple Linear Regression in nursing research\n\n\n\n\nMultiple Linear Regression (MLR) is a statistical technique that models the relationship between two or more independent variables (predictors) and a continuous dependent variable (outcome). It is used to predict the outcome variable based on the values of the independent variables and to understand the impact of each predictor on the outcome.\n\n\n\nLinearity: The relationship between the predictors and the outcome is linear.\nIndependence: Observations are independent of each other.\nHomoscedasticity: The variance of residuals (errors) is constant across all levels of the independent variables.\nNo multicollinearity: Independent variables are not too highly correlated.\nNormality: Residuals should be normally distributed.\n\n\n\n\n\nThe multiple linear regression equation is expressed as: \\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n + \\epsilon\n\\] where: - ( y ): Dependent variable - ( _0 ): Y-intercept - ( _1, _2, , _n ): Coefficients of predictors ( x_1, x_2, , x_n ) - ( ): Random error\n\n\n\nRegression coefficients are estimated using the method of least squares, which minimizes the sum of the squared differences between observed and predicted values.\n\n\n\nCoefficient (( )): \\[\n\\beta = (X^TX)^{-1}X^Ty\n\\]\n\n\n\n\n\nStatistical tests (typically t-tests) are used to determine whether each coefficient significantly differs from zero, indicating a significant relationship between the predictor and the outcome.\n\n\n\\[\nt = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\n\\]\n\n\n\n\nConfidence intervals provide a range of values which are likely to contain the population coefficients.\n\n\n\\[\n\\hat{\\beta} \\pm t^* \\cdot SE(\\hat{\\beta})\n\\]\n\n\n\n\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\n\n\\[\nR^2 = \\frac{\\sum (\\hat{y} - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}\n\\]\n\n\n\n\n\nAdjusted R-squared: Adjusts the R-squared value based on the number of predictors and the sample size to penalize excessive use of predictors. \\[\n\\bar{R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\n\\] where:\n( n ): Sample size\n( k ): Number of predictors\nF-statistic: Tests whether at least one predictor variable has a non-zero coefficient. \\[\nF = \\frac{\\frac{SSR}{k}}{\\frac{SSE}{n - k - 1}}\n\\]\n\n\n\n\nThe coefficients tell you the expected change in the dependent variable for one unit of change in the predictor variable, holding all other predictors constant.\n\n\n\n\n\nMeasures how much the variance of a regression coefficient is inflated due to multicollinearity.\n\n\n\n\\[\nVIF = \\frac{1}{1 - R_j^2}\n\\] where ( R_j^2 ) is the R-squared value obtained by regressing the predictor ( j ) on all other predictors.\n\n\n\n\nRemove highly correlated predictors\nCombine correlated predictors using techniques like PCA\nRegularization methods (e.g., Ridge Regression)\n\n\n\n\n\n\n\nA method to select significant predictors by iteratively adding or removing variables based on their statistical significance.\n\n\n\n\n\n\n\nResidual vs. Fitted Plot: Checks for homoscedasticity and linearity.\nNormal Q-Q Plot: Checks for normality of residuals.\nResidual vs. Leverage Plot: Identifies influential observations.\n\n\n\n\n\nCook’s Distance: Identifies influential data points that affect the regression model.\n\n\n\n\n\n\n\n\nPredictors: Age, Medication adherence, Physiotherapy sessions\nOutcome: Recovery time\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous outcome variable and move it to the Y column.\n\n\nAdd predictors to the model and configure interactions if necessary.\n\n\nClick Run to fit the model and interpret the results.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(456)\ndata &lt;- data.frame(\n  Age = rnorm(100, mean = 50, sd = 10),\n  Medication_Adherence = rnorm(100, mean = 75, sd = 10),\n  Physiotherapy_Sessions = rnorm(100, mean = 20, sd = 5),\n  Recovery_Time = rnorm(100, mean = 30, sd = 5)\n)\n\n# Multiple Linear Regression\nfit &lt;- lm(Recovery_Time ~ Age + Medication_Adherence + Physiotherapy_Sessions, data = data)\nsummary(fit)\n\n# Residual Plots\npar(mfrow = c(2, 2))\nplot(fit)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(456)\ndata = pd.DataFrame({\n  'Age': np.random.normal(50, 10, 100),\n  'Medication_Adherence': np.random.normal(75, 10, 100),\n  'Physiotherapy_Sessions': np.random.normal(20, 5, 100),\n  'Recovery_Time': np.random.normal(30, 5, 100)\n})\n\n# Multiple Linear Regression\nmodel = ols('Recovery_Time ~ Age + Medication_Adherence + Physiotherapy_Sessions', data=data).fit()\nprint(model.summary())\n\n# Residual Plots\nimport matplotlib.pyplot as plt\nfig = sm.graphics.plot_regress_exog(model, 'Age', fig=plt.figure(figsize=(12, 8)))\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Regression &gt; Linear.\n\n\nSelect your continuous outcome variable and move it to the Dependent box.\n\n\nSelect your predictor variables and move them to the Independent(s) box.\n\n\nClick Statistics and check Estimates, R squared change, and Durbin-Watson.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 456\n\n* Simulated data\nset obs 100\ngen Age = rnormal(50, 10)\ngen Medication_Adherence = rnormal(75, 10)\ngen Physiotherapy_Sessions = rnormal(20, 5)\ngen Recovery_Time = rnormal(30, 5)\n\n* Multiple Linear Regression\nregress Recovery_Time Age Medication_Adherence Physiotherapy_Sessions\n      \n\n\n\n\n\n\n\nPredictors: Exercise frequency, Diet quality, Sleep duration\nOutcome: Heart Health Index\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous outcome variable and move it to the Y column.\n\n\nAdd predictors to the model and configure interactions if necessary.\n\n\nClick Run to fit the model and interpret the results.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(789)\ndata2 &lt;- data.frame(\n  Exercise_Frequency = rnorm(100, mean = 3, sd = 1),\n  Diet_Quality = rnorm(100, mean = 7, sd = 2),\n  Sleep_Duration = rnorm(100, mean = 8, sd = 1),\n  Heart_Health_Index = rnorm(100, mean = 75, sd = 10)\n)\n\n# Multiple Linear Regression\nfit2 &lt;- lm(Heart_Health_Index ~ Exercise_Frequency + Diet_Quality + Sleep_Duration, data = data2)\nsummary(fit2)\n\n# Residual Plots\npar(mfrow = c(2, 2))\nplot(fit2)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(789)\ndata2 = pd.DataFrame({\n  'Exercise_Frequency': np.random.normal(3, 1, 100),\n  'Diet_Quality': np.random.normal(7, 2, 100),\n  'Sleep_Duration': np.random.normal(8, 1, 100),\n  'Heart_Health_Index': np.random.normal(75, 10, 100)\n})\n\n# Multiple Linear Regression\nmodel2 = ols('Heart_Health_Index ~ Exercise_Frequency + Diet_Quality + Sleep_Duration', data=data2).fit()\nprint(model2.summary())\n\n# Residual Plots\nimport matplotlib.pyplot as plt\nfig = sm.graphics.plot_regress_exog(model2, 'Diet_Quality', fig=plt.figure(figsize=(12, 8)))\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Regression &gt; Linear.\n\n\nSelect your continuous outcome variable and move it to the Dependent box.\n\n\nSelect your predictor variables and move them to the Independent(s) box。\n\n\nClick Statistics and check Estimates, R squared change, and Durbin-Watson.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 789\n\n* Simulated data\nset obs 100\ngen Exercise_Frequency = rnormal(3, 1)\ngen Diet_Quality = rnormal(7, 2)\ngen Sleep_Duration = rnormal(8, 1)\ngen Heart_Health_Index = rnormal(75, 10)\n\n* Multiple Linear Regression\nregress Heart_Health_Index Exercise_Frequency Diet_Quality Sleep_Duration"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#objectives",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#objectives",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Understand the fundamentals of Multiple Linear Regression\nLearn the definition and purpose of Multiple Linear Regression\nIdentify key concepts and assumptions\nFormulate the regression equation\nEstimate the regression coefficients\nTest the significance of the regression coefficients\nCalculate confidence intervals for the coefficients\nUnderstand the coefficient of determination (R-squared)\nAdjust R-squared and interpret the F-statistic\nInterpret the regression results\nIdentify and handle multicollinearity\nExplore variable selection techniques\nCheck assumptions and perform diagnostics\nExplore examples of Multiple Linear Regression in nursing research"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#definition-and-purpose-of-multiple-linear-regression",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#definition-and-purpose-of-multiple-linear-regression",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Multiple Linear Regression (MLR) is a statistical technique that models the relationship between two or more independent variables (predictors) and a continuous dependent variable (outcome). It is used to predict the outcome variable based on the values of the independent variables and to understand the impact of each predictor on the outcome.\n\n\n\nLinearity: The relationship between the predictors and the outcome is linear.\nIndependence: Observations are independent of each other.\nHomoscedasticity: The variance of residuals (errors) is constant across all levels of the independent variables.\nNo multicollinearity: Independent variables are not too highly correlated.\nNormality: Residuals should be normally distributed."
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#formulating-the-regression-equation",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#formulating-the-regression-equation",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "The multiple linear regression equation is expressed as: \\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n + \\epsilon\n\\] where: - ( y ): Dependent variable - ( _0 ): Y-intercept - ( _1, _2, , _n ): Coefficients of predictors ( x_1, x_2, , x_n ) - ( ): Random error"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#estimating-the-regression-coefficients",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#estimating-the-regression-coefficients",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Regression coefficients are estimated using the method of least squares, which minimizes the sum of the squared differences between observed and predicted values.\n\n\n\nCoefficient (( )): \\[\n\\beta = (X^TX)^{-1}X^Ty\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#testing-the-significance-of-the-regression-coefficients",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#testing-the-significance-of-the-regression-coefficients",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Statistical tests (typically t-tests) are used to determine whether each coefficient significantly differs from zero, indicating a significant relationship between the predictor and the outcome.\n\n\n\\[\nt = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#confidence-intervals-for-the-coefficients",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#confidence-intervals-for-the-coefficients",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Confidence intervals provide a range of values which are likely to contain the population coefficients.\n\n\n\\[\n\\hat{\\beta} \\pm t^* \\cdot SE(\\hat{\\beta})\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#coefficient-of-determination-r-squared",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#coefficient-of-determination-r-squared",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\n\n\\[\nR^2 = \\frac{\\sum (\\hat{y} - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#adjusted-r-squared-and-f-statistic",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#adjusted-r-squared-and-f-statistic",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Adjusted R-squared: Adjusts the R-squared value based on the number of predictors and the sample size to penalize excessive use of predictors. \\[\n\\bar{R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\n\\] where:\n( n ): Sample size\n( k ): Number of predictors\nF-statistic: Tests whether at least one predictor variable has a non-zero coefficient. \\[\nF = \\frac{\\frac{SSR}{k}}{\\frac{SSE}{n - k - 1}}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#interpreting-the-regression-results",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#interpreting-the-regression-results",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "The coefficients tell you the expected change in the dependent variable for one unit of change in the predictor variable, holding all other predictors constant."
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#identifying-and-handling-multicollinearity",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#identifying-and-handling-multicollinearity",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Measures how much the variance of a regression coefficient is inflated due to multicollinearity.\n\n\n\n\\[\nVIF = \\frac{1}{1 - R_j^2}\n\\] where ( R_j^2 ) is the R-squared value obtained by regressing the predictor ( j ) on all other predictors.\n\n\n\n\nRemove highly correlated predictors\nCombine correlated predictors using techniques like PCA\nRegularization methods (e.g., Ridge Regression)"
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#variable-selection-techniques",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#variable-selection-techniques",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "A method to select significant predictors by iteratively adding or removing variables based on their statistical significance."
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#checking-assumptions-and-diagnostics",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#checking-assumptions-and-diagnostics",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Residual vs. Fitted Plot: Checks for homoscedasticity and linearity.\nNormal Q-Q Plot: Checks for normality of residuals.\nResidual vs. Leverage Plot: Identifies influential observations.\n\n\n\n\n\nCook’s Distance: Identifies influential data points that affect the regression model."
  },
  {
    "objectID": "Lectures/introduction-to-multiple-linear-regression.html#examples-of-multiple-linear-regression-in-nursing-research",
    "href": "Lectures/introduction-to-multiple-linear-regression.html#examples-of-multiple-linear-regression-in-nursing-research",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Predictors: Age, Medication adherence, Physiotherapy sessions\nOutcome: Recovery time\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous outcome variable and move it to the Y column.\n\n\nAdd predictors to the model and configure interactions if necessary.\n\n\nClick Run to fit the model and interpret the results.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(456)\ndata &lt;- data.frame(\n  Age = rnorm(100, mean = 50, sd = 10),\n  Medication_Adherence = rnorm(100, mean = 75, sd = 10),\n  Physiotherapy_Sessions = rnorm(100, mean = 20, sd = 5),\n  Recovery_Time = rnorm(100, mean = 30, sd = 5)\n)\n\n# Multiple Linear Regression\nfit &lt;- lm(Recovery_Time ~ Age + Medication_Adherence + Physiotherapy_Sessions, data = data)\nsummary(fit)\n\n# Residual Plots\npar(mfrow = c(2, 2))\nplot(fit)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(456)\ndata = pd.DataFrame({\n  'Age': np.random.normal(50, 10, 100),\n  'Medication_Adherence': np.random.normal(75, 10, 100),\n  'Physiotherapy_Sessions': np.random.normal(20, 5, 100),\n  'Recovery_Time': np.random.normal(30, 5, 100)\n})\n\n# Multiple Linear Regression\nmodel = ols('Recovery_Time ~ Age + Medication_Adherence + Physiotherapy_Sessions', data=data).fit()\nprint(model.summary())\n\n# Residual Plots\nimport matplotlib.pyplot as plt\nfig = sm.graphics.plot_regress_exog(model, 'Age', fig=plt.figure(figsize=(12, 8)))\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Regression &gt; Linear.\n\n\nSelect your continuous outcome variable and move it to the Dependent box.\n\n\nSelect your predictor variables and move them to the Independent(s) box.\n\n\nClick Statistics and check Estimates, R squared change, and Durbin-Watson.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 456\n\n* Simulated data\nset obs 100\ngen Age = rnormal(50, 10)\ngen Medication_Adherence = rnormal(75, 10)\ngen Physiotherapy_Sessions = rnormal(20, 5)\ngen Recovery_Time = rnormal(30, 5)\n\n* Multiple Linear Regression\nregress Recovery_Time Age Medication_Adherence Physiotherapy_Sessions\n      \n\n\n\n\n\n\n\nPredictors: Exercise frequency, Diet quality, Sleep duration\nOutcome: Heart Health Index\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous outcome variable and move it to the Y column.\n\n\nAdd predictors to the model and configure interactions if necessary.\n\n\nClick Run to fit the model and interpret the results.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(789)\ndata2 &lt;- data.frame(\n  Exercise_Frequency = rnorm(100, mean = 3, sd = 1),\n  Diet_Quality = rnorm(100, mean = 7, sd = 2),\n  Sleep_Duration = rnorm(100, mean = 8, sd = 1),\n  Heart_Health_Index = rnorm(100, mean = 75, sd = 10)\n)\n\n# Multiple Linear Regression\nfit2 &lt;- lm(Heart_Health_Index ~ Exercise_Frequency + Diet_Quality + Sleep_Duration, data = data2)\nsummary(fit2)\n\n# Residual Plots\npar(mfrow = c(2, 2))\nplot(fit2)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(789)\ndata2 = pd.DataFrame({\n  'Exercise_Frequency': np.random.normal(3, 1, 100),\n  'Diet_Quality': np.random.normal(7, 2, 100),\n  'Sleep_Duration': np.random.normal(8, 1, 100),\n  'Heart_Health_Index': np.random.normal(75, 10, 100)\n})\n\n# Multiple Linear Regression\nmodel2 = ols('Heart_Health_Index ~ Exercise_Frequency + Diet_Quality + Sleep_Duration', data=data2).fit()\nprint(model2.summary())\n\n# Residual Plots\nimport matplotlib.pyplot as plt\nfig = sm.graphics.plot_regress_exog(model2, 'Diet_Quality', fig=plt.figure(figsize=(12, 8)))\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Regression &gt; Linear.\n\n\nSelect your continuous outcome variable and move it to the Dependent box.\n\n\nSelect your predictor variables and move them to the Independent(s) box。\n\n\nClick Statistics and check Estimates, R squared change, and Durbin-Watson.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 789\n\n* Simulated data\nset obs 100\ngen Exercise_Frequency = rnormal(3, 1)\ngen Diet_Quality = rnormal(7, 2)\ngen Sleep_Duration = rnormal(8, 1)\ngen Heart_Health_Index = rnormal(75, 10)\n\n* Multiple Linear Regression\nregress Heart_Health_Index Exercise_Frequency Diet_Quality Sleep_Duration"
  },
  {
    "objectID": "Lectures/QI_and_EBP.html",
    "href": "Lectures/QI_and_EBP.html",
    "title": "Statistical Q&A for EBP and QI Projects",
    "section": "",
    "text": "This page presents frequently asked statistics questions by students related to QI and EBP final projects. Llinks for the t-test or chi-square. Please email joshua.lambert@uc.edu with more questions."
  },
  {
    "objectID": "Lectures/QI_and_EBP.html#questions",
    "href": "Lectures/QI_and_EBP.html#questions",
    "title": "Statistical Q&A for EBP and QI Projects",
    "section": "Questions",
    "text": "Questions\n\n\nWhat’s the best statistical test to compare CRNA autonomy levels (supervised vs collaborative) between different types of counties (urban vs rural)?\n\nThe Chi-square test is appropriate since you’re comparing two categorical variables: autonomy level (supervised vs. collaborative) and county type (urban vs. rural).\n\nIs it okay to use only descriptive stats for current state data, or do I need inferential tests too?\n\nFor QI projects that just describe the current state (no group comparisons/time points), descriptive statistics are sufficient. Inferential statistics are only needed if you want to generalize or compare groups.\n\nShould I use one Excel sheet or multiple for both CRNA-level and hospital-level data?\n\nUse one Excel file with two tabs (worksheets):\n\nOne for CRNA responses\n\nOne for hospital-level data\nInclude a shared column like “Hospital ID” for linking.\n\n\nDo I need unique identifiers for one-time anonymous surveys?\n\nNo, not for one-time anonymous surveys. But use them if you want to link responses later.\n\nHow many Likert scale questions are recommended per topic for analysis?\n\nAim for 3–5 questions per topic to ensure reliable measurement while keeping the survey short.\n\nTips on generating anonymous but trackable unique identifiers?\n\nUse a self-generated code from personal but non-identifiable info (e.g., first 2 letters of mother’s first name + last 2 digits of birth year + last letter of city born in). Always pilot-test for uniqueness.\n\nWhat statistical tests are suitable for qualitative data?\n\nQualitative data should be analyzed with thematic analysis or coding. Present counts of how often themes appear and use example quotes.\n\nDoes project type affect software choice for statistics?\n\nYes. For QI projects, Excel and online calculators t-test or chi-square are enough. For complex stats, consider software like SPSS or JMP.\n\nIs Chi-square used for analyzing how questionnaire responses affect rates of screenings, diagnoses, or referrals?\n\nYes, when both variables are categorical (e.g., “yes/no” responses and completion of screening).\n\nRecommended YouTube or instructional videos for learning statistics in Excel?\n\n\nLeila Gharani (Excel tutorials)\nStatQuest with Josh Starmer\nUniversity of Cincinnati’s tools (t-test or chi-square)\n\n\nHow to keep unique identifiers in EMR/MyChart QI projects?\n\nDon’t use names or MRNs. Generate a study-specific code, and ensure the exported data is de-identified.\n\nHow to run multiple statistical tests with one data table?\n\nCreate a table with columns for:\n\nVariable or question\nGroup 1 result\nGroup 2 result\nP-value\nUse footnotes to explain test types.\n\n\nHow to choose the right statistical test for mixed yes/no results and average scores in small QI cycles?\n\n\nYes/no or categories: Chi-square or McNemar’s\nAverages: T-test (independent or paired) Use descriptive stats first.\n\n\nFor small data sets, how to draw useful conclusions without power analysis?\n\nFocus on trends or patterns, percent change, or confidence intervals. Describe improvements qualitatively and use graphs.\n\nDealing with missing survey answers or chart audit gaps?\n\nNote the percent missing (surveys). Add a “Data Present (Y/N)” column for audits. Report frequency of missing data for transparency.\n\nHow to verify pre/post quiz score improvements reflect learning, not just question remembering?\n\nUse a Paired t-test for pre/post score comparison. Change question order or use equivalent but different questions to minimize memory bias.\n\nDoes using a self-created Likert scale affect pre/post-test validity and use of paired t-test?\n\nIt may affect validity but you can still use a paired t-test. Report that it’s a new scale and consider pilot testing for clarity.\n\nHow to store data for analyses involving both surveys and simulation scores?\n\nUse one Excel file with separate tabs for surveys and simulation scores. Keep a consistent ID to match participants.\n\nIs it better to store Likert responses as words or numbers?\n\nEither is fine—just be consistent and include a codebook or key to explain.\n\nHow to get consent to add a study at a clinical site?\n\nCheck for IRB approval. Use a brief consent script/form stating:\n\nWho you are\n\nThe study purpose\n\nVoluntary/confidential participation\n\n\nWhat if intervention isn’t allowed at my clinical site?\n\nFocus on current state assessment; consider surveys, interviews, policy comparisons, or education evaluations instead.\n\nBest ways to organize data collection for large projects?\n\nUse Excel with clear headers, each row as one participant. Separate tabs by data type. Label and back up files regularly.\n\nHow to code data for privacy and IRB standards?\n\nRemove all direct identifiers and use non-identifying study codes. Save the ID key separately and securely.\n\nWhen should data collection pause or end for QI projects?\n\nStop when you’ve reached your target, completed a QI cycle, or are seeing consistent, unchanging themes.\n\nBest platforms for data collection and analysis besides Excel?\n\n\nREDCap (best for surveys/audit)\nSurveyMonkey or Google Forms (simple)\nJMP or SPSS for advanced statistics\n\n\nCan email addresses be used as unique identifiers in Google Forms?\n\nNo, emails are not anonymous. Use self-generated codes.\n\nCan question styles be mixed in pre and post surveys?\n\nYes, as long as they are consistent between pre and post and are clearly organized.\n\nIdeal number of questions on pre/post surveys?\n\n5–10 focused questions, grouped by theme, is ideal.\n\nHow to report percentages for screening tool answers? Are further tests needed?\n\nPercentages and bar graphs are fine. For paired pre/post comparisons, consider McNemar’s test.\n\nCan surveys and interviews be used for community needs assessments?\n\nYes. Keep surveys focused and interviews thematically coded; charts or counts can support findings.\n\nAdvice for using surveys in CPG (clinical practice guideline) development?\n\nGather end-user input to tailor the CPG and ensure it reflects their needs.\n\nShould CPG effectiveness be evaluated by patient outcomes or APN feedback?\n\nUse the measure that aligns best with your goals. Patient outcomes show clinical impact; APN input shows usability.\n\nBest practices for pre/post-test question quantity and reliability?\n\nOpt for 5–10 clear, relevant questions for reliable measurement.\n\nHow to use Excel’s Analysis Toolpak vs. a t-test calculator?\n\n\nAnalysis Toolpak: More manual, keeps data organized\n\nCalculator: Faster/easier for one-off tests t-test or chi-square Use whichever is most comfortable and accessible.\n\n\nHow to improve statistical literacy for nurse educators and researchers?\n\nTake formal statistics courses, watch YouTube tutorials (StatQuest, Leila Gharani), and seek out free online materials.\n\nHow do you assign unique identifiers?\n\nEither self-generated codes (using non-identifiable info) or simply sequential IDs (001, 002, etc.).\n\nHow do you determine which software/tool to use for data collection and analysis?\n\nBeginners: Microsoft Forms, SurveyMonkey, Excel.\nAdvanced: Qualtrics, REDCap, JMP, SPSS, R, or Python.\nChoose what fits your skill and time, use tools you already know if short on time.\n\nAny additional advice for choosing pre/post evaluation scales?\n\nUse validated scales with clear scoring and good reported reliability.\n\nShould we calculate a p-value in QI projects with small samples?\n\nYou can, but focus on direction and clinical relevance rather than statistical significance.\n\nBest ways to present statistical data visually?\n\n\nBar charts for categorical comparisons\n\nBox plots for score changes\n\nLine graphs for trends\n\nPie charts for demographics (sparingly)"
  },
  {
    "objectID": "Lectures/introduction-to-ancova.html",
    "href": "Lectures/introduction-to-ancova.html",
    "title": "Introduction to ANCOVA",
    "section": "",
    "text": "Understanding Hypothesis Testing and ANCOVA\n\nDefinition and Purpose of ANCOVA\nAnalysis of Covariance (ANCOVA) is a statistical method that combines ANOVA and regression. It assesses the main and interaction effects of categorical variables on a continuous outcome while controlling for the influence of one or more continuous covariates.\n\n\nKey Differences Between ANCOVA and ANOVA\n\nANOVA examines differences in means between groups without adjusting for other variables.\nANCOVA adjusts group means to account for the effect of covariates.\n\n\n\nAssumptions of ANCOVA\n\nThe dependent variable is measured on an interval or ratio scale.\nThe independent variable is categorical.\nCovariates are measured on an interval or ratio scale.\nRandom sampling.\nNormal distribution of the dependent variable.\nHomogeneity of variance.\nHomogeneity of regression slopes between groups.\n\n\n\nIndependent Variables and Covariates\n\nIndependent Variables: Categorical factors of interest.\nCovariates: Continuous variables used to adjust the dependent variable.\n\n\n\nVisualizing ANCOVA\nVisualizing ANCOVA often involves making line plots to show the trajectory of the independent variable (x-axis) and dependent variable (y-axis) by group (color). One of the assumptions of an ANCOVA is “Homogeneity of regression slopes between groups”. As we can see in this graph, there doesn’t seem to be graphical evidence for that.\n\n\n\n\n\nAs we can see here, this example shows slopes that are much more similar. Thats good because if we are going to use ANCOVA, we are assuming they are the same or atleast very similar.\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your categorical variable as the Model Effects and covariate as Continuous.\n\n\nClick RUN.\n\n\nClick the red triangle and choose Least Squares Fit to visualize.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Example visualization in R\n        library(ggplot2)\n\n        data &lt;- data.frame(\n          group = rep(c(\"Placebo\", \"Behavioral\", \"Drug\"), each = 10),\n          pre_stress = rnorm(30, 5, 1),\n          post_stress = c(rnorm(10, 5, 1), rnorm(10, 7, 1), rnorm(10, 6, 1))\n        )\n\n        # Interaction plot\n        ggplot(data, aes(x = pre_stress, y = post_stress, color = group)) +\n          geom_point() +\n          geom_smooth(method = \"lm\", se = FALSE) +\n          labs(title = \"Interaction Plot\", x = \"Pre-Stress Score\", y = \"Post-Stress Score\")\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example visualization in Python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Generate data\ndata = pd.DataFrame({\n\"group\": np.repeat([\"Placebo\", \"Behavioral\", \"Drug\"], 10),\n\"pre_stress\": np.random.normal(5, 1, 30),\n\"post_stress\": np.concatenate([np.random.normal(5, 1, 10),\nnp.random.normal(7, 1, 10),\nnp.random.normal(6, 1, 10)])\n        })\n\n# Interaction plot\nsns.lmplot(x=\"pre_stress\", y=\"post_stress\", hue=\"group\", data=data, ci=None)\nplt.title(\"Interaction Plot\")\nplt.xlabel(\"Pre-Stress Score\")\nplt.ylabel(\"Post-Stress Score\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your categorical variable as the Fixed Factor and the covariate as Covariate.\n\n\nClick Plots to visualize the interaction.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Example visualization in Stata\n        sysuse auto, clear\n        scatter mpg weight, by(foreign)\n        twoway lfitci mpg weight if foreign == 0, addplot(lfitci mpg weight if foreign == 1)\n      \n\n\n\n\n\nCalculation of Adjusted Means\nAdjusted means, or least-squares means, are averages that account for the influence of covariates. They are particularly useful in analyses like ANCOVA to provide unbiased comparisons across groups by removing the effects of nuisance variables. This adjustment ensures that the differences between groups reflect the effect of the primary variables of interest, not the covariates. Thus, adjusted means enable a fair comparison of group differences after controlling for external influences.\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your categorical variable as the Model Effects and covariate as Continuous.\n\n\nClick OK.\n\n\nClick the red triangle on right side and choose LSMeans Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Adjusted means in R\n        library(car)\n        model &lt;- lm(post_stress ~ group + pre_stress, data = data)\n        Anova(model, type = 3)\n        lsmeans &lt;- emmeans::emmeans(model, \"group\")\n        print(lsmeans)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Adjusted means in Python\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels.stats.anova as anova\n\n# ANCOVA model\nmodel = ols('post_stress ~ group + pre_stress', data=data).fit()\nprint(anova.anova_lm(model, typ=3))\n\n# Adjusted means\nimport statsmodels.stats.multicomp as mc\nmeans = mc.pairwise_tukeyhsd(model.predict(), data['group'])\nprint(means)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your categorical variable as the Fixed Factor and the covariate as Covariate.\n\n\nClick EMMeans and select your categorical variable.\n\n\nClick Continue and OK to generate adjusted means.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Adjusted means in Stata\n        sysuse auto, clear\n        anova mpg foreign weight\n        margins foreign, at(weight=(2000 3000 4000))\n      \n\n\n\n\n\nTesting Main Effects and Investigating the Interaction\nTesting main effects reveals the individual impact of each independent and covariate variable on the outcome. Investigating interaction effects uncovers how combinations of these variables jointly influence the outcome. Together, they provide a comprehensive understanding of the relationships within the data, guiding more accurate interpretations and conclusions about our ANCOVA result.\n\nMain Effects: The impact of each independent and covariate variable on the outcome.\nInteraction Effects: How combinations of independent and covariate variables influence the outcome.\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your categorical variable as the Model Effects and covariate as Continuous.\n\n\nClick OK.\n\n\nClick the red triangle and choose LSMeans Plot and Interaction Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Testing main effects and interaction in R\n        model &lt;- lm(post_stress ~ group * pre_stress, data = data)\n        Anova(model, type = 3)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Testing main effects and interaction in Python\n        model = ols('post_stress ~ group * pre_stress', data=data).fit()\n        print(anova.anova_lm(model, typ=3))\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your categorical variable as the Fixed Factor and the covariate as Covariate.\n\n\nClick Plots to generate an Interaction Plot.\n\n\nClick OK to view the results.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Testing main effects and interaction in Stata\n        sysuse auto, clear\n        anova mpg foreign weight foreign#weight\n      \n\n\n\n\n\nEffect Sizes and Significance, F Test, SSC, SSB, SSW, P-value\n\nF Test: Determines if group means are significantly different.\nSSB (Sum of Squares Between): Measures variation between groups.\nSSC (Sum of Squares Covariate): Measures the variation explained by the covariate.\nSSW (Sum of Squares Within): Measures variation within groups.\nP-value: Indicates statistical significance.\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your categorical variable as the Model Effects and covariate as Continuous.\n\n\nClick OK.\n\n\nClick the red triangle and choose Effect Tests.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Effect sizes and significance in R\n        model &lt;- lm(post_stress ~ group + pre_stress, data = data)\n        Anova(model, type = 3)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Effect sizes and significance in Python\n        model = ols('post_stress ~ group + pre_stress', data=data).fit()\n        print(anova.anova_lm(model, typ=3))\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your categorical variable as the Fixed Factor and the covariate as Covariate.\n\n\nClick OK to view the results.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Effect sizes and significance in Stata\n        sysuse auto, clear\n        anova mpg foreign weight\n      \n\n\n\n\n\nExamples of ANCOVA in Nursing Research\n\nPatient Recovery Rates: Comparing patient recovery rates across different treatment groups while controlling for age and baseline health status.\nStress Reduction Programs: Evaluating the effectiveness of stress reduction programs while controlling for initial stress levels.\n\n\n\nHands-on Example: Interpreting Your Results\ndownload ex. 1 data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your categorical variable as the Model Effects and covariate as Continuous.\n\n\nClick OK.\n\n\nClick the red triangle and choose Effect Tests and LSMeans Plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Example ANCOVA in R\n        model &lt;- lm(post_stress ~ group + pre_stress, data = data)\n        summary(model)\n        Anova(model, type = 3)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n# Example ANCOVA in Python\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\nmodel = ols('post_stress ~ group + pre_stress', data=data).fit()\nprint(model.summary())\nprint(anova_lm(model, typ=3))\n        \n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Univariate.\n\n\nSelect your categorical variable as the Fixed Factor and the covariate as Covariate.\n\n\nClick OK to generate the ANCOVA results.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Example ANCOVA in Stata\n        sysuse auto, clear\n        anova mpg foreign weight"
  },
  {
    "objectID": "Lectures/introduction-to-anova.html",
    "href": "Lectures/introduction-to-anova.html",
    "title": "Introduction to ANOVA",
    "section": "",
    "text": "Understanding Hypothesis Testing and ANOVA\n\nConstructing a Written and Mathematical Hypothesis\nWhen constructing a hypothesis for ANOVA, it’s important to formulate it both in a written and mathematical format.\n\nWritten Hypothesis (Example 1):\n\nNull Hypothesis (H₀): The average stress in cardiac patients is not different between a placebo group and a behavioral intervention group.\nAlternative Hypothesis (H₁): The average stress in cardiac patients differs between the placebo and behavioral intervention groups.\n\nMathematical Hypothesis (Example 1): \\[ H_{0}: \\mu_{placebo} = \\mu_{behavior}\\]\n\n\\[H_{1}: \\mu_{placebo} \\neq \\mu_{behavior}\\]\n\n\nT-test vs. ANOVA\nWhen comparing means between two groups, the t-test and ANOVA will yield the same results. However, ANOVA extends to cases with more than two groups.\n\n\nWriting ANOVA Hypothesis, Type 1 and 2 Errors\nIn ANOVA, the hypotheses are:\n\nNull Hypothesis (H₀): All group means are equal.\nAlternative Hypothesis (H₁): At least one group mean is different.\n\nType 1 Error: Rejecting the null hypothesis when it is true (false positive).\nType 2 Error: Failing to reject the null hypothesis when it is false (false negative).\n\n\nAssumptions of ANOVA\n\nThe dependent variable is measured on an interval or ratio scale.\nThe independent variable is categorical.\nRandom sampling.\nNormal distribution of the dependent variable.\nHomogeneity of variance.\n\n\n\n\n\nVisualizing Using Boxplots\nBoxplots are essential for visualizing data distribution across groups in ANOVA.\n\n\nJMP Instructions:\n\n\n\nGo to Graph &gt; Graph Builder.\n\n\nSelect your categorical variable for the x-axis and your continuous variable for the y-axis.\n\n\nClick Box Plot in the ribbon to display.\n\n\nClick Done to finalize.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Example boxplot in R\n        data &lt;- data.frame(\n          group = rep(c(\"Placebo\", \"Behavioral\", \"Drug\"), each = 10),\n          stress = c(rnorm(10, 5, 1), rnorm(10, 7, 1), rnorm(10, 6, 1))\n        )\n        boxplot(stress ~ group, data = data, main = \"Boxplot of Stress Scores\",\n                xlab = \"Group\", ylab = \"Stress Score\")\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n  # Example boxplot in Python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n        # Generate data\ndata = pd.DataFrame({\n\"group\": np.repeat([\"Placebo\", \"Behavioral\", \"Drug\"], 10),\n\"stress\": np.concatenate([np.random.normal(5, 1, 10),\nnp.random.normal(7, 1, 10),\nnp.random.normal(6, 1, 10)])\n        })\n\n        # Boxplot\nsns.boxplot(x=\"group\", y=\"stress\", data=data)\nplt.title(\"Boxplot of Stress Scores\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Stress Score\")\nplt.show()\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Descriptive Statistics &gt; Explore.\n\n\nSelect your categorical variable as the factor and continuous variable as the dependent list.\n\n\nClick Plots, check Boxplots, and click Continue.\n\n\nClick OK to generate the boxplot.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Example boxplot in Stata\n        sysuse auto, clear\n        graph box price, over(foreign)\n      \n\n\n\n\n\nCalculating Variation and Degrees of Freedom\n\nSum of Squares Between (SSB): Variation between groups.\nSum of Squares Within (SSW): Variation within groups.\nTotal Sum of Squares (SST): Total variation.\n\n\nDegrees of Freedom:\n\n\\[df_{between} = k - 1\\] (where k is the number of groups)\n\\[df_{within} = N - k\\] (where N is the total sample size)\n\n\n\n\nThe F Statistic and P-Value\nThe F statistic is calculated using the ratio of mean squares:\n\\[\nF = \\frac{MSB}{MSW}\n\\] - MSB: Mean Square Between - MSW: Mean Square Within\nThe p-value is derived from the F distribution, given the degrees of freedom.\nThe F-distribution is a continuous probability distribution that arises frequently as the null distribution of test statistics, particularly in analysis of variance (ANOVA). It is characterized by two parameters: the degrees of freedom of the numerator (df_between) and the degrees of freedom of the denominator (df_within).\n\n\n\n\nHands-on Example: Interpreting Results\ndownload ex. 1 data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Y by X.\n\n\nSelect the categorical variable as the x-axis and continuous variable as the y-axis.\n\n\nClick OK.\n\n\nClick the red triangle and choose Means/ANOVA to view the results.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Example ANOVA in R\n        model &lt;- aov(stress ~ group, data = data)\n        summary(model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n        # Example ANOVA in Python\n        import statsmodels.api as sm\n        from statsmodels.formula.api import ols\n\n        # ANOVA model\n        model = ols('stress ~ group', data=data).fit()\n        anova_table = sm.stats.anova_lm(model, typ=2)\n        print(anova_table)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Compare Means &gt; One-Way ANOVA.\n\n\nSelect the categorical variable as the factor and continuous variable as the dependent list.\n\n\nClick Post Hoc, choose Tukey, and click Continue.\n\n\nClick OK to view the results.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Example ANOVA in Stata\n        oneway stress group, bonferroni\n      \n\n\n\n\n\nPost-hoc Analysis\nPost-hoc analysis is essential in ANOVA to identify specific group differences following the detection of a significant F-statistic. This analysis involves comparing all possible pairs of group means to determine which differences are statistically significant. Various post-hoc tests, such as Tukey’s HSD or Bonferroni correction, are employed to control for multiple comparisons and mitigate the risk of Type I errors. Careful consideration of the study design and interpretation of post-hoc test results are crucial for drawing meaningful conclusions about group differences.\n\nTukey HSD vs Bonferroni\nTukey’s Honestly Significant Difference (HSD) test and the Bonferroni correction are both methods used in multiple comparison procedures, particularly in analysis of variance (ANOVA). Tukey’s HSD test compares all possible pairwise differences between group means while controlling the overall Type I error rate. It tends to be more powerful than Bonferroni but might be less conservative. The Bonferroni correction adjusts the significance level for each individual comparison to maintain a desired family-wise error rate, making it more conservative but simpler to apply. The choice between the two depends on factors like sample size, number of comparisons, and the balance between Type I and Type II error risks. Ultimately, researchers should consider the specific context and assumptions to determine the most appropriate method.\n\n\n\n\nJMP Instructions:\n\n\n\nClick the red triangle next to your ANOVA results.\n\n\nSelect Tukey HSD under Compare Means.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Tukey HSD in R\n        tukey &lt;- TukeyHSD(model)\n        print(tukey)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Tukey HSD in Python\n        from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n        # Tukey HSD\n        tukey = pairwise_tukeyhsd(data['stress'], data['group'])\n        print(tukey)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Compare Means &gt; One-Way ANOVA.\n\n\nClick Post Hoc, choose Tukey, and click Continue.\n\n\nClick OK to view the results.\n\n\n\n\n\nStata Code Example:\n\n\n\n        // Tukey HSD in Stata\n        oneway stress group, bonferroni"
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html",
    "href": "Lectures/introduction-to-repeated-measures-anova.html",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Understand the fundamentals of Repeated Measures ANOVA\nLearn the definition and purpose of Repeated Measures ANOVA\nIdentify the key differences between Repeated Measures ANOVA and One-Way ANOVA\nUnderstand the structure of a Repeated Measures design\nRecognize within-subjects factors and levels\nComprehend the sphericity assumption\nUnderstand the Greenhouse-Geisser correction\nCalculate Sum of Squares (SS) and Mean Squares (MS)\nConduct hypothesis testing for main effects and interaction\nMeasure effect sizes and significance\nInterpret main effects and interaction\nApply the Bonferroni correction\nCheck the assumptions of Repeated Measures ANOVA\nExplore examples of Repeated Measures ANOVA in nursing research\n\n\n\n\nRepeated Measures ANOVA is a statistical test used to examine the changes in a continuous dependent variable across multiple measurements over time or under different conditions. It is an extension of One-Way ANOVA but accounts for the correlation between repeated measures on the same subjects.\n\n\n\nOne-Way ANOVA: Evaluates the effect of a single independent variable on a continuous outcome.\nRepeated Measures ANOVA: Evaluates the effect of a single or multiple within-subject factors over time, accounting for the correlation between repeated measurements.\n\n\n\n\n\nIn a Repeated Measures design, each participant is measured multiple times under different conditions or over time. The design includes: - Within-Subjects Factors: Variables measured repeatedly on the same subjects. - Levels: The different conditions or time points for each within-subjects factor.\n\n\n\nWithin-Subjects Factor: Time\nLevels: Baseline, Week 1, Week 2, Week 3\n\n\n\n\n\nSphericity refers to the assumption that the variances of the differences between all combinations of related groups are equal. If violated, it can lead to biased F-statistics and increased Type I errors.\n\n\nThe Greenhouse-Geisser correction adjusts the degrees of freedom to correct for violations of sphericity, providing a more conservative estimate.\n\n\n\n\n\n\n\nTotal Sum of Squares (SST): \\[\nSST = \\sum_{i=1}^{n} \\sum_{j=1}^{t} (Y_{ij} - \\bar{Y})^2\n\\] where ( Y_{ij} ) is each individual observation, and ( {Y} ) is the overall mean.\nSum of Squares Within (SSW): \\[\nSSW = \\sum_{i=1}^{n} \\sum_{j=1}^{t} (Y_{ij} - \\bar{Y}_{i\\cdot})^2\n\\] where ( {Y}_{i} ) is the mean of all measurements for subject ( i ).\nSum of Squares Between (SSB): \\[\nSSB = \\sum_{j=1}^{t} n_j (\\bar{Y}_{\\cdot j} - \\bar{Y})^2\n\\] where ( {Y}_{j} ) is the mean for level ( j ), and ( n_j ) is the number of observations at level ( j ).\n\n\n\n\n\nMean Square Within (MSW): \\[\nMSW = \\frac{SSW}{(n - 1)(t - 1)}\n\\]\nMean Square Between (MSB): \\[\nMSB = \\frac{SSB}{t - 1}\n\\]\n\n\n\n\n\n\n\n\nMain Effect of Within-Subjects Factor:\n\nNull Hypothesis (( H_0 )): No difference in the means across levels.\nAlternative Hypothesis (( H_1 )): At least one level has a different mean.\n\nInteraction Effect:\n\nNull Hypothesis (( H_0 )): No interaction between within-subjects factors.\nAlternative Hypothesis (( H_1 )): An interaction exists between factors.\n\n\n\n\n\n\nMain Effect of Within-Subjects Factor: \\[\nF = \\frac{MSB}{MSW}\n\\]\n\n\n\n\n\nCompare the F-ratio to the critical value from the F-distribution.\nIf the calculated F-ratio is greater than the critical value, reject the null hypothesis.\n\n\n\n\n\n\n\nRepresents the proportion of the total variance explained by the within-subjects factor.\n\nWithin-Subjects Factor: \\[\n\\eta^2 = \\frac{SSB}{SST}\n\\]\n\n\n\n\n\nA post-hoc correction used to reduce Type I errors when multiple comparisons are made.\n\n\n\\[\n\\alpha_{\\text{corrected}} = \\frac{\\alpha}{m}\n\\] where ( m ) is the number of comparisons.\n\n\n\n\n\n\n\nUse Mauchly’s test to check the assumption of sphericity.\n\n\n\n\n\nUse Q-Q plots to check the normality of residuals.\n\n\n\n\n\nEnsure variances are approximately equal across conditions.\n\n\n\n\n\n\n\n\nResearch Question: How does blood pressure change over a 3-week intervention period?\nWithin-Subjects Factor: Time\nLevels: Baseline, Week 1, Week 3\nResponse Variable: Blood Pressure\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Repeated Structure, select Time as the repeated factor.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(123)\ndata &lt;- data.frame(\n  Subject = rep(1:20, each = 3),\n  Time = factor(rep(c(\"Baseline\", \"Week1\", \"Week3\"), times = 20)),\n  BP = rnorm(60, mean = rep(c(120, 115, 110), each = 20), sd = 10)\n)\n\n# Repeated Measures ANOVA\ninstall.packages(\"ez\")\nlibrary(ez)\nanova_results &lt;- ezANOVA(\n  data = data,\n  dv = BP,\n  wid = Subject,\n  within = Time\n)\nprint(anova_results)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(123)\ndata = pd.DataFrame({\n  'Subject': pd.Categorical([str(i) for i in range(20)] * 3),\n  'Time': pd.Categorical(['Baseline', 'Week1', 'Week3'] * 20),\n  'BP': np.random.normal(loc=115, scale=10, size=60)\n})\n\n# Repeated Measures ANOVA\nmodel = ols('BP ~ C(Time) + C(Subject)', data=data).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\nprint(anova_table)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Repeated Measures.\n\n\nDefine your within-subjects factor (e.g., Time) and specify the number of levels.\n\n\nSelect your continuous dependent variable.\n\n\nClick Options and specify Descriptive Statistics.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 123\n\n* Simulated data\nset obs 60\ngen Subject = cond(mod(_n, 3) == 1, \"1\", cond(mod\n\n(_n, 3) == 2, \"2\", \"3\"))\ngen Time = cond(mod(_n, 3) == 1, \"Baseline\", cond(mod(_n, 3) == 2, \"Week1\", \"Week3\"))\ngen BP = rnormal(cond(mod(_n, 3) == 1, 120, cond(mod(_n, 3) == 2, 115, 110)), 10)\n\n* Repeated Measures ANOVA\nanova BP Time|Subject\n      \n\n\n\n\n\n\n\nResearch Question: How does cognitive function change across four training sessions?\nWithin-Subjects Factor: Session\nLevels: Session 1, Session 2, Session 3, Session 4\nResponse Variable: Cognitive Function Score\n\ndownload example 2 data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Repeated Structure, select Session as the repeated factor.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(456)\ndata2 &lt;- data.frame(\n  Subject = rep(1:15, each = 4),\n  Session = factor(rep(c(\"Session1\", \"Session2\", \"Session3\", \"Session4\"), times = 15)),\n  Score = rnorm(60, mean = rep(c(80, 85, 90, 95), each = 15), sd = 5)\n)\n\n# Repeated Measures ANOVA\nlibrary(ez)\nanova_results2 &lt;- ezANOVA(\n  data = data2,\n  dv = Score,\n  wid = Subject,\n  within = Session\n)\nprint(anova_results2)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(456)\ndata2 = pd.DataFrame({\n  'Subject': pd.Categorical([str(i) for i in range(15)] * 4),\n  'Session': pd.Categorical(['Session1', 'Session2', 'Session3', 'Session4'] * 15),\n  'Score': np.random.normal(loc=[80, 85, 90, 95], scale=5, size=60)\n})\n\n# Repeated Measures ANOVA\nmodel2 = ols('Score ~ C(Session) + C(Subject)', data=data2).fit()\nanova_table2 = sm.stats.anova_lm(model2, typ=2)\nprint(anova_table2)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Repeated Measures.\n\n\nDefine your within-subjects factor (e.g., Session) and specify the number of levels.\n\n\nSelect your continuous dependent variable.\n\n\nClick Options and specify Descriptive Statistics.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 456\n\n* Simulated data\nset obs 60\ngen Subject = cond(mod(_n, 4) == 1, \"1\", cond(mod(_n, 4) == 2, \"2\", cond(mod(_n, 4) == 3, \"3\", \"4\")))\ngen Session = cond(mod(_n, 4) == 1, \"Session1\", cond(mod(_n, 4) == 2, \"Session2\", cond(mod(_n, 4) == 3, \"Session3\", \"Session4\")))\ngen Score = rnormal(cond(mod(_n, 4) == 1, 80, cond(mod(_n, 4) == 2, 85, cond(mod(_n, 4) == 3, 90, 95))), 5)\n\n* Repeated Measures ANOVA\nanova Score Session|Subject"
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#objectives",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#objectives",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Understand the fundamentals of Repeated Measures ANOVA\nLearn the definition and purpose of Repeated Measures ANOVA\nIdentify the key differences between Repeated Measures ANOVA and One-Way ANOVA\nUnderstand the structure of a Repeated Measures design\nRecognize within-subjects factors and levels\nComprehend the sphericity assumption\nUnderstand the Greenhouse-Geisser correction\nCalculate Sum of Squares (SS) and Mean Squares (MS)\nConduct hypothesis testing for main effects and interaction\nMeasure effect sizes and significance\nInterpret main effects and interaction\nApply the Bonferroni correction\nCheck the assumptions of Repeated Measures ANOVA\nExplore examples of Repeated Measures ANOVA in nursing research"
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#definition-and-purpose-of-repeated-measures-anova",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#definition-and-purpose-of-repeated-measures-anova",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Repeated Measures ANOVA is a statistical test used to examine the changes in a continuous dependent variable across multiple measurements over time or under different conditions. It is an extension of One-Way ANOVA but accounts for the correlation between repeated measures on the same subjects.\n\n\n\nOne-Way ANOVA: Evaluates the effect of a single independent variable on a continuous outcome.\nRepeated Measures ANOVA: Evaluates the effect of a single or multiple within-subject factors over time, accounting for the correlation between repeated measurements."
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#understanding-the-structure-of-a-repeated-measures-design",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#understanding-the-structure-of-a-repeated-measures-design",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "In a Repeated Measures design, each participant is measured multiple times under different conditions or over time. The design includes: - Within-Subjects Factors: Variables measured repeatedly on the same subjects. - Levels: The different conditions or time points for each within-subjects factor.\n\n\n\nWithin-Subjects Factor: Time\nLevels: Baseline, Week 1, Week 2, Week 3"
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#sphericity-assumption",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#sphericity-assumption",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Sphericity refers to the assumption that the variances of the differences between all combinations of related groups are equal. If violated, it can lead to biased F-statistics and increased Type I errors.\n\n\nThe Greenhouse-Geisser correction adjusts the degrees of freedom to correct for violations of sphericity, providing a more conservative estimate."
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#calculation-of-ss-sum-of-squares-and-ms-mean-squares",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#calculation-of-ss-sum-of-squares-and-ms-mean-squares",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Total Sum of Squares (SST): \\[\nSST = \\sum_{i=1}^{n} \\sum_{j=1}^{t} (Y_{ij} - \\bar{Y})^2\n\\] where ( Y_{ij} ) is each individual observation, and ( {Y} ) is the overall mean.\nSum of Squares Within (SSW): \\[\nSSW = \\sum_{i=1}^{n} \\sum_{j=1}^{t} (Y_{ij} - \\bar{Y}_{i\\cdot})^2\n\\] where ( {Y}_{i} ) is the mean of all measurements for subject ( i ).\nSum of Squares Between (SSB): \\[\nSSB = \\sum_{j=1}^{t} n_j (\\bar{Y}_{\\cdot j} - \\bar{Y})^2\n\\] where ( {Y}_{j} ) is the mean for level ( j ), and ( n_j ) is the number of observations at level ( j ).\n\n\n\n\n\nMean Square Within (MSW): \\[\nMSW = \\frac{SSW}{(n - 1)(t - 1)}\n\\]\nMean Square Between (MSB): \\[\nMSB = \\frac{SSB}{t - 1}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#hypothesis-testing-for-main-effects-and-interaction",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#hypothesis-testing-for-main-effects-and-interaction",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Main Effect of Within-Subjects Factor:\n\nNull Hypothesis (( H_0 )): No difference in the means across levels.\nAlternative Hypothesis (( H_1 )): At least one level has a different mean.\n\nInteraction Effect:\n\nNull Hypothesis (( H_0 )): No interaction between within-subjects factors.\nAlternative Hypothesis (( H_1 )): An interaction exists between factors.\n\n\n\n\n\n\nMain Effect of Within-Subjects Factor: \\[\nF = \\frac{MSB}{MSW}\n\\]\n\n\n\n\n\nCompare the F-ratio to the critical value from the F-distribution.\nIf the calculated F-ratio is greater than the critical value, reject the null hypothesis."
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#effect-sizes-and-significance",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#effect-sizes-and-significance",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Represents the proportion of the total variance explained by the within-subjects factor.\n\nWithin-Subjects Factor: \\[\n\\eta^2 = \\frac{SSB}{SST}\n\\]"
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#bonferroni-correction",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#bonferroni-correction",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "A post-hoc correction used to reduce Type I errors when multiple comparisons are made.\n\n\n\\[\n\\alpha_{\\text{corrected}} = \\frac{\\alpha}{m}\n\\] where ( m ) is the number of comparisons."
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#checking-assumptions-of-repeated-measures-anova",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#checking-assumptions-of-repeated-measures-anova",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Use Mauchly’s test to check the assumption of sphericity.\n\n\n\n\n\nUse Q-Q plots to check the normality of residuals.\n\n\n\n\n\nEnsure variances are approximately equal across conditions."
  },
  {
    "objectID": "Lectures/introduction-to-repeated-measures-anova.html#examples-of-repeated-measures-anova-in-nursing-research",
    "href": "Lectures/introduction-to-repeated-measures-anova.html#examples-of-repeated-measures-anova-in-nursing-research",
    "title": "Introduction to Repeated Measures ANOVA",
    "section": "",
    "text": "Research Question: How does blood pressure change over a 3-week intervention period?\nWithin-Subjects Factor: Time\nLevels: Baseline, Week 1, Week 3\nResponse Variable: Blood Pressure\n\ndownload example data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Repeated Structure, select Time as the repeated factor.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(123)\ndata &lt;- data.frame(\n  Subject = rep(1:20, each = 3),\n  Time = factor(rep(c(\"Baseline\", \"Week1\", \"Week3\"), times = 20)),\n  BP = rnorm(60, mean = rep(c(120, 115, 110), each = 20), sd = 10)\n)\n\n# Repeated Measures ANOVA\ninstall.packages(\"ez\")\nlibrary(ez)\nanova_results &lt;- ezANOVA(\n  data = data,\n  dv = BP,\n  wid = Subject,\n  within = Time\n)\nprint(anova_results)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(123)\ndata = pd.DataFrame({\n  'Subject': pd.Categorical([str(i) for i in range(20)] * 3),\n  'Time': pd.Categorical(['Baseline', 'Week1', 'Week3'] * 20),\n  'BP': np.random.normal(loc=115, scale=10, size=60)\n})\n\n# Repeated Measures ANOVA\nmodel = ols('BP ~ C(Time) + C(Subject)', data=data).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\nprint(anova_table)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Repeated Measures.\n\n\nDefine your within-subjects factor (e.g., Time) and specify the number of levels.\n\n\nSelect your continuous dependent variable.\n\n\nClick Options and specify Descriptive Statistics.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 123\n\n* Simulated data\nset obs 60\ngen Subject = cond(mod(_n, 3) == 1, \"1\", cond(mod\n\n(_n, 3) == 2, \"2\", \"3\"))\ngen Time = cond(mod(_n, 3) == 1, \"Baseline\", cond(mod(_n, 3) == 2, \"Week1\", \"Week3\"))\ngen BP = rnormal(cond(mod(_n, 3) == 1, 120, cond(mod(_n, 3) == 2, 115, 110)), 10)\n\n* Repeated Measures ANOVA\nanova BP Time|Subject\n      \n\n\n\n\n\n\n\nResearch Question: How does cognitive function change across four training sessions?\nWithin-Subjects Factor: Session\nLevels: Session 1, Session 2, Session 3, Session 4\nResponse Variable: Cognitive Function Score\n\ndownload example 2 data (CSV)\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your continuous response variable and move it to the Y column.\n\n\nFor Repeated Structure, select Session as the repeated factor.\n\n\nClick Run.\n\n\n\n\n\nR Code Example:\n\n\n\n# Example in R\n# Simulated data\nset.seed(456)\ndata2 &lt;- data.frame(\n  Subject = rep(1:15, each = 4),\n  Session = factor(rep(c(\"Session1\", \"Session2\", \"Session3\", \"Session4\"), times = 15)),\n  Score = rnorm(60, mean = rep(c(80, 85, 90, 95), each = 15), sd = 5)\n)\n\n# Repeated Measures ANOVA\nlibrary(ez)\nanova_results2 &lt;- ezANOVA(\n  data = data2,\n  dv = Score,\n  wid = Subject,\n  within = Session\n)\nprint(anova_results2)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n\n# Example in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Simulated data\nnp.random.seed(456)\ndata2 = pd.DataFrame({\n  'Subject': pd.Categorical([str(i) for i in range(15)] * 4),\n  'Session': pd.Categorical(['Session1', 'Session2', 'Session3', 'Session4'] * 15),\n  'Score': np.random.normal(loc=[80, 85, 90, 95], scale=5, size=60)\n})\n\n# Repeated Measures ANOVA\nmodel2 = ols('Score ~ C(Session) + C(Subject)', data=data2).fit()\nanova_table2 = sm.stats.anova_lm(model2, typ=2)\nprint(anova_table2)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; General Linear Model &gt; Repeated Measures.\n\n\nDefine your within-subjects factor (e.g., Session) and specify the number of levels.\n\n\nSelect your continuous dependent variable.\n\n\nClick Options and specify Descriptive Statistics.\n\n\nClick OK.\n\n\n\n\n\nStata Code Example:\n\n\n\n* Example in Stata\nclear\nset seed 456\n\n* Simulated data\nset obs 60\ngen Subject = cond(mod(_n, 4) == 1, \"1\", cond(mod(_n, 4) == 2, \"2\", cond(mod(_n, 4) == 3, \"3\", \"4\")))\ngen Session = cond(mod(_n, 4) == 1, \"Session1\", cond(mod(_n, 4) == 2, \"Session2\", cond(mod(_n, 4) == 3, \"Session3\", \"Session4\")))\ngen Score = rnormal(cond(mod(_n, 4) == 1, 80, cond(mod(_n, 4) == 2, 85, cond(mod(_n, 4) == 3, 90, 95))), 5)\n\n* Repeated Measures ANOVA\nanova Score Session|Subject"
  }
]