---
title: "Introduction to ANOVA"
author: "Dr. Joshua Lambert"
format:
  html:
    toc: true
    toc-title: "Table of Contents"
    code-fold: false
    code-tools: true
    css: ../styles.css
    width: "99%"
---

[**Download R Code for this Lecture**](code/introduction-to-anova.R){ .btn .btn-primary }



# Understanding Hypothesis Testing and ANOVA

## Constructing a Written and Mathematical Hypothesis

When constructing a hypothesis for ANOVA, it's important to formulate it both in a written and mathematical format.

-   **Written Hypothesis (Example 1):**
    -   Null Hypothesis (Hâ‚€): The average stress in cardiac patients is not different between a placebo group and a behavioral intervention group.
    -   Alternative Hypothesis (Hâ‚): The average stress in cardiac patients differs between the placebo and behavioral intervention groups.
-   **Mathematical Hypothesis (Example 1):** $$ H_{0}: \mu_{placebo} = \mu_{behavior}$$

$$H_{1}: \mu_{placebo} \neq \mu_{behavior}$$

> **ðŸ’¡ R Tip**: In R, we test these hypotheses using the `aov()` function (analysis of variance). The formula notation `outcome ~ group` means "outcome explained by group".

## T-test vs. ANOVA

When comparing means between two groups, the t-test and ANOVA will yield the same results. However, ANOVA extends to cases with more than two groups.

> **ðŸ’¡ R Tip**: For two groups, you can use either `t.test()` or `aov()` in R. For three or more groups, you must use ANOVA.

## Writing ANOVA Hypothesis, Type 1 and 2 Errors

In ANOVA, the hypotheses are:

-   Null Hypothesis (Hâ‚€): All group means are equal.
-   Alternative Hypothesis (Hâ‚): At least one group mean is different.

Type 1 Error: Rejecting the null hypothesis when it is true (false positive).  
Type 2 Error: Failing to reject the null hypothesis when it is false (false negative).

## Assumptions of ANOVA

1.  The dependent variable is measured on an interval or ratio scale.
2.  The independent variable is categorical.
3.  Random sampling.
4.  Normality of Errors
    - The population errors should be normally distributed. We check this by looking at a histogram or Q-Q plot of the model residuals.
5.  Homogeneity of population variance.

> **ðŸ’¡ R Tip**: You can check ANOVA assumptions in R (for more detail on assumption checking, see the [Preliminaries](preliminaries.qmd#taking-your-data-on-a-date-exploratory-data-analysis) lecture):
> - **Normality**: `shapiro.test()` on residuals or Q-Q plot
> - **Homogeneity of variance**: `leveneTest()` from the car package
> - **Visual checks**: Residual plots using `plot(model)`

### What if one of my assumptions is violated?

- **Normality:** One-way ANOVA is generally robust to moderate violations of normality (especially with similar group sizes). Focus on large outliers and whether conclusions change.
- **Be transparent:** Clearly report what you checked, what was violated (if anything), and how you handled it (Results + Limitations).
- **Sensitivity analysis:** You can run (and report) a follow-up test that does not rely on the same violated assumptions.

### Kruskal-Wallis Rank Sum Test (Alternative to One-Way ANOVA)

The Kruskal-Wallis (KW) test is a common nonparametric alternative to one-way ANOVA.

- Useful when your outcome is **ordinal** (e.g., Likert-type responses) or when normality is clearly not reasonable.
- KW does **not** require normality.
- KW does assume the group distributions have a similar **shape** (they may have different medians).

```{r, eval=FALSE}
# Nonparametric alternative to one-way ANOVA
kw_model <- kruskal.test(stress ~ group, data = data)
print(kw_model)
```

<iframe src="http://modeling.nursing.uc.edu/anova_app_2/" width="100%" height="1000px">

</iframe>

## Visualizing Using Boxplots

Boxplots are essential for visualizing data distribution across groups in ANOVA.

```{r, eval=FALSE}
# Example boxplot in R
data <- data.frame(
    group = rep(c("Placebo", "Behavioral", "Drug"), each = 10),
    stress = c(rnorm(10, 5, 1), rnorm(10, 7, 1), rnorm(10, 6, 1))
)

# Basic boxplot
boxplot(stress ~ group,
    data = data,
    main = "Boxplot of Stress Scores",
    xlab = "Group",
    ylab = "Stress Score",
    col = c("lightblue", "lightgreen", "lightcoral")
)

# Add individual points
stripchart(stress ~ group,
    data = data,
    vertical = TRUE,
    method = "jitter",
    add = TRUE,
    pch = 19,
    col = "darkgray"
)
```

> **ðŸ’¡ R Tip**: The formula `stress ~ group` means "stress by group". The `~` symbol separates the dependent variable (left) from the independent variable (right).

## Calculating Variation and Degrees of Freedom

1.  **Sum of Squares Between (SSB):** Variation between groups.
2.  **Sum of Squares Within (SSW):** Variation within groups.
3.  **Total Sum of Squares (SST):** Total variation.

-   **Degrees of Freedom:**
    -   $$df_{between} = k - 1$$ (where k is the number of groups)
    -   $$df_{within} = N - k$$ (where N is the total sample size)

## The F Statistic and P-Value

The F statistic is calculated using the ratio of mean squares:

$$
F = \frac{MSB}{MSW}
$$ 

- MSB: Mean Square Between 
- MSW: Mean Square Within

The p-value is derived from the F distribution, given the degrees of freedom.

The F-distribution is a continuous probability distribution that arises frequently as the null distribution of test statistics, particularly in analysis of variance (ANOVA). It is characterized by two parameters: the degrees of freedom of the numerator (df_between) and the degrees of freedom of the denominator (df_within).

<iframe src="http://modeling.nursing.uc.edu/anova_app_1/" width="100%" height="600px">

</iframe>

## Conducting ANOVA in R

### Basic ANOVA

```{r, eval=FALSE}
# Fit ANOVA model
model <- aov(stress ~ group, data = data)

# View results
summary(model)

# The output shows:
# - Df: Degrees of freedom
# - Sum Sq: Sum of squares
# - Mean Sq: Mean squares
# - F value: F statistic
# - Pr(>F): P-value
```

> **ðŸ’¡ R Tip**: The `summary()` function on an `aov` object gives you the ANOVA table. Look for the `Pr(>F)` column for the p-value.

### Extracting Specific Values

```{r, eval=FALSE}
# Extract F-statistic
f_stat <- summary(model)[[1]]$`F value`[1]

# Extract p-value
p_value <- summary(model)[[1]]$`Pr(>F)`[1]

# Print results
cat("F-statistic:", f_stat, "\n")
cat("P-value:", p_value, "\n")
```

### Checking Assumptions

```{r, eval=FALSE}
# Check normality of residuals
shapiro.test(residuals(model))

# Q-Q plot for normality
qqnorm(residuals(model))
qqline(residuals(model), col = "red")

# Check homogeneity of variance (requires car package)
library(car)
leveneTest(stress ~ group, data = data)

# Diagnostic plots
par(mfrow = c(2, 2))
plot(model)
par(mfrow = c(1, 1))
```

## Post-hoc Analysis

Post-hoc analysis is essential in ANOVA to identify specific group differences following the detection of a significant F-statistic. This analysis involves comparing all possible pairs of group means to determine which differences are statistically significant.

### The Problem of Multiple Comparisons

When we conduct multiple statistical tests simultaneously, the probability of making at least one **Type I error** (a false positive) increases. This is known as **Alpha Inflation** or the **Family-Wise Error Rate (FWER)**.

If you conduct one test at $\alpha = 0.05$, there is a 5% chance of a false positive. However, if you compare 3 groups, you have 3 possible pairwise comparisons. The probability of making *at least one* error is:
$$1 - (1 - 0.05)^3 \approx 0.14 \text{ or 14%}$$

To protect against this, we must use one of two strategies:

1.  **Adjust the Alpha ($\alpha$):** Lower the threshold for significance for each individual test (e.g., the Bonferroni method uses $\alpha / \text{number of tests}$).
2.  **Adjust the P-values:** Keep the $\alpha$ at 0.05 but "penalize" the p-values by making them larger based on the number of comparisons.

### Tukey HSD vs Bonferroni

Tukey's Honestly Significant Difference (HSD) test and the Bonferroni correction are both methods used to manage these multiple comparisons. 

- **Tukey's HSD**: Specifically designed for pairwise comparisons of all group means. It adjusts the p-values and confidence intervals to maintain the family-wise error rate at exactly 0.05.
- **Bonferroni Correction**: The most conservative method. It simply divides your overall alpha (0.05) by the number of comparisons. For 3 comparisons, you would only call a result significant if $p < 0.0167$.

<iframe src="http://modeling.nursing.uc.edu/anova_app_3/" width="100%" height="850px">

</iframe>

### Tukey HSD in R

```{r, eval=FALSE}
# Tukey HSD post-hoc test
tukey_result <- TukeyHSD(model)
print(tukey_result)

# Plot the results
plot(tukey_result)

# The output shows:
# - diff: Difference in means
# - lwr, upr: 95% confidence interval
# - p adj: Adjusted p-value
```

> **ðŸ’¡ R Tip**: If the confidence interval for a pairwise comparison does not include 0, the difference is statistically significant at the 0.05 level.

### Pairwise t-tests with Bonferroni Correction

```{r, eval=FALSE}
# Pairwise t-tests with Bonferroni adjustment
pairwise.t.test(data$stress, data$group, p.adjust.method = "bonferroni")
```

## Comprehensive Example: Complete ANOVA Workflow

Let's work through a complete ANOVA analysis from start to finish:

```{r}
# ============================================
# Complete ANOVA Example
# Research Question: Does treatment type affect pain reduction?
# ============================================

# Step 1: Load required packages
library(car) # For Levene's test

# Step 2: Create or read data
set.seed(456)
pain_data <- data.frame(
    patient_id = 1:60,
    treatment = rep(c("Standard", "Physical Therapy", "Medication"), each = 20),
    pain_reduction = c(
        rnorm(20, mean = 2.5, sd = 1.2), # Standard
        rnorm(20, mean = 4.2, sd = 1.3), # Physical Therapy
        rnorm(20, mean = 3.8, sd = 1.1) # Medication
    )
)

# Convert treatment to factor
pain_data$treatment <- factor(pain_data$treatment)

# Step 3: Explore the data
cat("=== Data Summary ===\n")
summary(pain_data)

cat("\n=== Sample Sizes ===\n")
table(pain_data$treatment)

cat("\n=== Descriptive Statistics by Group ===\n")
cat("Means:\n")
tapply(pain_data$pain_reduction, pain_data$treatment, mean)

cat("\nStandard Deviations:\n")
tapply(pain_data$pain_reduction, pain_data$treatment, sd)

# Step 4: Visualize the data
par(mfrow = c(1, 2))

# Boxplot
boxplot(pain_reduction ~ treatment,
    data = pain_data,
    main = "Pain Reduction by Treatment",
    xlab = "Treatment",
    ylab = "Pain Reduction (points)",
    col = c("lightblue", "lightgreen", "lightcoral")
)

# Add points
stripchart(pain_reduction ~ treatment,
    data = pain_data,
    vertical = TRUE,
    method = "jitter",
    add = TRUE,
    pch = 19,
    col = "darkgray"
)

# Histogram of all data
hist(pain_data$pain_reduction,
    main = "Distribution of Pain Reduction",
    xlab = "Pain Reduction (points)",
    col = "lightblue",
    border = "white"
)

par(mfrow = c(1, 1))

# Step 5: Check assumptions
cat("\n=== Assumption Checks ===\n")

# Normality (overall)
cat("\nShapiro-Wilk Test for Normality:\n")
shapiro_test <- shapiro.test(pain_data$pain_reduction)
print(shapiro_test)

# Homogeneity of variance
cat("\nLevene's Test for Homogeneity of Variance:\n")
levene_test <- leveneTest(pain_reduction ~ treatment, data = pain_data)
print(levene_test)

# Step 6: Conduct ANOVA
cat("\n=== ANOVA Results ===\n")
anova_model <- aov(pain_reduction ~ treatment, data = pain_data)
anova_summary <- summary(anova_model)
print(anova_summary)

# Extract key values
f_value <- anova_summary[[1]]$`F value`[1]
p_value <- anova_summary[[1]]$`Pr(>F)`[1]

cat("\nF-statistic:", round(f_value, 3), "\n")
cat("P-value:", format.pval(p_value, digits = 3), "\n")

# Step 7: Check model diagnostics
cat("\n=== Model Diagnostics ===\n")
par(mfrow = c(2, 2))
plot(anova_model)
par(mfrow = c(1, 1))

# Step 8: Post-hoc tests (if ANOVA is significant)
if (p_value < 0.05) {
    cat("\n=== Post-hoc Analysis (Tukey HSD) ===\n")
    tukey_results <- TukeyHSD(anova_model)
    print(tukey_results)

    # Plot Tukey results
    plot(tukey_results, las = 1)

    cat("\n=== Pairwise Comparisons (Bonferroni) ===\n")
    pairwise_results <- pairwise.t.test(pain_data$pain_reduction,
        pain_data$treatment,
        p.adjust.method = "bonferroni"
    )
    print(pairwise_results)
} else {
    cat("\nANOVA not significant (p >= 0.05). Post-hoc tests not needed.\n")
}

# Step 9: Effect size (eta-squared)
ss_between <- anova_summary[[1]]$`Sum Sq`[1]
ss_total <- sum(anova_summary[[1]]$`Sum Sq`)
eta_squared <- ss_between / ss_total

cat("\n=== Effect Size ===\n")
cat("Eta-squared:", round(eta_squared, 3), "\n")
cat(
    "Interpretation: ",
    ifelse(eta_squared < 0.01, "negligible",
        ifelse(eta_squared < 0.06, "small",
            ifelse(eta_squared < 0.14, "medium", "large")
        )
    ), "\n"
)

# Step 10: Report results
cat("\n=== Summary for Reporting ===\n")
cat("A one-way ANOVA was conducted to compare pain reduction across three treatment groups.\n")
cat(
    "The ANOVA was", ifelse(p_value < 0.05, "significant", "not significant"),
    sprintf(
        "F(%d, %d) = %.2f, p %s.\n",
        anova_summary[[1]]$Df[1],
        anova_summary[[1]]$Df[2],
        f_value,
        ifelse(p_value < 0.001, "< .001", paste("=", round(p_value, 3)))
    )
)

if (p_value < 0.05) {
    cat("Post-hoc comparisons using Tukey HSD indicated significant differences between groups.\n")
}

cat("\n=== Analysis Complete ===\n")
```

## Key Takeaways

âœ… ANOVA tests if means differ across 3+ groups  
âœ… Check assumptions before running ANOVA (normality, homogeneity of variance)  
âœ… Use `aov()` function in R with formula notation `outcome ~ group`  
âœ… If significant, conduct post-hoc tests to identify which groups differ  
âœ… Tukey HSD controls for multiple comparisons  
âœ… Always visualize your data with boxplots before and after analysis  
âœ… Report F-statistic, degrees of freedom, and p-value  

> **ðŸ’¡ R Tip**: Save your ANOVA model as an object (`model <- aov(...)`). You can then use this object for post-hoc tests, diagnostic plots, and extracting results without re-running the analysis!
